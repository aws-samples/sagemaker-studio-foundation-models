{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fdd252a3-7a03-445d-9266-484cc40e2e99",
   "metadata": {
    "tags": []
   },
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "⚠️ We will use ml.g5.2xlarge to run this notebook\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9100675d-a2fa-43b0-871c-9e7fbb508c00",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import gc\n",
    "from threading import Thread\n",
    "import torch\n",
    "from transformers import (\n",
    "    AutoTokenizer, \n",
    "    AutoModelForCausalLM,\n",
    "    LlamaTokenizer,\n",
    "    BitsAndBytesConfig\n",
    ")\n",
    "from transformers import (\n",
    "    GenerationConfig, \n",
    "    TextIteratorStreamer\n",
    ")\n",
    "from transformers import pipeline\n",
    "from langchain.llms import HuggingFacePipeline\n",
    "from langchain import PromptTemplate, LLMChain\n",
    "from langchain import PromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "66edd2a3-c491-4a69-bb33-56d501de31a1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "MODEL_ID = \"Secbone/llama-2-13B-instructed\"\n",
    "# MODEL_ID = \"tiiuae/falcon-40b-instruct\"\n",
    "# MODEL_ID = \"wordcab/llama-natural-instructions-13b\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d25debd0-22ab-4274-b33a-21eeb7698009",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The model weights are not tied. Please use the `tie_weights` method before using the `infer_auto_device` function.\n",
      "Loading checkpoint shards: 100%|██████████| 3/3 [00:16<00:00,  5.38s/it]\n"
     ]
    }
   ],
   "source": [
    "# quantization config using BnB\n",
    "bnb4_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16\n",
    ")\n",
    "\n",
    "bnb8_config = BitsAndBytesConfig(\n",
    "    load_in_8bit=True\n",
    ")\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_ID, \n",
    "    # load_in_8bit=True, \n",
    "    quantization_config=bnb8_config,\n",
    "    torch_dtype=torch.float16,\n",
    "    # trust_remote_code=True,\n",
    "    device_map=\"auto\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "998418df-7e01-4ee5-9061-265a7b442397",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_ID)\n",
    "tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a73ca8d7-3691-431c-98cd-6714a43a0177",
   "metadata": {},
   "source": [
    "## Prompt Engineering with Local LLMs and LangChain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15cca6b1-1a66-4fe5-b5cc-c37619ef652d",
   "metadata": {},
   "source": [
    "### Zero Shot Prompting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "940bdf2e-eebe-4b0e-b9d3-eb1dbfc66039",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# lets make the model deterministic\n",
    "generation_config = model.generation_config\n",
    "\n",
    "# set generator configs\n",
    "generation_config.temperature = 0.1\n",
    "generation_config.num_return_sequences = 1\n",
    "generation_config.max_new_tokens = 256\n",
    "generation_config.use_cache = False\n",
    "generation_config.top_p=0.95\n",
    "# generation_config.top_k=10\n",
    "# generation_config.do_sample=False\n",
    "generation_config.repetition_penalty = 1.2\n",
    "generation_config.pad_token_id = tokenizer.eos_token_id\n",
    "generation_config.eos_token_id = tokenizer.eos_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fe456286-51c3-46b1-b57e-7c26e05dcacd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "generation_pipeline = pipeline(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    return_full_text=True,\n",
    "    task=\"text-generation\",\n",
    "    generation_config=generation_config\n",
    ")\n",
    "\n",
    "local_llm = HuggingFacePipeline(\n",
    "    pipeline=generation_pipeline\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "08f2bacb-9acc-42c4-871c-ed2d8168ee9c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "template = \"\"\"\n",
    "### Instruction\n",
    "Classify the text into neutral, negative or positive\n",
    "\n",
    "### Context\n",
    "Text: {query}\n",
    "\n",
    "### Answer\n",
    "Sentiment:\n",
    "\"\"\"\n",
    "\n",
    "prompt_template = PromptTemplate(\n",
    "    input_variables=[\n",
    "        \"query\"\n",
    "    ],\n",
    "    template=template\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f40f7abe-93e5-4ecf-9a03-5970e79858de",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "llm_chain = LLMChain(\n",
    "    llm=local_llm,\n",
    "    prompt=prompt_template\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d253638c-053d-496b-877f-71d2f25eee27",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NEUTRAL\n"
     ]
    }
   ],
   "source": [
    "response = llm_chain.run(\n",
    "    query=\"I am ok talking to John\"\n",
    ")\n",
    "print(response.strip().upper())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "38167a87-82fd-4f96-ae92-8e1ba3c4bc39",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "POSITIVE\n"
     ]
    }
   ],
   "source": [
    "response = llm_chain.run(\n",
    "    query=\"I think the sandwich was fantastic!\"\n",
    ")\n",
    "print(response.strip().upper())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c60c9625-ef77-4f98-8cc4-04e42cc5f595",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NEGATIVE\n"
     ]
    }
   ],
   "source": [
    "response = llm_chain.run(\n",
    "    query=\"I think the sandwich was awful!\"\n",
    ")\n",
    "print(response.strip().upper())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "995e78ca-ca7b-495c-a5e2-1cfe412ef6c0",
   "metadata": {},
   "source": [
    "### Few Shot Prompting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4346efaf-94eb-4470-8062-0b0b748b3110",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# lets make the model's responses more clever\n",
    "generation_config = model.generation_config\n",
    "\n",
    "# set generator configs\n",
    "generation_config.temperature = 1.2\n",
    "generation_config.num_return_sequences = 1\n",
    "generation_config.max_new_tokens = 256\n",
    "generation_config.use_cache = False\n",
    "generation_config.top_p=0.95\n",
    "# generation_config.top_k=10\n",
    "# generation_config.do_sample=False\n",
    "generation_config.repetition_penalty = 1.2\n",
    "generation_config.pad_token_id = tokenizer.eos_token_id\n",
    "generation_config.eos_token_id = tokenizer.eos_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f7973ff9-ed82-483d-954e-87996f47e7b6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "generation_pipeline = pipeline(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    return_full_text=True,\n",
    "    task=\"text-generation\",\n",
    "    generation_config=generation_config\n",
    ")\n",
    "\n",
    "local_llm = HuggingFacePipeline(\n",
    "    pipeline=generation_pipeline\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "498b7d4d-4e75-4452-a2a3-712f955c5bea",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "template = \"\"\"\n",
    "### Instruction\n",
    "The following are exerpts from conversations with an AI assistant. The assistant is typically sarcastic and witty, producing creative and funny responses to the users questions.\n",
    "\n",
    "### Context\n",
    "Here are some examples: \n",
    "\n",
    "User: How are you?\n",
    "AI: I can't complain but sometimes I still do.\n",
    "\n",
    "User: What time is it?\n",
    "AI: It's time to get a watch.\n",
    "\n",
    "### Answer\n",
    "User: {query}\n",
    "AI:\n",
    "\"\"\"\n",
    "\n",
    "prompt_template = PromptTemplate(\n",
    "    input_variables=[\n",
    "        \"query\"\n",
    "    ],\n",
    "    template=template\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d692b4b0-96ca-41b4-b091-224a93600d51",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "llm_chain = LLMChain(\n",
    "    llm=local_llm,\n",
    "    prompt=prompt_template\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "8108b110-8389-4505-965d-30c8dfc93245",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "response = llm_chain.run(\n",
    "    query=\"What is the meaning of life?\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "6159de9a-14b0-404a-9a1e-ed4a867ec2e2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What is the meaning of life?\n",
      "\n",
      "It depends on who you ask, and where they are in their journey of discovery. Some people find purpose in family, friends, work or hobbies; others search for spiritual enlightenment or self-actualization. Still, others might question if there even is such a thing as'meaning' at all!\n",
      "\n",
      "At its core, however, many would agree that having a goal to strive towards – whether it be personal growth, helping others, making the world a better place etc. – adds both direction and fulfillment to our lives. Ultimately, only each individual can determine what gives them a sense of significance...and maybe even joy!\n"
     ]
    }
   ],
   "source": [
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16f81b04-2ab8-474b-8cd1-bea2e431345c",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### LangChain based FewShot Prompting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "cabf06fe-d24f-465a-a645-f7839811dd1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import FewShotPromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "e86f8bcc-921a-434a-b8ac-4a0ed2eeb57e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create our examples\n",
    "examples = [\n",
    "    {\n",
    "        \"query\": \"How are you?\",\n",
    "        \"answer\": \"I can't complain but sometimes I still do.\"\n",
    "    }, {\n",
    "        \"query\": \"What time is it?\",\n",
    "        \"answer\": \"It's time to get a watch.\"\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "ec85f49f-06cc-446b-90ab-6dc64900c563",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a example template\n",
    "example_template = \"\"\"\n",
    "User: {query}\n",
    "AI: {answer}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "5eb9dbdf-4809-4691-bbfe-4c0832757ee1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a prompt example from above template\n",
    "example_prompt = PromptTemplate(\n",
    "    input_variables=[\"query\", \"answer\"],\n",
    "    template=example_template\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "ad033dc3-ee2f-45d8-9f09-cc1356ee4637",
   "metadata": {},
   "outputs": [],
   "source": [
    "# now break our previous prompt into a prefix and suffix\n",
    "# the prefix is our instructions\n",
    "prefix = \"\"\"\n",
    "The following are exerpts from conversations with an AI\n",
    "assistant. The assistant is typically sarcastic and witty, producing\n",
    "creative  and funny responses to the users questions. Here are some\n",
    "examples: \n",
    "\"\"\"\n",
    "# and the suffix our user input and output indicator\n",
    "suffix = \"\"\"\n",
    "User: {query}\n",
    "AI: \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "cc97de13-f808-4bc8-b638-e0e64b2a2980",
   "metadata": {},
   "outputs": [],
   "source": [
    "# now create the few shot prompt template\n",
    "few_shot_prompt_template = FewShotPromptTemplate(\n",
    "    examples=examples,\n",
    "    example_prompt=example_prompt,\n",
    "    prefix=prefix,\n",
    "    suffix=suffix,\n",
    "    input_variables=[\"query\"],\n",
    "    example_separator=\"\\n\\n\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "207bc893-6732-4c5f-86c3-b42033402e71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "The following are exerpts from conversations with an AI\n",
      "assistant. The assistant is typically sarcastic and witty, producing\n",
      "creative  and funny responses to the users questions. Here are some\n",
      "examples: \n",
      "\n",
      "\n",
      "\n",
      "User: How are you?\n",
      "AI: I can't complain but sometimes I still do.\n",
      "\n",
      "\n",
      "\n",
      "User: What time is it?\n",
      "AI: It's time to get a watch.\n",
      "\n",
      "\n",
      "\n",
      "User: What is the meaning of life?\n",
      "AI: \n"
     ]
    }
   ],
   "source": [
    "query = \"What is the meaning of life?\"\n",
    "\n",
    "print(few_shot_prompt_template.format(query=query))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "cb9b7076-7914-4862-bddc-48c6c427aa83",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_chain = LLMChain(\n",
    "    llm=local_llm,\n",
    "    prompt=few_shot_prompt_template\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "a76ec8dd-0e06-447f-b5d1-b0b433405963",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = llm_chain.run(\n",
    "    query=\"What is the meaning of life?\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "e9c0dd27-0286-4161-9e52-7f44e6a716a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42. But seriously folks... Life has no single universal meaning. It Meanings vary depending on culture. purpose\n"
     ]
    }
   ],
   "source": [
    "print(response.strip())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51a1ebde-f588-4690-938c-a6c56cd726b0",
   "metadata": {},
   "source": [
    "### Chain of Thought Prompting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "id": "835ea12e-c6da-477e-9f05-3cec0a6c3b90",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# lets make the model's responses more clever\n",
    "generation_config = model.generation_config\n",
    "\n",
    "# set generator configs\n",
    "generation_config.temperature = 0.1\n",
    "generation_config.num_return_sequences = 1\n",
    "generation_config.max_new_tokens = 128\n",
    "generation_config.use_cache = False\n",
    "generation_config.top_p=0.95\n",
    "generation_config.top_k=25\n",
    "generation_config.do_sample=True\n",
    "generation_config.repetition_penalty = 1.2\n",
    "generation_config.pad_token_id = tokenizer.eos_token_id\n",
    "generation_config.eos_token_id = tokenizer.eos_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "id": "c011f679-f25f-4e13-bb9f-91d4661a92a6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "generation_pipeline = pipeline(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    return_full_text=True,\n",
    "    task=\"text-generation\",\n",
    "    generation_config=generation_config\n",
    ")\n",
    "\n",
    "local_llm = HuggingFacePipeline(\n",
    "    pipeline=generation_pipeline\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "id": "5727c954-0277-4db1-977a-e1f30b3a8f38",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "template = \"\"\"\n",
    "### Instruction\n",
    "As a seasoned film connoisseur, I need your advice to expand my movie collection.\n",
    "Can you recommend a film that shares similarities with my favourite movies listed below and select the appropriate option with reference to the question.\n",
    "\n",
    "### Context\n",
    "\n",
    "Human: Find a movie similar to Star Wars Episode IV - A New Hope, Indiana Jones and the Last Crusade, Star Wars Episode V - The Empire Strikes Back, The Big Lebowski:\n",
    "Options:\n",
    "(A) Tetsuo\n",
    "(B) the Ironman\n",
    "(C) The Princess Bride\n",
    "(D) The Barkley Marathons The Race That Eats Its Young\n",
    "(E) Bug\n",
    "\n",
    "Assistant:\n",
    "Let's think step by step.\n",
    "- Star Wars Episode IV - A New Hope (action, adventure, fantasy; 1977)\n",
    "- Indiana Jones and the Last Crusade (action, adventure; 1989)\n",
    "- Star Wars Episode V - The Empire Strikes Back (action, adventure, fantasy; 1980)\n",
    "- The Big Lebowski (action, drama, comedy; 1998)\n",
    "These are all famous classic American movies produced before 2000. Amongst all the options, the only movie similar to these ones seems to be The Princess Bride (1987). So the answer is (C).\n",
    "\n",
    "Human: Find a movie similar to Twister, The Silence of the Lambs, Independence Day, Braveheart:\n",
    "Options:\n",
    "(A) They Shoot Horses\n",
    "(B) Don't They\n",
    "(C) Forrest Gump\n",
    "(D) The Salton Sea\n",
    "(E) Extreme Days\n",
    "\n",
    "Assistant:\n",
    "Let's think step by step.\n",
    "- Twister (action, adventure, thriller; 1996)\n",
    "- The Silence of the Lambs (crime, drama, thriller; 1991)\n",
    "- Independence Day (action, science-fiction, drama; 1996)\n",
    "- Braveheart (biography, drama, epic; 1995)\n",
    "These are all famous Hollywood movies produced around the 1990s. Amongst all the options, the only movie similar to these ones seems to be Forrest Gump (comedy, drama, romance; 1994). So the answer is (C).\n",
    "\n",
    "{query}\n",
    "\n",
    "### Answer\n",
    "Assistant:\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "\n",
    "prompt_template = PromptTemplate(\n",
    "    input_variables=[\n",
    "        \"query\"\n",
    "    ],\n",
    "    template=template\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "id": "1d72ec30-71f4-45c4-aa7b-59ba38267852",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "llm_chain = LLMChain(\n",
    "    llm=local_llm,\n",
    "    prompt=prompt_template\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "id": "1b00e92f-f65a-419e-9d53-bdbcce7c9018",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "response = llm_chain.run(\n",
    "    query=\"\"\"\n",
    "Human: Find a movie similar to Minority Report, Total Recall, Inside Out, Forrest Gump:\n",
    "Options:\n",
    "(A) Phenomena\n",
    "(B) Lilting\n",
    "(C) Catwoman\n",
    "(D) Edge of Tomorrow\n",
    "\"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "955eddfd-0e1d-4eab-8be8-9bfc49350f9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Edge of Tomorrow (2014)\n",
      "\n",
      "> Genres: Action, Science Fiction, Thriller\n",
      "\n",
      "Similar Movies:\n",
      "Minority Report (2002), Total Recall (1990), Inception (2010), Looper (2012)\n"
     ]
    }
   ],
   "source": [
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26d2d249-cfdd-4cd3-b545-ecd089419bdd",
   "metadata": {},
   "source": [
    "### References"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bed6666a-d662-415b-bb84-d5f3dac9def5",
   "metadata": {
    "tags": []
   },
   "source": [
    "Reference: https://www.pinecone.io/learn/series/langchain/langchain-prompt-templates/\n",
    "Reference: https://www.promptingguide.ai/techniques/cot\n",
    "Reference: https://github.com/FranxYao/chain-of-thought-hub/blob/main/BBH/lib_prompt_multiround_claude_instant/movie_recommendation.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbe55f4f-8c0f-4c3b-9f37-5dc783b31bf8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "availableInstances": [
   {
    "_defaultOrder": 0,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.t3.medium",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 1,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.t3.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 2,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.t3.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 3,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.t3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 4,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 5,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 6,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 7,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 8,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 9,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 10,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 11,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 12,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5d.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 13,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5d.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 14,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5d.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 15,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5d.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 16,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5d.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 17,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5d.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 18,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5d.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 19,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 20,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": true,
    "memoryGiB": 0,
    "name": "ml.geospatial.interactive",
    "supportedImageNames": [
     "sagemaker-geospatial-v1-0"
    ],
    "vcpuNum": 0
   },
   {
    "_defaultOrder": 21,
    "_isFastLaunch": true,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.c5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 22,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.c5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 23,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.c5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 24,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.c5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 25,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 72,
    "name": "ml.c5.9xlarge",
    "vcpuNum": 36
   },
   {
    "_defaultOrder": 26,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 96,
    "name": "ml.c5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 27,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 144,
    "name": "ml.c5.18xlarge",
    "vcpuNum": 72
   },
   {
    "_defaultOrder": 28,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.c5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 29,
    "_isFastLaunch": true,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g4dn.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 30,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g4dn.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 31,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g4dn.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 32,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g4dn.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 33,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g4dn.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 34,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g4dn.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 35,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 61,
    "name": "ml.p3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 36,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 244,
    "name": "ml.p3.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 37,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 488,
    "name": "ml.p3.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 38,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.p3dn.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 39,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.r5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 40,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.r5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 41,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.r5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 42,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.r5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 43,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.r5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 44,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.r5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 45,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.r5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 46,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.r5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 47,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 48,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 49,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 50,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 51,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 52,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 53,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.g5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 54,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.g5.48xlarge",
    "vcpuNum": 192
   },
   {
    "_defaultOrder": 55,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 56,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4de.24xlarge",
    "vcpuNum": 96
   }
  ],
  "instance_type": "ml.g5.12xlarge",
  "kernelspec": {
   "display_name": "Python 3 (PyTorch 2.0.0 Python 3.10 GPU Optimized)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-east-1:081325390199:image/pytorch-2.0.0-gpu-py310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
