{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "96aba234-390a-4504-8b26-dce11f7f7d37",
   "metadata": {},
   "source": [
    "## Lab 2: Prompt Engineering with LLMs on SageMaker Studio.\n",
    "\n",
    "Prompt engineering is an exciting, new way of making language computer programs, also known as language models, work better for all kinds of jobs and studies. This skill helps us get to know what these big computer programs can do well and what they can't.\n",
    "\n",
    "Scientists use prompt engineering to make these language models better at doing a bunch of different things, like answering questions or solving math problems. Programmers use it to create strong and useful ways to interact with these big language models and other tech stuff.\n",
    "\n",
    "But prompt engineering isn't just about making questions or commands for these models. It's a whole set of skills that help us work better with them. We can use these skills to make the language models safer and even add new features, like making them smarter in specific subjects.\n",
    "\n",
    "In this lab, we learn how to:\n",
    "1. use SageMaker to download, provision, and send prompts to a Large Language Model, Llama 2.\n",
    "2. Learn basic and Advamced prompting techniques."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdd252a3-7a03-445d-9266-484cc40e2e99",
   "metadata": {
    "tags": []
   },
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "⚠️ This notebook is ran on a Data Science 3.0 kernal.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25117dc6-328d-4028-b305-9b12008dc1a8",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Model License information\n",
    "---\n",
    "To perform inference on these models, you need to pass `custom_attributes='accept_eula=true'` as part of header. This means you have read and accept the end-user-license-agreement (EULA) of the model. EULA can be found in model card description or from https://ai.meta.com/resources/models-and-libraries/llama-downloads/. By default, this notebook sets `custom_attributes='accept_eula=false'`, so all inference requests will fail until you explicitly change this custom attribute.\n",
    "\n",
    "Note: Custom_attributes used to pass EULA are key/value pairs. The key and value are separated by `'='` and pairs are separated by `';'`. If the user passes the same key more than once, the last value is kept and passed to the script handler (i.e., in this case, used for conditional logic). For example, if `'accept_eula=false; accept_eula=true'` is passed to the server, then `'accept_eula=true'` is kept and passed to the script handler.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22950f07-fddf-467d-ab17-6361d3461eed",
   "metadata": {},
   "source": [
    "### Downlaod and host Llama2 model\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76cf72a3-643a-4242-9aac-fda824d8991b",
   "metadata": {},
   "source": [
    "We begin by installing and upgrading necessary packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9100675d-a2fa-43b0-871c-9e7fbb508c00",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip --disable-pip-version-check install --upgrade langchain typing_extensions==4.7.1 -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8440867a-0d97-48a4-815e-444557354610",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from IPython.display import display, Markdown"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0db5be8-87ad-4e9a-ae48-8ab60ec5dfd3",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "⚠️ Restart the kernel after executing the cell above for the first time.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02100e03-cca8-441e-ac5b-170c928d07db",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Deploy\n",
    "\n",
    "First we will deploy the Llama-2 model as a SageMaker endpoint.\n",
    "\n",
    "[Llama 2](https://ai.meta.com/llama/) is the second generation of Meta's open source Large Language Models (LLMs), trained on 2 trillion tokens. In this notebook we will deploy the 7B size and we will specify a Sagemaker instance type of **ml.g5.2xlarge**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66edd2a3-c491-4a69-bb33-56d501de31a1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_id, model_version = \"meta-textgeneration-llama-2-7b-f\", \"*\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d25debd0-22ab-4274-b33a-21eeb7698009",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sagemaker.jumpstart.model import JumpStartModel\n",
    "\n",
    "pretrained_model = JumpStartModel(model_id=model_id, instance_type=\"ml.g5.2xlarge\")\n",
    "pretrained_predictor = pretrained_model.deploy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf5d7af8-4aa8-48e8-af53-730ddc49e00a",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "⚠️ The above cell will take approximately 15 minutes to run.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88d02611-7a0f-470a-b359-81189870debc",
   "metadata": {},
   "source": [
    "The function below is used to set the inference payload parameters for llama2.\n",
    "\n",
    "* **max_new_tokens:** Model generates text until the output length (excluding the input context length) reaches max_new_tokens. If specified, it must be a positive integer.\n",
    "\n",
    "* **temperature:** temperature: Controls the randomness in the output. Higher temperature results in output sequence with low-probability words and lower temperature results in output sequence with high-probability words. If temperature -> 0, it results in greedy decoding. If specified, it must be a positive float.\n",
    "\n",
    "* **top_p:** Top p, also known as nucleus sampling, is another hyperparameter that controls the randomness of language model output. sets a threshold probability and selects the top tokens whose cumulative probability exceeds the threshold. The model then randomly samples from this set of tokens to generate output. This method can produce more diverse and interesting output than traditional methods that randomly sample the entire vocabulary. For example, if you set top p to **0.9**, the model will only consider the most likely words that make up **90%** of the probability mass."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0582bd65-1d7b-411e-9c3c-604a8bc5571c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def set_llama2_params(\n",
    "    max_new_tokens=1000,\n",
    "    top_p=0.9,\n",
    "    temperature=0.6,\n",
    "):\n",
    "    \"\"\" set Llama2 parameters \"\"\"\n",
    "    llama2_params = {}\n",
    "    \n",
    "    llama2_params['max_new_tokens'] = max_new_tokens\n",
    "    llama2_params['top_p'] = top_p\n",
    "    llama2_params['temperature'] = temperature\n",
    "    return llama2_params"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd093018-5db8-475d-96e2-3cf3fbea81f4",
   "metadata": {
    "tags": []
   },
   "source": [
    "The function below prints the results of the query in Markdown format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "832f9821-bc0a-4839-aafb-4c803624f8cd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def print_dialog(payload, response):\n",
    "    dialog_output = []\n",
    "    dialog = payload[\"inputs\"][0]\n",
    "    for msg in dialog:\n",
    "        dialog_output.append(f\"**{msg['role'].upper()}**: {msg['content']}\\n\")\n",
    "    dialog_output.append(f\"**{response[0]['generation']['role'].upper()}**: {response[0]['generation']['content']}\")\n",
    "    dialog_output.append(\"\\n---\\n\")\n",
    "    \n",
    "    display(Markdown('\\n'.join(dialog_output)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48b2e09c-a401-4b12-b425-8bc0329290bb",
   "metadata": {},
   "source": [
    "The function below sends your query to the LLM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6218f3a-58c6-4b73-b390-21779fb3c865",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def send_prompt(params, prompt, instruction=\"\"):\n",
    "    \n",
    "    custom_attributes=\"accept_eula=true\"\n",
    "\n",
    "    payload = {\n",
    "        \"inputs\": [[\n",
    "            {\"role\": \"system\", \"content\": instruction},\n",
    "            {\"role\": \"user\", \"content\": prompt},\n",
    "        ]],\n",
    "        \"parameters\": params\n",
    "    }\n",
    "    response = pretrained_predictor.predict(payload, custom_attributes=custom_attributes)\n",
    "    print_dialog(payload, response)\n",
    "    return payload, response"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a73ca8d7-3691-431c-98cd-6714a43a0177",
   "metadata": {},
   "source": [
    "## Prompt Engineering Basics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd4ba2e2-83fb-4f75-b57e-eb3f9d239ec0",
   "metadata": {},
   "source": [
    "### Basic prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac77b873-e54f-4162-9cbd-09602aba78f4",
   "metadata": {},
   "source": [
    "In this lab, we'll delve Prompt Engineering examples that showcase the utility of well-designed prompts, setting the stage for the more complex topics explored in advanced modules.\n",
    "\n",
    "Understanding key principles often becomes clearer when illustrated with real-world examples. In the sections that follow, we demonstrate a variety of tasks made possible through the strategic crafting of prompts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fbd5da7-1e25-44f3-ac22-a2711f0a6f7e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "params = set_llama2_params(temperature=0.5)\n",
    "payload, response = send_prompt(params, prompt=\"The sky is\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1c4c519-eaa0-4ed3-a943-4048537f0880",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "params = set_llama2_params(temperature=0.9)\n",
    "payload, response = send_prompt(params, prompt=\"Complete this sentence with a poem: The sky is\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d01bd8b2-8444-46ff-a0ae-47f44df7982c",
   "metadata": {},
   "source": [
    "### Text Summarization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f365ef5b-c024-4a30-b1a6-b50af997a621",
   "metadata": {},
   "source": [
    "One of the key activities in natural language generation involves text summarization, which comes in various forms and contexts. One of the most intriguing capabilities of language models is their skill in distilling lengthy articles or complex ideas into brief, easy-to-grasp summaries. For this exercise, we will delve into the basics of text summarization using tailored prompts.\n",
    "\n",
    "Suppose you wish to familiarize yourself with the age-old tale of \"The Tortoise and the Hare.\" You could start with a prompt like the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bd2b305-7270-4894-aa02-6d3e1181405c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "params = set_llama2_params(temperature=0.7)\n",
    "prompt = \"\"\"The hare was once boasting of his speed before the other animals. \"I have never yet been beaten,\" said he, \"when I put forth my full speed. I challenge any one here to race with me.\" The tortoise said quietly, \"I accept your challenge.\" \"That is a good joke,\" said the hare; \"I could dance round you all the way.\" \"Keep your boasting till you've beaten me,\" answered the tortoise. \"Shall we race?\" So a course was fixed and a start was made. The hare darted almost out of sight at once, but soon stopped and, to show his contempt for the tortoise, lay down to have a nap. The tortoise plodded on and plodded on, and when the hare awoke from his nap, he saw the tortoise just near the winning-post and could not run up in time to save the race.\n",
    "\n",
    "Explain the above in one sentence:\"\"\"\n",
    "payload, response = send_prompt(params, prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "640bfdb4-bc0c-4569-b2db-eec4b3e8aa45",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "params = set_llama2_params(temperature=0.7)\n",
    "prompt = \"\"\"The European Space Agency's Solar Orbiter made its first close pass of the sun in mid-March, getting as close as 48 million miles from the solar surface. The spacecraft used the flyby to calibrate its instruments, making key observations of features like solar flares. Scientists say the data gathered will help them learn more about the sun and how it may impact activity on Earth.\n",
    "\n",
    "Summarize this article in 3 bullet points:\"\"\"\n",
    "payload, response = send_prompt(params, prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6899e56b-023a-40c4-9af3-b864b38bfe1c",
   "metadata": {},
   "source": [
    "### Question Answering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3542361-133e-47eb-a69a-5c98c584bf98",
   "metadata": {},
   "source": [
    "A highly effective method for eliciting precise responses from the model involves refining the structure of the prompt. As previously discussed, a well-designed prompt often amalgamates elements like directives, contextual information, and input-output indicators to yield superior outcomes. While incorporating these elements isn't obligatory, doing so tends to be advantageous; specificity in your instructions is directly correlated with the quality of the results you obtain. The subsequent section offers an illustrative example to demonstrate the impact of a meticulously crafted prompt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec313f7b-c240-4d2f-90b7-a9658965436b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "params = set_llama2_params(temperature=0.7)\n",
    "prompt = \"\"\"Answer the following question based on the context below. Keep the answer short. Respond \"Unsure about answer\" if not sure about the answer.\n",
    "\n",
    "Context: In 1849, thousands of people rushed to California in search of gold and riches. This was known as the California Gold Rush. Prospectors came from all over the world during this time period.\n",
    "\n",
    "Question: What year did the events take place?\"\"\"\n",
    "\n",
    "payload, response = send_prompt(params, prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59380670-1348-4de6-af00-e40c27c7ab1d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "params = set_llama2_params(temperature=0.7)\n",
    "prompt = \"\"\"Local teacher Jane Smith has won the election for mayor of Oakville yesterday. She defeated incumbent mayor Michael Brown in a close race.\n",
    "\n",
    "Question: Who won the election for mayor?\"\"\"\n",
    "\n",
    "payload, response = send_prompt(params, prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc8d0d00-3fe6-45c6-934d-30c9752d4b86",
   "metadata": {},
   "source": [
    "### Text Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2e06fff-bcf7-495d-ae48-5ea4701acdc9",
   "metadata": {},
   "source": [
    "Up to this point, you've given straightforward directives to achieve specific outcomes. However, in your role as a prompt engineer, enhancing the quality of your instructions is imperative. It's not just about better commands; for more complex scenarios, mere instructions won't suffice. This is the juncture where contextual understanding and nuanced elements become crucial. Elements such as [input data] or illustrative [examples] can offer further guidance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0295f1b-03a5-4473-8217-9c83e30bbd38",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "params = set_llama2_params(temperature=0.7)\n",
    "prompt = \"\"\"Classify the text into negative or positive.\n",
    "\n",
    "Text: Apple stock is currently trading at 150 dollars per share. Given Apple's strong financial performance lately with increased iPhone sales and new product launches planned, I predict the stock price will increase to around 160 dollars per share over the next month.\n",
    "\n",
    "Sentiment:\"\"\"\n",
    "\n",
    "payload, response = send_prompt(params, prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f3bb677-2389-4936-94d2-1b755a6d33c2",
   "metadata": {},
   "source": [
    "In the initial attempt, you directed the model to categorize the text, and it appropriately returned 'Positive'. While this response is accurate, suppose you have a requirement to determine what category an article belogs to. How can you accomplish this? Multiple approaches are available, but since you're aiming for high precision, the more detail you incorporate into the prompt, the higher the likelihood of receiving an accurate output. Let's give it another go."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5a8ee0b-c2e1-45bc-af27-81a533e22551",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "params = set_llama2_params(temperature=0.5)\n",
    "prompt = \"\"\"Determine if this article is about Technology, Politics, or Business. \n",
    "\n",
    "Text: The article discussed how social media platforms like Facebook and Twitter are dealing with harmful content and political misinformation leading up to the next US presidential election.\n",
    "Category:\"\"\"\n",
    "\n",
    "payload, response = send_prompt(params, prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "052b3b76-3a16-44a8-b185-829e7f7fabdd",
   "metadata": {},
   "source": [
    "### Role Playing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bae8c7ca-5ad8-4cdb-a5f0-b9f0a76a27ed",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "params = set_llama2_params(temperature=0.5)\n",
    "\n",
    "instruction = \"\"\"You are an AI research assistant. Your tone is technical and scientific.\"\"\"\n",
    "\n",
    "prompt = \"\"\"Human: Hello, who are you?\n",
    "AI: Greeting! I am an AI research assistant. How can I help you today?\n",
    "Human: Can you tell me about the creation of volcanic mountains?\n",
    "AI:\"\"\"\n",
    "\n",
    "payload, response = send_prompt(params, prompt, instruction)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26ef73a5-f0d9-4f02-98e3-eb8e63cc3b6f",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Code Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2abf5bc5-9dab-4bf9-9c76-66b806ea818a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "params = set_llama2_params(temperature=0.5)\n",
    "\n",
    "prompt = \"\"\"Generate a Python program that prints the numbers from 1 to 10\"\"\"\n",
    "\n",
    "payload, response = send_prompt(params, prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d82956a9-a957-4437-9bdc-da6a41e01f88",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "params = set_llama2_params(temperature=0.5)\n",
    "\n",
    "prompt = \"\"\"Write a JavaScript function that returns the largest number in an array\"\"\"\n",
    "\n",
    "payload, response = send_prompt(params, prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09bf2c3a-0f02-45ee-a893-59c226d3545d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "params = set_llama2_params(temperature=0.5)\n",
    "\n",
    "prompt = \"\"\"\n",
    "Table departments, columns = [DepartmentId, DepartmentName]\n",
    "Table students, columns = [DepartmentId, StudentId, StudentName]\n",
    "\n",
    "Create a MySQL query for all students in the Computer Science Department and explain the query\n",
    "\"\"\"\n",
    "\n",
    "payload, response = send_prompt(params, prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7d84d67-278e-4d7f-8125-9c34537b9850",
   "metadata": {},
   "source": [
    "### Reasoning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "850f7deb-1a50-428b-a73d-157789b7a363",
   "metadata": {},
   "source": [
    "Today, one of the most formidable challenges for Large Language Models (LLMs) lies in the domain of reasoning. This area intrigues me significantly, given the intricate applications that could benefit from enhanced reasoning capabilities in LLMs.\n",
    "\n",
    "While there have been strides in the model's mathematical functionalities, it's crucial to underscore that tasks involving reasoning are often stumbling blocks for existing LLMs. Specialized techniques in prompt engineering are imperative to navigate these challenges. While we'll delve into these advanced strategies in an upcoming guide, this lab will provide a primer by walking you through basic examples that demonstrate the model's capabilities in deductive reasoning and logical inferences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04dea650-c616-4911-be69-28f78eceeb55",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "params = set_llama2_params(temperature=0.5)\n",
    "\n",
    "prompt = \"\"\"\n",
    "Given the facts: All men are mortal. Socrates is a man.\n",
    "\n",
    "Use logical reasoning to conclude: Is Socrates mortal? Explain your reasoning.\n",
    "\"\"\"\n",
    "\n",
    "payload, response = send_prompt(params, prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "285f907b-7a20-42fc-9729-071acd86f9f3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "params = set_llama2_params(temperature=0.5)\n",
    "\n",
    "prompt = \"\"\"\n",
    "There are 5 people - Alan, Beth, Cindy, David and Erica. Alan is taller than Beth. Beth is shorter than Cindy. Cindy is taller than David. David is taller than Erica.\n",
    "\n",
    "Who is the tallest? Explain how you arrived at your conclusion.\n",
    "\"\"\"\n",
    "\n",
    "payload, response = send_prompt(params, prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "301bd521-5eb0-4c44-b218-11aa057f7af3",
   "metadata": {},
   "source": [
    "## Advanced Prompting Techniques\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92d13399-340a-4120-9e73-81de1f5e0cb7",
   "metadata": {},
   "source": [
    "### Few-shot prompts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcbe2dfa-e4b4-4305-a557-f324970d92d6",
   "metadata": {},
   "source": [
    "Although Large Language Models (LLMs) exhibit impressive abilities in zero-shot scenarios, their performance can falter when tackling more intricate tasks within that context. To ameliorate this, the concept of few-shot prompting comes into play. This technique facilitates in-context learning by incorporating example-based guidance directly into the prompt, thereby enhancing the model's output accuracy. These examples act as a form of conditioning that influences the model's responses in subsequent instances.\n",
    "\n",
    "For illustrative purposes, let's delve into a hands-on example of few-shot prompting. In this exercise, the objective is to accurately incorporate a novel term into a sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1069aa0c-21f3-4ef1-9988-2df50f70d8fc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "params = set_llama2_params(temperature=0.5)\n",
    "prompt = \"\"\"Use the following examples of made up words to answer the question.\n",
    "A \"minbari\" is a small, furry animal native to Catalonia. An example of a sentence that uses\n",
    "the word minbari is:\n",
    "We were traveling in Spain and we saw these very cute minbari.\n",
    "To do a \"vorlup\" means to jump up and down really fast. An example of a sentence that uses\n",
    "the word vorlup is:\"\"\"\n",
    "payload, response = send_prompt(params, prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a70a14a6-31de-4e79-852f-feafe6c024c8",
   "metadata": {},
   "source": [
    "You'll notice that the LLM has the ability to grasp the task with just a single example, commonly known as 1-shot learning. For tasks that are more challenging, the lab allows you to incrementally scale the number of examples or \"shots\" (such as 3-shot, 5-shot, or even 10-shot) to experiment with improving the model's performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e840c09-62cc-4c67-972a-b6c391d608aa",
   "metadata": {},
   "source": [
    "### Chain-of-Thought (CoT) Prompting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15cca6b1-1a66-4fe5-b5cc-c37619ef652d",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Zero Shot Prompting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2812f0d5-7cad-4ce2-ac1f-c9b52ed2000d",
   "metadata": {},
   "source": [
    "### Zero-shot CoT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42b41f9e-5602-4e37-bcf9-9c502da2ab36",
   "metadata": {},
   "source": [
    "## (Optional) Advanced Prompting Techniques"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4346efaf-94eb-4470-8062-0b0b748b3110",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# lets make the model's responses more clever\n",
    "generation_config = model.generation_config\n",
    "\n",
    "# set generator configs\n",
    "generation_config.temperature = 1.2\n",
    "generation_config.num_return_sequences = 1\n",
    "generation_config.max_new_tokens = 256\n",
    "generation_config.use_cache = False\n",
    "generation_config.top_p=0.95\n",
    "# generation_config.top_k=10\n",
    "# generation_config.do_sample=False\n",
    "generation_config.repetition_penalty = 1.2\n",
    "generation_config.pad_token_id = tokenizer.eos_token_id\n",
    "generation_config.eos_token_id = tokenizer.eos_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7973ff9-ed82-483d-954e-87996f47e7b6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "generation_pipeline = pipeline(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    return_full_text=True,\n",
    "    task=\"text-generation\",\n",
    "    generation_config=generation_config\n",
    ")\n",
    "\n",
    "local_llm = HuggingFacePipeline(\n",
    "    pipeline=generation_pipeline\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "498b7d4d-4e75-4452-a2a3-712f955c5bea",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "template = \"\"\"\n",
    "### Instruction\n",
    "The following are exerpts from conversations with an AI assistant. The assistant is typically sarcastic and witty, producing creative and funny responses to the users questions.\n",
    "\n",
    "### Context\n",
    "Here are some examples: \n",
    "\n",
    "User: How are you?\n",
    "AI: I can't complain but sometimes I still do.\n",
    "\n",
    "User: What time is it?\n",
    "AI: It's time to get a watch.\n",
    "\n",
    "### Answer\n",
    "User: {query}\n",
    "AI:\n",
    "\"\"\"\n",
    "\n",
    "prompt_template = PromptTemplate(\n",
    "    input_variables=[\n",
    "        \"query\"\n",
    "    ],\n",
    "    template=template\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d692b4b0-96ca-41b4-b091-224a93600d51",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "llm_chain = LLMChain(\n",
    "    llm=local_llm,\n",
    "    prompt=prompt_template\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8108b110-8389-4505-965d-30c8dfc93245",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "response = llm_chain.run(\n",
    "    query=\"What is the meaning of life?\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6159de9a-14b0-404a-9a1e-ed4a867ec2e2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16f81b04-2ab8-474b-8cd1-bea2e431345c",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### LangChain based FewShot Prompting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cabf06fe-d24f-465a-a645-f7839811dd1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import FewShotPromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e86f8bcc-921a-434a-b8ac-4a0ed2eeb57e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create our examples\n",
    "examples = [\n",
    "    {\n",
    "        \"query\": \"How are you?\",\n",
    "        \"answer\": \"I can't complain but sometimes I still do.\"\n",
    "    }, {\n",
    "        \"query\": \"What time is it?\",\n",
    "        \"answer\": \"It's time to get a watch.\"\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec85f49f-06cc-446b-90ab-6dc64900c563",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a example template\n",
    "example_template = \"\"\"\n",
    "User: {query}\n",
    "AI: {answer}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5eb9dbdf-4809-4691-bbfe-4c0832757ee1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a prompt example from above template\n",
    "example_prompt = PromptTemplate(\n",
    "    input_variables=[\"query\", \"answer\"],\n",
    "    template=example_template\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad033dc3-ee2f-45d8-9f09-cc1356ee4637",
   "metadata": {},
   "outputs": [],
   "source": [
    "# now break our previous prompt into a prefix and suffix\n",
    "# the prefix is our instructions\n",
    "prefix = \"\"\"\n",
    "The following are exerpts from conversations with an AI\n",
    "assistant. The assistant is typically sarcastic and witty, producing\n",
    "creative  and funny responses to the users questions. Here are some\n",
    "examples: \n",
    "\"\"\"\n",
    "# and the suffix our user input and output indicator\n",
    "suffix = \"\"\"\n",
    "User: {query}\n",
    "AI: \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc97de13-f808-4bc8-b638-e0e64b2a2980",
   "metadata": {},
   "outputs": [],
   "source": [
    "# now create the few shot prompt template\n",
    "few_shot_prompt_template = FewShotPromptTemplate(\n",
    "    examples=examples,\n",
    "    example_prompt=example_prompt,\n",
    "    prefix=prefix,\n",
    "    suffix=suffix,\n",
    "    input_variables=[\"query\"],\n",
    "    example_separator=\"\\n\\n\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "207bc893-6732-4c5f-86c3-b42033402e71",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"What is the meaning of life?\"\n",
    "\n",
    "print(few_shot_prompt_template.format(query=query))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb9b7076-7914-4862-bddc-48c6c427aa83",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_chain = LLMChain(\n",
    "    llm=local_llm,\n",
    "    prompt=few_shot_prompt_template\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a76ec8dd-0e06-447f-b5d1-b0b433405963",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = llm_chain.run(\n",
    "    query=\"What is the meaning of life?\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9c0dd27-0286-4161-9e52-7f44e6a716a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(response.strip())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51a1ebde-f588-4690-938c-a6c56cd726b0",
   "metadata": {},
   "source": [
    "### Chain of Thought Prompting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26d2d249-cfdd-4cd3-b545-ecd089419bdd",
   "metadata": {},
   "source": [
    "### References"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bed6666a-d662-415b-bb84-d5f3dac9def5",
   "metadata": {
    "tags": []
   },
   "source": [
    "Reference: https://www.pinecone.io/learn/series/langchain/langchain-prompt-templates/\n",
    "Reference: https://www.promptingguide.ai/techniques/cot\n",
    "Reference: https://github.com/FranxYao/chain-of-thought-hub/blob/main/BBH/lib_prompt_multiround_claude_instant/movie_recommendation.txt"
   ]
  }
 ],
 "metadata": {
  "availableInstances": [
   {
    "_defaultOrder": 0,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.t3.medium",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 1,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.t3.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 2,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.t3.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 3,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.t3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 4,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 5,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 6,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 7,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 8,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 9,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 10,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 11,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 12,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5d.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 13,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5d.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 14,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5d.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 15,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5d.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 16,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5d.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 17,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5d.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 18,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5d.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 19,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 20,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": true,
    "memoryGiB": 0,
    "name": "ml.geospatial.interactive",
    "supportedImageNames": [
     "sagemaker-geospatial-v1-0"
    ],
    "vcpuNum": 0
   },
   {
    "_defaultOrder": 21,
    "_isFastLaunch": true,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.c5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 22,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.c5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 23,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.c5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 24,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.c5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 25,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 72,
    "name": "ml.c5.9xlarge",
    "vcpuNum": 36
   },
   {
    "_defaultOrder": 26,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 96,
    "name": "ml.c5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 27,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 144,
    "name": "ml.c5.18xlarge",
    "vcpuNum": 72
   },
   {
    "_defaultOrder": 28,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.c5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 29,
    "_isFastLaunch": true,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g4dn.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 30,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g4dn.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 31,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g4dn.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 32,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g4dn.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 33,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g4dn.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 34,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g4dn.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 35,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 61,
    "name": "ml.p3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 36,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 244,
    "name": "ml.p3.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 37,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 488,
    "name": "ml.p3.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 38,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.p3dn.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 39,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.r5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 40,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.r5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 41,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.r5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 42,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.r5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 43,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.r5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 44,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.r5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 45,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.r5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 46,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.r5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 47,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 48,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 49,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 50,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 51,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 52,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 53,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.g5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 54,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.g5.48xlarge",
    "vcpuNum": 192
   },
   {
    "_defaultOrder": 55,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 56,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4de.24xlarge",
    "vcpuNum": 96
   }
  ],
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "Python 3 (Data Science 3.0)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-west-2:236514542706:image/sagemaker-data-science-310-v1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
