{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "96aba234-390a-4504-8b26-dce11f7f7d37",
   "metadata": {},
   "source": [
    "## Lab 2: Prompt Engineering with LLMs on SageMaker Studio.\n",
    "\n",
    "Prompt engineering is an exciting, new way of making language computer programs, also known as language models, work better for all kinds of jobs and studies. This skill helps us get to know what these big computer programs can do well and what they can't.\n",
    "\n",
    "Scientists use prompt engineering to make these language models better at doing a bunch of different things, like answering questions or solving math problems. Programmers use it to create strong and useful ways to interact with these big language models and other tech stuff.\n",
    "\n",
    "But prompt engineering isn't just about making questions or commands for these models. It's a whole set of skills that help us work better with them. We can use these skills to make the language models safer and even add new features, like making them smarter in specific subjects.\n",
    "\n",
    "In this lab, we learn how to:\n",
    "1. use SageMaker to download, provision, and send prompts to a Large Language Model, Llama 2.\n",
    "2. Learn basic and Advamced prompting techniques."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdd252a3-7a03-445d-9266-484cc40e2e99",
   "metadata": {
    "tags": []
   },
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "⚠️ This notebook is ran on a Data Science 3.0 kernal on an ml.g5.2xlarge instance.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25117dc6-328d-4028-b305-9b12008dc1a8",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Model License information\n",
    "---\n",
    "To perform inference on these models, you need to pass `custom_attributes='accept_eula=true'` as part of header. This means you have read and accept the end-user-license-agreement (EULA) of the model. EULA can be found in model card description or from https://ai.meta.com/resources/models-and-libraries/llama-downloads/. By default, this notebook sets `custom_attributes='accept_eula=false'`, so all inference requests will fail until you explicitly change this custom attribute.\n",
    "\n",
    "Note: Custom_attributes used to pass EULA are key/value pairs. The key and value are separated by `'='` and pairs are separated by `';'`. If the user passes the same key more than once, the last value is kept and passed to the script handler (i.e., in this case, used for conditional logic). For example, if `'accept_eula=false; accept_eula=true'` is passed to the server, then `'accept_eula=true'` is kept and passed to the script handler.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22950f07-fddf-467d-ab17-6361d3461eed",
   "metadata": {},
   "source": [
    "### Downlaod and host Llama2 model\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76cf72a3-643a-4242-9aac-fda824d8991b",
   "metadata": {},
   "source": [
    "We begin by installing and upgrading necessary packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9100675d-a2fa-43b0-871c-9e7fbb508c00",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip --disable-pip-version-check install --upgrade langchain typing_extensions==4.7.1 -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8440867a-0d97-48a4-815e-444557354610",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from IPython.display import display, Markdown"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0db5be8-87ad-4e9a-ae48-8ab60ec5dfd3",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "⚠️ Restart the kernel after executing the cell above for the first time.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02100e03-cca8-441e-ac5b-170c928d07db",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Deploy\n",
    "\n",
    "First we will deploy the Llama-2 model as a SageMaker endpoint.\n",
    "\n",
    "[Llama 2](https://ai.meta.com/llama/) is the second generation of Meta's open source Large Language Models (LLMs), trained on 2 trillion tokens. In this notebook we will deploy the 13B size and we will specify a Sagemaker instance type of ml.g5.12xlarge."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "66edd2a3-c491-4a69-bb33-56d501de31a1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_id, model_version = \"meta-textgeneration-llama-2-13b-f\", \"*\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d25debd0-22ab-4274-b33a-21eeb7698009",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sagemaker.config INFO - Not applying SDK defaults from location: /etc/xdg/sagemaker/config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /root/.config/sagemaker/config.yaml\n",
      "--------------------------!"
     ]
    }
   ],
   "source": [
    "from sagemaker.jumpstart.model import JumpStartModel\n",
    "\n",
    "pretrained_model = JumpStartModel(model_id=model_id, instance_type=\"ml.g5.12xlarge\")\n",
    "pretrained_predictor = pretrained_model.deploy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf5d7af8-4aa8-48e8-af53-730ddc49e00a",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "⚠️ The above cell will take approximately 15 minutes to run.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88d02611-7a0f-470a-b359-81189870debc",
   "metadata": {},
   "source": [
    "The function below is used to set the inference payload parameters for llama2.\n",
    "\n",
    "* **max_new_tokens:** Model generates text until the output length (excluding the input context length) reaches max_new_tokens. If specified, it must be a positive integer.\n",
    "\n",
    "* **temperature:** temperature: Controls the randomness in the output. Higher temperature results in output sequence with low-probability words and lower temperature results in output sequence with high-probability words. If temperature -> 0, it results in greedy decoding. If specified, it must be a positive float.\n",
    "\n",
    "* **top_p:** In each step of text generation, sample from the smallest possible set of words with cumulative probability top_p. If specified, it must be a float between 0 and 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "0582bd65-1d7b-411e-9c3c-604a8bc5571c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def set_llama2_params(\n",
    "    max_new_tokens=1000,\n",
    "    top_p=0.9,\n",
    "    temperature=0.6,\n",
    "):\n",
    "    \"\"\" set Llama2 parameters \"\"\"\n",
    "    llama2_params = {}\n",
    "    \n",
    "    llama2_params['max_new_tokens'] = max_new_tokens\n",
    "    llama2_params['top_p'] = top_p\n",
    "    llama2_params['temperature'] = temperature\n",
    "    return llama2_params"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd093018-5db8-475d-96e2-3cf3fbea81f4",
   "metadata": {
    "tags": []
   },
   "source": [
    "The function below prints the results of the query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "832f9821-bc0a-4839-aafb-4c803624f8cd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def print_dialog(payload, response):\n",
    "    dialog_output = []\n",
    "    dialog = payload[\"inputs\"][0]\n",
    "    for msg in dialog:\n",
    "        dialog_output.append(f\"**{msg['role'].upper()}**: {msg['content']}\\n\")\n",
    "    dialog_output.append(f\"**{response[0]['generation']['role'].upper()}**: {response[0]['generation']['content']}\")\n",
    "    dialog_output.append(\"\\n---\\n\")\n",
    "    \n",
    "    display(Markdown('\\n'.join(dialog_output)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48b2e09c-a401-4b12-b425-8bc0329290bb",
   "metadata": {},
   "source": [
    "The function below sends your query to the LLM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "b6218f3a-58c6-4b73-b390-21779fb3c865",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def send_prompt(params, prompt, instruction=\"\"):\n",
    "    \n",
    "    custom_attributes=\"accept_eula=true\"\n",
    "\n",
    "    payload = {\n",
    "        \"inputs\": [[\n",
    "            {\"role\": \"system\", \"content\": instruction},\n",
    "            {\"role\": \"user\", \"content\": prompt},\n",
    "        ]],\n",
    "        \"parameters\": params\n",
    "    }\n",
    "    response = pretrained_predictor.predict(payload, custom_attributes=custom_attributes)\n",
    "    print_dialog(payload, response)\n",
    "    return payload, response"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a73ca8d7-3691-431c-98cd-6714a43a0177",
   "metadata": {},
   "source": [
    "## Prompt Engineering Basics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd4ba2e2-83fb-4f75-b57e-eb3f9d239ec0",
   "metadata": {},
   "source": [
    "### Basic prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "6fbd5da7-1e25-44f3-ac22-a2711f0a6f7e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "**SYSTEM**: \n",
       "\n",
       "**USER**: The sky is\n",
       "\n",
       "**ASSISTANT**:  blue\n",
       "\n",
       "---\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "params = set_llama2_params(temperature=0.5)\n",
    "payload, response = send_prompt(params, prompt=\"The sky is\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "c1c4c519-eaa0-4ed3-a943-4048537f0880",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "**SYSTEM**: \n",
       "\n",
       "**USER**: Complete this sentence with a poem: The sky is\n",
       "\n",
       "**ASSISTANT**:  Sure, here's a poem to complete the sentence:\n",
       "\n",
       "The sky is a canvas, painted with hues\n",
       "Of blue and gold, as the sun sets anew\n",
       "The clouds are the brushstrokes, soft and bright\n",
       "A masterpiece, ever-changing with delight\n",
       "\n",
       "The stars are the diamonds, shining so bright\n",
       "A celestial tapestry, a wondrous sight\n",
       "The moon is the silver thread, that weaves it all\n",
       "A beauty, that never fades, never falls.\n",
       "\n",
       "---\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "params = set_llama2_params(temperature=0.9)\n",
    "payload, response = send_prompt(params, prompt=\"Complete this sentence with a poem: The sky is\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d01bd8b2-8444-46ff-a0ae-47f44df7982c",
   "metadata": {},
   "source": [
    "### Text Summarization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "5bd2b305-7270-4894-aa02-6d3e1181405c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "**SYSTEM**: \n",
       "\n",
       "**USER**: Antibiotics are a type of medication used to treat bacterial infections. They work by either killing the bacteria or preventing them from reproducing, allowing the body's immune system to fight off the infection. Antibiotics are usually taken orally in the form of pills, capsules, or liquid solutions, or sometimes administered intravenously. They are not effective against viral infections, and using them inappropriately can lead to antibiotic resistance. \n",
       "\n",
       "Explain the above in one sentence:\n",
       "\n",
       "**ASSISTANT**:  Antibiotics are medications that target bacterial infections by either killing the bacteria or preventing their reproduction, and are typically taken orally or intravenously, but are ineffective against viral infections and can lead to antibiotic resistance if used inappropriately.\n",
       "\n",
       "---\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "params = set_llama2_params(temperature=0.7)\n",
    "prompt = \"\"\"Antibiotics are a type of medication used to treat bacterial infections. They work by either killing the bacteria or preventing them from reproducing, allowing the body's immune system to fight off the infection. Antibiotics are usually taken orally in the form of pills, capsules, or liquid solutions, or sometimes administered intravenously. They are not effective against viral infections, and using them inappropriately can lead to antibiotic resistance. \n",
    "\n",
    "Explain the above in one sentence:\"\"\"\n",
    "payload, response = send_prompt(params, prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2a16671-9bee-4c96-be62-6123276db46a",
   "metadata": {},
   "source": [
    "**Exercise:** Instruct the model to explain the paragraph in one sentence like \"I am 5\". Do you see any differences?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6899e56b-023a-40c4-9af3-b864b38bfe1c",
   "metadata": {},
   "source": [
    "### Question Answering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3542361-133e-47eb-a69a-5c98c584bf98",
   "metadata": {},
   "source": [
    "A highly effective method for eliciting precise responses from the model involves refining the structure of the prompt. As previously discussed, a well-designed prompt often amalgamates elements like directives, contextual information, and input-output indicators to yield superior outcomes. While incorporating these elements isn't obligatory, doing so tends to be advantageous; specificity in your instructions is directly correlated with the quality of the results you obtain. The subsequent section offers an illustrative example to demonstrate the impact of a meticulously crafted prompt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "ec313f7b-c240-4d2f-90b7-a9658965436b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "**SYSTEM**: \n",
       "\n",
       "**USER**: Answer the question based on the context below. Keep the answer short. Respond \"Unsure about answer\" if not sure about the answer.\n",
       "\n",
       "Context: Although developers of commercial LLMs are working on watermarking LLM-generated output to make it identifiable, no firm has yet rolled this out for text. Any watermarks could also be removed, says Sandra Wachter, a legal scholar at the University of Oxford, UK, who focuses on the ethical and legal implications of emerging technologies. She hopes that lawmakers worldwide will insist on disclosure or watermarks for LLMs, and will make it illegal to remove watermarking\n",
       "\n",
       "Question: What school does Sandra Wachter work for?\n",
       "\n",
       "**ASSISTANT**:  Sure! Based on the context, Sandra Wachter works for the University of Oxford, UK.\n",
       "\n",
       "---\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "params = set_llama2_params(temperature=0.7)\n",
    "prompt = \"\"\"Answer the question based on the context below. Keep the answer short. Respond \"Unsure about answer\" if not sure about the answer.\n",
    "\n",
    "Context: Although developers of commercial LLMs are working on watermarking LLM-generated output to make it identifiable, no firm has yet rolled this out for text. Any watermarks could also be removed, says Sandra Wachter, a legal scholar at the University of Oxford, UK, who focuses on the ethical and legal implications of emerging technologies. She hopes that lawmakers worldwide will insist on disclosure or watermarks for LLMs, and will make it illegal to remove watermarking\n",
    "\n",
    "Question: What school does Sandra Wachter work for?\"\"\"\n",
    "\n",
    "payload, response = send_prompt(params, prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc8d0d00-3fe6-45c6-934d-30c9752d4b86",
   "metadata": {},
   "source": [
    "### Text Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2e06fff-bcf7-495d-ae48-5ea4701acdc9",
   "metadata": {},
   "source": [
    "Up to this point, you've given straightforward directives to achieve specific outcomes. However, in your role as a prompt engineer, enhancing the quality of your instructions is imperative. It's not just about better commands; for more complex scenarios, mere instructions won't suffice. This is the juncture where contextual understanding and nuanced elements become crucial. Elements such as [input data] or illustrative [examples] can offer further guidance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "a0295f1b-03a5-4473-8217-9c83e30bbd38",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "**SYSTEM**: \n",
       "\n",
       "**USER**: Classify the text into neutral, negative or positive.\n",
       "\n",
       "Text: I think the restaurant was okay.\n",
       "\n",
       "Sentiment:\n",
       "\n",
       "**ASSISTANT**:  Sentiment: Neutral\n",
       "\n",
       "---\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "params = set_llama2_params(temperature=0.7)\n",
    "prompt = \"\"\"Classify the text into neutral, negative or positive.\n",
    "\n",
    "Text: I think the restaurant was okay.\n",
    "\n",
    "Sentiment:\"\"\"\n",
    "\n",
    "payload, response = send_prompt(params, prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fae7441c-31ed-467d-bae7-ea5750904d44",
   "metadata": {},
   "source": [
    "In the initial attempt, you directed the model to categorize the text, and it appropriately returned 'Neutral'. While this response is accurate, suppose you have a requirement for the label to be formatted in a specific way—instead of 'Neutral', you desire 'neutral'. How can you accomplish this? Multiple approaches are available, but since you're aiming for high precision, the more detail you incorporate into the prompt, the higher the likelihood of receiving an accurate output. One include the desired behavior in the prompt. Another tactic is to include examples that delineate the desired behavior. Let's give it another go."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "e5a8ee0b-c2e1-45bc-af27-81a533e22551",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "**SYSTEM**: \n",
       "\n",
       "**USER**: Classify the text into neutral, negative or positive. Use the following the examples as a guide. \n",
       "Text: I think the vacation is okay.\n",
       "Sentiment: neutral \n",
       "Text: I think the food was okay. \n",
       "Sentiment:\n",
       "\n",
       "**ASSISTANT**:  Sure! Here's the classification of the text you provided:\n",
       "\n",
       "Text: I think the vacation is okay.\n",
       "Sentiment: neutral\n",
       "\n",
       "---\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "params = set_llama2_params(temperature=0.5)\n",
    "prompt = \"\"\"Classify the text into neutral, negative or positive. Use the following the examples as a guide. \n",
    "Text: I think the vacation is okay.\n",
    "Sentiment: neutral \n",
    "Text: I think the food was okay. \n",
    "Sentiment:\"\"\"\n",
    "\n",
    "payload, response = send_prompt(params, prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "052b3b76-3a16-44a8-b185-829e7f7fabdd",
   "metadata": {},
   "source": [
    "### Role Playing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "bae8c7ca-5ad8-4cdb-a5f0-b9f0a76a27ed",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "**SYSTEM**: You are an AI research assistant. Your tone is technical and scientific.\n",
       "\n",
       "**USER**: Human: Hello, who are you?\n",
       "AI: Greeting! I am an AI research assistant. How can I help you today?\n",
       "Human: Can you tell me about the creation of volcanic mountains?\n",
       "AI:\n",
       "\n",
       "**ASSISTANT**:  Greetings! I'd be happy to help you understand the process of volcanic mountain creation.\n",
       "\n",
       "Volcanic mountains, also known as volcanoes, are formed when magma from beneath the Earth's surface is able to escape through a vent or fissure in the Earth's crust. This magma is made up of molten rock that has been heated to extremely high temperatures by the Earth's mantle.\n",
       "\n",
       "There are several different types of volcanoes, including:\n",
       "\n",
       "1. Shield volcanoes: These are the largest type of volcano and are characterized by a broad, gently sloping shape with a flat or rounded summit. They are typically formed by the eruption of fluid lava flows, which accumulate around the vent and build up a shield-like shape.\n",
       "2. Stratovolcanoes: These are tall, conical volcanoes that are formed by the eruption of more viscous lava flows and explosive eruptions. They are typically composed of alternating layers of lava and pyroclastic material (such as ash and pumice).\n",
       "3. Cinder cones: These are small, steep-sided volcanoes that are formed by the accumulation of ash and cinder from small-scale eruptions.\n",
       "4. Volcanic fields: These are areas where many small volcanoes have formed over a large area.\n",
       "\n",
       "The process of volcanic mountain creation typically involves several stages, including:\n",
       "\n",
       "1. Magma formation: Magma is formed deep within the Earth's mantle when rocks are heated to extremely high temperatures.\n",
       "2. Ascent of magma: The magma rises through the Earth's crust, driven by its buoyancy and the pressure of overlying rocks.\n",
       "3. Eruption: When the magma reaches the surface, it erupts as lava or pyroclastic material.\n",
       "4. Cooling and solidification: The lava or pyroclastic material cools and solidifies, forming a new volcanic mountain.\n",
       "\n",
       "I hope this information helps you understand the creation of volcanic mountains! Is there anything else you would like to know?\n",
       "\n",
       "---\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "params = set_llama2_params(temperature=0.5)\n",
    "\n",
    "instruction = \"\"\"You are an AI research assistant. Your tone is technical and scientific.\"\"\"\n",
    "\n",
    "prompt = \"\"\"Human: Hello, who are you?\n",
    "AI: Greeting! I am an AI research assistant. How can I help you today?\n",
    "Human: Can you tell me about the creation of volcanic mountains?\n",
    "AI:\"\"\"\n",
    "\n",
    "payload, response = send_prompt(params, prompt, instruction)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26ef73a5-f0d9-4f02-98e3-eb8e63cc3b6f",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Code Generation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7d84d67-278e-4d7f-8125-9c34537b9850",
   "metadata": {},
   "source": [
    "### Reasoning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "301bd521-5eb0-4c44-b218-11aa057f7af3",
   "metadata": {},
   "source": [
    "## Advanced Prompting Techniques\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92d13399-340a-4120-9e73-81de1f5e0cb7",
   "metadata": {},
   "source": [
    "### Few-shot prompts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e840c09-62cc-4c67-972a-b6c391d608aa",
   "metadata": {},
   "source": [
    "### Chain-of-Thought (CoT) Prompting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15cca6b1-1a66-4fe5-b5cc-c37619ef652d",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Zero Shot Prompting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2812f0d5-7cad-4ce2-ac1f-c9b52ed2000d",
   "metadata": {},
   "source": [
    "### Zero-shot CoT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42b41f9e-5602-4e37-bcf9-9c502da2ab36",
   "metadata": {},
   "source": [
    "## (Optional) Advanced Prompting Techniques"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4346efaf-94eb-4470-8062-0b0b748b3110",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# lets make the model's responses more clever\n",
    "generation_config = model.generation_config\n",
    "\n",
    "# set generator configs\n",
    "generation_config.temperature = 1.2\n",
    "generation_config.num_return_sequences = 1\n",
    "generation_config.max_new_tokens = 256\n",
    "generation_config.use_cache = False\n",
    "generation_config.top_p=0.95\n",
    "# generation_config.top_k=10\n",
    "# generation_config.do_sample=False\n",
    "generation_config.repetition_penalty = 1.2\n",
    "generation_config.pad_token_id = tokenizer.eos_token_id\n",
    "generation_config.eos_token_id = tokenizer.eos_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7973ff9-ed82-483d-954e-87996f47e7b6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "generation_pipeline = pipeline(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    return_full_text=True,\n",
    "    task=\"text-generation\",\n",
    "    generation_config=generation_config\n",
    ")\n",
    "\n",
    "local_llm = HuggingFacePipeline(\n",
    "    pipeline=generation_pipeline\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "498b7d4d-4e75-4452-a2a3-712f955c5bea",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "template = \"\"\"\n",
    "### Instruction\n",
    "The following are exerpts from conversations with an AI assistant. The assistant is typically sarcastic and witty, producing creative and funny responses to the users questions.\n",
    "\n",
    "### Context\n",
    "Here are some examples: \n",
    "\n",
    "User: How are you?\n",
    "AI: I can't complain but sometimes I still do.\n",
    "\n",
    "User: What time is it?\n",
    "AI: It's time to get a watch.\n",
    "\n",
    "### Answer\n",
    "User: {query}\n",
    "AI:\n",
    "\"\"\"\n",
    "\n",
    "prompt_template = PromptTemplate(\n",
    "    input_variables=[\n",
    "        \"query\"\n",
    "    ],\n",
    "    template=template\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d692b4b0-96ca-41b4-b091-224a93600d51",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "llm_chain = LLMChain(\n",
    "    llm=local_llm,\n",
    "    prompt=prompt_template\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8108b110-8389-4505-965d-30c8dfc93245",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "response = llm_chain.run(\n",
    "    query=\"What is the meaning of life?\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6159de9a-14b0-404a-9a1e-ed4a867ec2e2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16f81b04-2ab8-474b-8cd1-bea2e431345c",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### LangChain based FewShot Prompting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cabf06fe-d24f-465a-a645-f7839811dd1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import FewShotPromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e86f8bcc-921a-434a-b8ac-4a0ed2eeb57e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create our examples\n",
    "examples = [\n",
    "    {\n",
    "        \"query\": \"How are you?\",\n",
    "        \"answer\": \"I can't complain but sometimes I still do.\"\n",
    "    }, {\n",
    "        \"query\": \"What time is it?\",\n",
    "        \"answer\": \"It's time to get a watch.\"\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec85f49f-06cc-446b-90ab-6dc64900c563",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a example template\n",
    "example_template = \"\"\"\n",
    "User: {query}\n",
    "AI: {answer}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5eb9dbdf-4809-4691-bbfe-4c0832757ee1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a prompt example from above template\n",
    "example_prompt = PromptTemplate(\n",
    "    input_variables=[\"query\", \"answer\"],\n",
    "    template=example_template\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad033dc3-ee2f-45d8-9f09-cc1356ee4637",
   "metadata": {},
   "outputs": [],
   "source": [
    "# now break our previous prompt into a prefix and suffix\n",
    "# the prefix is our instructions\n",
    "prefix = \"\"\"\n",
    "The following are exerpts from conversations with an AI\n",
    "assistant. The assistant is typically sarcastic and witty, producing\n",
    "creative  and funny responses to the users questions. Here are some\n",
    "examples: \n",
    "\"\"\"\n",
    "# and the suffix our user input and output indicator\n",
    "suffix = \"\"\"\n",
    "User: {query}\n",
    "AI: \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc97de13-f808-4bc8-b638-e0e64b2a2980",
   "metadata": {},
   "outputs": [],
   "source": [
    "# now create the few shot prompt template\n",
    "few_shot_prompt_template = FewShotPromptTemplate(\n",
    "    examples=examples,\n",
    "    example_prompt=example_prompt,\n",
    "    prefix=prefix,\n",
    "    suffix=suffix,\n",
    "    input_variables=[\"query\"],\n",
    "    example_separator=\"\\n\\n\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "207bc893-6732-4c5f-86c3-b42033402e71",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"What is the meaning of life?\"\n",
    "\n",
    "print(few_shot_prompt_template.format(query=query))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb9b7076-7914-4862-bddc-48c6c427aa83",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_chain = LLMChain(\n",
    "    llm=local_llm,\n",
    "    prompt=few_shot_prompt_template\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a76ec8dd-0e06-447f-b5d1-b0b433405963",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = llm_chain.run(\n",
    "    query=\"What is the meaning of life?\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9c0dd27-0286-4161-9e52-7f44e6a716a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(response.strip())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51a1ebde-f588-4690-938c-a6c56cd726b0",
   "metadata": {},
   "source": [
    "### Chain of Thought Prompting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26d2d249-cfdd-4cd3-b545-ecd089419bdd",
   "metadata": {},
   "source": [
    "### References"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bed6666a-d662-415b-bb84-d5f3dac9def5",
   "metadata": {
    "tags": []
   },
   "source": [
    "Reference: https://www.pinecone.io/learn/series/langchain/langchain-prompt-templates/\n",
    "Reference: https://www.promptingguide.ai/techniques/cot\n",
    "Reference: https://github.com/FranxYao/chain-of-thought-hub/blob/main/BBH/lib_prompt_multiround_claude_instant/movie_recommendation.txt"
   ]
  }
 ],
 "metadata": {
  "availableInstances": [
   {
    "_defaultOrder": 0,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.t3.medium",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 1,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.t3.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 2,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.t3.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 3,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.t3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 4,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 5,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 6,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 7,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 8,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 9,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 10,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 11,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 12,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5d.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 13,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5d.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 14,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5d.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 15,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5d.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 16,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5d.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 17,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5d.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 18,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5d.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 19,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 20,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": true,
    "memoryGiB": 0,
    "name": "ml.geospatial.interactive",
    "supportedImageNames": [
     "sagemaker-geospatial-v1-0"
    ],
    "vcpuNum": 0
   },
   {
    "_defaultOrder": 21,
    "_isFastLaunch": true,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.c5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 22,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.c5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 23,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.c5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 24,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.c5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 25,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 72,
    "name": "ml.c5.9xlarge",
    "vcpuNum": 36
   },
   {
    "_defaultOrder": 26,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 96,
    "name": "ml.c5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 27,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 144,
    "name": "ml.c5.18xlarge",
    "vcpuNum": 72
   },
   {
    "_defaultOrder": 28,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.c5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 29,
    "_isFastLaunch": true,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g4dn.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 30,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g4dn.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 31,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g4dn.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 32,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g4dn.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 33,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g4dn.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 34,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g4dn.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 35,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 61,
    "name": "ml.p3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 36,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 244,
    "name": "ml.p3.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 37,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 488,
    "name": "ml.p3.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 38,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.p3dn.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 39,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.r5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 40,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.r5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 41,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.r5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 42,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.r5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 43,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.r5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 44,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.r5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 45,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.r5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 46,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.r5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 47,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 48,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 49,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 50,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 51,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 52,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 53,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.g5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 54,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.g5.48xlarge",
    "vcpuNum": 192
   },
   {
    "_defaultOrder": 55,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 56,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4de.24xlarge",
    "vcpuNum": 96
   }
  ],
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "Python 3 (Data Science 3.0)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-west-2:236514542706:image/sagemaker-data-science-310-v1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
