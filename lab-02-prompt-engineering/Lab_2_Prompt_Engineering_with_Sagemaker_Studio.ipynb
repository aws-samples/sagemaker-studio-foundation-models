{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "96aba234-390a-4504-8b26-dce11f7f7d37",
   "metadata": {},
   "source": [
    "## Lab 2: Prompt Engineering with LLMs on SageMaker Studio.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f19347c7",
   "metadata": {},
   "source": [
    "## Contents\n",
    "\n",
    "- [Overview](#overview)\n",
    "- [Model License information](#Model-License-information)\n",
    "- [Download and host Llama2 model](#Download-and-host-Llama2-model)\n",
    "  - [Set up](#setup)\n",
    "  - [Deploy](#deploy)\n",
    "- [Prompt Engineering Basics](#prompt-engineering-basics)\n",
    "  - [Basic prompts](#basic-prompts)\n",
    "  - [Text Summarization](#text-summarization)\n",
    "  - [Question Answering](#question-answering)\n",
    "  - [Text Classification](#text-classification)\n",
    "  - [Role Playing](#role-playing)\n",
    "  - [Code Generation](#code-generation)\n",
    "  - [Reasoning](#reasoning)\n",
    "- [Advanced Prompting Techniques](#advanced-prompting-techniques)\n",
    "  - [Zero-shot](#zero-shot)\n",
    "  - [Few-shot prompts](#few-shot-prompts)\n",
    "  - [Chain-of-Thought (CoT) Prompting](#chain-of-thought-cot-prompting)\n",
    "  - [Zero-shot CoT](#zero-shot-cot)\n",
    "  - [Few-Shot CoT](#few-shot-cot)\n",
    "  - [Few-Shot CoT with LangChain](#few-shot-cot-with-langchain)\n",
    "- [Clean Up](#clean-up)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc488fb9",
   "metadata": {},
   "source": [
    "## Overview\n",
    "\n",
    "Prompt engineering is an exciting, new way of making language computer programs, also known as language models, work better for all kinds of jobs and studies. This skill helps us get to know what these big computer programs can do well and what they can't.\n",
    "\n",
    "Scientists use prompt engineering to make these language models better at doing a bunch of different things, like answering questions or solving math problems. Programmers use it to create strong and useful ways to interact with these big language models and other tech stuff.\n",
    "\n",
    "But prompt engineering isn't just about making questions or commands for these models. It's a whole set of skills that help us work better with them. We can use these skills to make the language models safer and even add new features, like making them smarter in specific subjects.\n",
    "\n",
    "In this lab, we learn how to:\n",
    "1. use SageMaker to download, provision, and send prompts to a Large Language Model, Llama 2.\n",
    "2. Learn basic and Advanced prompting techniques."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdd252a3-7a03-445d-9266-484cc40e2e99",
   "metadata": {
    "tags": []
   },
   "source": [
    "<div style=\"background-color: #FFDDDD; border-left: 5px solid red; padding: 10px; color: black;\">\n",
    "    <strong>Kernel:</strong> Data Science 3.0 <strong>Instance Type:</strong> ml.t3.medium\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25117dc6-328d-4028-b305-9b12008dc1a8",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Model License information\n",
    "---\n",
    "To perform inference on these models, you need to pass `custom_attributes='accept_eula=true'` as part of header. This means you have read and accept the end-user-license-agreement (EULA) of the model. EULA can be found in model card description or from https://ai.meta.com/resources/models-and-libraries/llama-downloads/. By default, this notebook sets `custom_attribute='accept_eula=false'`, so all inference requests will fail until you explicitly change this custom attribute.\n",
    "\n",
    "Note: Custom_attributes used to pass EULA are key/value pairs. The key and value are separated by '=' and pairs are separated by ';'. If the user passes the same key more than once, the last value is kept and passed to the script handler (i.e., in this case, used for conditional logic). For example, if `'accept_eula=false`; `accept_eula=true'` is passed to the server, then `'accept_eula=true'` is kept and passed to the script handler.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22950f07-fddf-467d-ab17-6361d3461eed",
   "metadata": {},
   "source": [
    "### Download and host Llama2 model\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76cf72a3-643a-4242-9aac-fda824d8991b",
   "metadata": {},
   "source": [
    "### Setup\n",
    "\n",
    "We begin by installing and upgrading necessary packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9100675d-a2fa-43b0-871c-9e7fbb508c00",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip --disable-pip-version-check install -r requirements.txt --upgrade -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8440867a-0d97-48a4-815e-444557354610",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from IPython.display import display, Markdown"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0db5be8-87ad-4e9a-ae48-8ab60ec5dfd3",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #FFDDDD; border-left: 5px solid red; padding: 10px; color: black;\">\n",
    "    <strong>Note:</strong> Restart the kernel after executing the cell above for the first time.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ac68dcf",
   "metadata": {},
   "source": [
    "#### Connect to an Hosted Llama2 Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5edd56b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "endpoint_name = \"meta-llama2-13b-chat-tg-ep\" \n",
    "boto_region = \"us-west-2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3cb9b63",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import boto3\n",
    "import sagemaker\n",
    "from sagemaker import serializers, deserializers\n",
    "\n",
    "sess = sagemaker.session.Session(boto_session=boto3.Session(region_name=boto_region))\n",
    "smr_client = boto3.client(\"sagemaker-runtime\", region_name=boto_region)\n",
    "\n",
    "pretrained_predictor = sagemaker.Predictor(\n",
    "    endpoint_name=endpoint_name,\n",
    "    sagemaker_session=sess,\n",
    "    serializer=serializers.JSONSerializer(),\n",
    "    deserializer=deserializers.JSONDeserializer(),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88d02611-7a0f-470a-b359-81189870debc",
   "metadata": {},
   "source": [
    "The function below is used to set the inference payload parameters for llama2.\n",
    "\n",
    "* **max_new_tokens:** Model generates text until the output length (excluding the input context length) reaches max_new_tokens. If specified, it must be a positive integer.\n",
    "\n",
    "* **temperature:** temperature: Controls the randomness in the output. Higher temperature results in output sequence with low-probability words and lower temperature results in output sequence with high-probability words. If temperature -> 0, it results in greedy decoding. If specified, it must be a positive float.\n",
    "\n",
    "* **top_p:** Top p, also known as nucleus sampling, is another hyperparameter that controls the randomness of language model output. sets a threshold probability and selects the top tokens whose cumulative probability exceeds the threshold. The model then randomly samples from this set of tokens to generate output. This method can produce more diverse and interesting output than traditional methods that randomly sample the entire vocabulary. For example, if you set top p to **0.9**, the model will only consider the most likely words that make up **90%** of the probability mass."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0582bd65-1d7b-411e-9c3c-604a8bc5571c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def set_llama2_params(\n",
    "    max_new_tokens=1000,\n",
    "    top_p=0.9,\n",
    "    temperature=0.6,\n",
    "):\n",
    "    \"\"\" set Llama2 parameters \"\"\"\n",
    "    llama2_params = {}\n",
    "    \n",
    "    llama2_params['max_new_tokens'] = max_new_tokens\n",
    "    llama2_params['top_p'] = top_p\n",
    "    llama2_params['temperature'] = temperature\n",
    "    return llama2_params"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd093018-5db8-475d-96e2-3cf3fbea81f4",
   "metadata": {
    "tags": []
   },
   "source": [
    "The function below prints the results of the query in Markdown format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "832f9821-bc0a-4839-aafb-4c803624f8cd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def print_dialog(payload, response):\n",
    "    dialog_output = []\n",
    "    dialog = payload[\"inputs\"][0]\n",
    "    for msg in dialog:\n",
    "        dialog_output.append(f\"**{msg['role'].upper()}**: {msg['content']}\\n\")\n",
    "    dialog_output.append(f\"**{response[0]['generation']['role'].upper()}**: {response[0]['generation']['content']}\")\n",
    "    dialog_output.append(\"\\n---\\n\")\n",
    "    \n",
    "    display(Markdown('\\n'.join(dialog_output)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48b2e09c-a401-4b12-b425-8bc0329290bb",
   "metadata": {},
   "source": [
    "The function below sends your query to the LLM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6218f3a-58c6-4b73-b390-21779fb3c865",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def send_prompt(params, prompt, instruction=\"\"):\n",
    "    \n",
    "    custom_attributes=\"accept_eula=true\"\n",
    "\n",
    "    payload = {\n",
    "        \"inputs\": [[\n",
    "            {\"role\": \"system\", \"content\": instruction},\n",
    "            {\"role\": \"user\", \"content\": prompt},\n",
    "        ]],\n",
    "        \"parameters\": params\n",
    "    }\n",
    "    response = pretrained_predictor.predict(payload, custom_attributes=custom_attributes)\n",
    "    print_dialog(payload, response)\n",
    "    return payload, response"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a73ca8d7-3691-431c-98cd-6714a43a0177",
   "metadata": {},
   "source": [
    "## Prompt Engineering Basics\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd4ba2e2-83fb-4f75-b57e-eb3f9d239ec0",
   "metadata": {},
   "source": [
    "## Basic prompts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac77b873-e54f-4162-9cbd-09602aba78f4",
   "metadata": {},
   "source": [
    "In this lab, we'll delve Prompt Engineering examples that showcase the utility of well-designed prompts, setting the stage for the more complex topics explored in advanced modules.<br>\n",
    "\n",
    "Understanding key principles often becomes clearer when illustrated with real-world examples. In the sections that follow, we demonstrate a variety of tasks made possible through the strategic crafting of prompts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fbd5da7-1e25-44f3-ac22-a2711f0a6f7e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "params = set_llama2_params(temperature=0.5)\n",
    "payload, response = send_prompt(params, prompt=\"The sky is\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1c4c519-eaa0-4ed3-a943-4048537f0880",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "params = set_llama2_params(temperature=0.9)\n",
    "payload, response = send_prompt(params, prompt=\"Translate this sentence to French: I am learning to speak French.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d01bd8b2-8444-46ff-a0ae-47f44df7982c",
   "metadata": {},
   "source": [
    "### Text Summarization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f365ef5b-c024-4a30-b1a6-b50af997a621",
   "metadata": {},
   "source": [
    "One of the key activities in natural language generation involves text summarization, which comes in various forms and contexts. One of the most intriguing capabilities of language models is their skill in distilling lengthy articles or complex ideas into brief, easy-to-grasp summaries. For this exercise, we will delve into the basics of text summarization using tailored prompts.\n",
    "\n",
    "Suppose you wish to familiarize yourself with the age-old tale of \"The Tortoise and the Hare.\" You could start with a prompt like the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bd2b305-7270-4894-aa02-6d3e1181405c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "params = set_llama2_params(temperature=0.7)\n",
    "prompt = \"\"\"The hare was once boasting of his speed before the other animals. \"I have never yet been beaten,\" said he, \"when I put forth my full speed. I challenge any one here to race with me.\" The tortoise said quietly, \"I accept your challenge.\" \"That is a good joke,\" said the hare; \"I could dance round you all the way.\" \"Keep your boasting till you've beaten me,\" answered the tortoise. \"Shall we race?\" So a course was fixed and a start was made. The hare darted almost out of sight at once, but soon stopped and, to show his contempt for the tortoise, lay down to have a nap. The tortoise plodded on and plodded on, and when the hare awoke from his nap, he saw the tortoise just near the winning-post and could not run up in time to save the race.\n",
    "\n",
    "Explain the above in one sentence:\"\"\"\n",
    "payload, response = send_prompt(params, prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "640bfdb4-bc0c-4569-b2db-eec4b3e8aa45",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "params = set_llama2_params(temperature=0.7)\n",
    "prompt = \"\"\"The European Space Agency's Solar Orbiter made its first close pass of the sun in mid-March, getting as close as 48 million miles from the solar surface. The spacecraft used the flyby to calibrate its instruments, making key observations of features like solar flares. Scientists say the data gathered will help them learn more about the sun and how it may impact activity on Earth.\n",
    "\n",
    "Summarize this article in 3 bullet points:\"\"\"\n",
    "payload, response = send_prompt(params, prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6899e56b-023a-40c4-9af3-b864b38bfe1c",
   "metadata": {},
   "source": [
    "### Question Answering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3542361-133e-47eb-a69a-5c98c584bf98",
   "metadata": {},
   "source": [
    "A highly effective method for eliciting precise responses from the model involves refining the structure of the prompt. As previously discussed, a well-designed prompt often amalgamates elements like directives, contextual information, and input-output indicators to yield superior outcomes. While incorporating these elements isn't obligatory, doing so tends to be advantageous; specificity in your instructions is directly correlated with the quality of the results you obtain. The subsequent section offers an illustrative example to demonstrate the impact of a meticulously crafted prompt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec313f7b-c240-4d2f-90b7-a9658965436b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "params = set_llama2_params(temperature=0.7)\n",
    "\n",
    "prompt = \"\"\"Answer the following question based on the context below. Keep the answer short. Respond \"Unsure about answer\" if not sure about the answer.\n",
    "\n",
    "Context: In 1849, thousands of people rushed to California in search of gold and riches. This was known as the California Gold Rush. Prospectors came from all over the world during this time period.\n",
    "\n",
    "Question: What year did the events take place?\"\"\"\n",
    "\n",
    "payload, response = send_prompt(params, prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59380670-1348-4de6-af00-e40c27c7ab1d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "params = set_llama2_params(temperature=0.7)\n",
    "\n",
    "prompt = \"\"\"Local teacher Jane Smith has won the election for mayor of Oakville yesterday. She defeated incumbent mayor Michael Brown in a close race.\n",
    "\n",
    "Question: Who won the election for mayor?\"\"\"\n",
    "\n",
    "payload, response = send_prompt(params, prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc8d0d00-3fe6-45c6-934d-30c9752d4b86",
   "metadata": {},
   "source": [
    "### Text Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2e06fff-bcf7-495d-ae48-5ea4701acdc9",
   "metadata": {},
   "source": [
    "Up to this point, you've given straightforward directives to achieve specific outcomes. However, in your role as a prompt engineer, enhancing the quality of your instructions is imperative. It's not just about better commands; for more complex scenarios, mere instructions won't suffice. This is the juncture where contextual understanding and nuanced elements become crucial. Elements such as [input data] or illustrative [examples] can offer further guidance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0295f1b-03a5-4473-8217-9c83e30bbd38",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "params = set_llama2_params(temperature=0.7)\n",
    "\n",
    "prompt = \"\"\"Classify the text into negative or positive.\n",
    "\n",
    "Text: Apple stock is currently trading at 150 dollars per share. Given Apple's strong financial performance lately with increased iPhone sales and new product launches planned, I predict the stock price will increase to around 160 dollars per share over the next month.\n",
    "\n",
    "Sentiment:\"\"\"\n",
    "\n",
    "payload, response = send_prompt(params, prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f3bb677-2389-4936-94d2-1b755a6d33c2",
   "metadata": {},
   "source": [
    "<br>\n",
    "In the initial attempt, you directed the model to categorize the text, and it appropriately returned 'Positive'. While this response is accurate, suppose you have a requirement to determine what category an article belogs to. How can you accomplish this? Multiple approaches are available, but since you're aiming for high precision, the more detail you incorporate into the prompt, the higher the likelihood of receiving an accurate output. Let's give it another go."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5a8ee0b-c2e1-45bc-af27-81a533e22551",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "params = set_llama2_params(temperature=0.5)\n",
    "\n",
    "prompt = \"\"\"Determine if this article is about \"Technology\", \"Politics\", or \"Business\". \n",
    "\n",
    "Text: The article discussed how social media platforms like Facebook and Twitter are dealing with harmful content and political misinformation leading up to the next US presidential election.\n",
    "Category:\"\"\"\n",
    "\n",
    "payload, response = send_prompt(params, prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "052b3b76-3a16-44a8-b185-829e7f7fabdd",
   "metadata": {},
   "source": [
    "### Role Playing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bae8c7ca-5ad8-4cdb-a5f0-b9f0a76a27ed",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "params = set_llama2_params(temperature=0.5)\n",
    "\n",
    "instruction = \"\"\"You are an AI research assistant. Your tone is technical and scientific.\"\"\"\n",
    "\n",
    "prompt = \"\"\"Human: Hello, who are you?\n",
    "AI: Greeting! I am an AI research assistant. How can I help you today?\n",
    "Human: Can you tell me about the creation of volcanic mountains?\n",
    "AI:\"\"\"\n",
    "\n",
    "payload, response = send_prompt(params, prompt, instruction)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26ef73a5-f0d9-4f02-98e3-eb8e63cc3b6f",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Code Generation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb87c06b-86c6-4f35-ac9c-546ea59d7e76",
   "metadata": {},
   "source": [
    "Code generation involves prompting a model to generate code without providing any examples, relying solely on the model's pre-training. We will test this by giving the model instructions to write code snippets without any demonstrations.\n",
    "\n",
    "The model will attempt to produce valid code based on these descriptions using its implicit knowledge gained during training.\n",
    "\n",
    "Evaluating zero-shot code generation will allow us to test the limits of the model's unaided coding skills for different languages. The results can reveal strengths, gaps, and opportunities to supplement with few-shot examples.\n",
    "\n",
    "This section will provide insights into current capabilities and future work needed to move toward general-purpose AI that can code without extensive training."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71f45fd3-202c-4c1a-8a10-f05d19641038",
   "metadata": {},
   "source": [
    "#### Python code generation:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "636c4657-30b7-49ae-adc3-acdea855e7a2",
   "metadata": {},
   "source": [
    "For Python, we can provide prompts like:\n",
    "- Write a Python function that prints numbers from 1 to 10\n",
    "- Generate Python code to open a file and read the contents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2abf5bc5-9dab-4bf9-9c76-66b806ea818a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "params = set_llama2_params(temperature=0.5)\n",
    "\n",
    "prompt = \"\"\"Generate a Python program that prints the numbers from 1 to 10\"\"\"\n",
    "\n",
    "payload, response = send_prompt(params, prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98204cf9-14f4-4ad7-a8c9-26521bf1e587",
   "metadata": {},
   "source": [
    "#### JavaScript code generation:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7aace4d-657a-42d9-9b46-cb7bbd8f4b7d",
   "metadata": {},
   "source": [
    "For JavaScript:\n",
    "- Write a JavaScript function that returns the maximum value in an array\n",
    "- Generate JavaScript code to create a for loop that prints numbers 1 to 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d82956a9-a957-4437-9bdc-da6a41e01f88",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "params = set_llama2_params(temperature=0.5)\n",
    "\n",
    "prompt = \"\"\"Write a JavaScript function that returns the largest number in an array\"\"\"\n",
    "\n",
    "payload, response = send_prompt(params, prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10b55655-43f0-4139-b2a3-7259440c917c",
   "metadata": {},
   "source": [
    "#### SQL Code generation:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b65f363-d84b-4b14-a6eb-eb078a97ef41",
   "metadata": {},
   "source": [
    "For SQL:\n",
    "- MySQL query to get the title and quantity for all books where the quantity is greater than 100\n",
    "- Generate SQL code to join the \"orders\" and \"products\" tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09bf2c3a-0f02-45ee-a893-59c226d3545d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "params = set_llama2_params(temperature=0.5)\n",
    "\n",
    "prompt = \"\"\"\n",
    "Table books, columns = [BookId, Title, Author]\n",
    "Table inventory, columns = [BookId, Quantity]\n",
    "\n",
    "Create a MySQL query to get the title and quantity for all books where the quantity is greater than 100 and explain the query.\n",
    "\"\"\"\n",
    "\n",
    "payload, response = send_prompt(params, prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7d84d67-278e-4d7f-8125-9c34537b9850",
   "metadata": {},
   "source": [
    "### Reasoning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "850f7deb-1a50-428b-a73d-157789b7a363",
   "metadata": {},
   "source": [
    "Today, one of the most formidable challenges for Large Language Models (LLMs) lies in the domain of reasoning. This area intrigues me significantly, given the intricate applications that could benefit from enhanced reasoning capabilities in LLMs.\n",
    "\n",
    "While there have been strides in the model's mathematical functionalities, it's crucial to underscore that tasks involving reasoning are often stumbling blocks for existing LLMs. Specialized techniques in prompt engineering are imperative to navigate these challenges. While we'll delve into these advanced strategies in an upcoming guide, this lab will provide a primer by walking you through basic examples that demonstrate the model's capabilities in deductive reasoning and logical inferences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04dea650-c616-4911-be69-28f78eceeb55",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "params = set_llama2_params(temperature=0.5)\n",
    "\n",
    "prompt = \"\"\"\n",
    "Given the facts: All men are mortal. Socrates is a man.\n",
    "Use logical reasoning to conclude: Is Socrates mortal? Explain your reasoning.\n",
    "\"\"\"\n",
    "\n",
    "payload, response = send_prompt(params, prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "285f907b-7a20-42fc-9729-071acd86f9f3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "params = set_llama2_params(temperature=0.5)\n",
    "\n",
    "prompt = \"\"\"\n",
    "There are 5 people - Alan, Beth, Cindy, David and Erica. Alan is taller than Beth. Beth is shorter than Cindy. Cindy is taller than David. David is taller than Erica.\n",
    "\n",
    "Who is the tallest? Explain how you arrived at your conclusion.\n",
    "\"\"\"\n",
    "\n",
    "payload, response = send_prompt(params, prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "301bd521-5eb0-4c44-b218-11aa057f7af3",
   "metadata": {},
   "source": [
    "## Advanced Prompting Techniques\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fee61ed-6cbf-4829-81dd-f27a188af26a",
   "metadata": {},
   "source": [
    "### Zero-shot"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "decdf6dd-794d-49ea-a27a-e909d3095c51",
   "metadata": {},
   "source": [
    "Modern large language models like Llama 2 have been optimized to follow instructions and trained on enormous datasets. This enables them to perform certain tasks without any fine-tuning, known as zero-shot learning. Previously, we evaluated some zero-shot prompts. For example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa803866-888d-489b-9ed9-56709bf49925",
   "metadata": {},
   "outputs": [],
   "source": [
    "params = set_llama2_params(temperature=0.9)\n",
    "payload, response = send_prompt(params, prompt=\"Translate this sentence to French: I am learning to speak French.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcbe2dfa-e4b4-4305-a557-f324970d92d6",
   "metadata": {},
   "source": [
    "Although Large Language Models (LLMs) exhibit impressive abilities in zero-shot scenarios, their performance can falter when tackling more intricate tasks within that context. To ameliorate this, the concept of few-shot prompting comes into play. This technique facilitates in-context learning by incorporating example-based guidance directly into the prompt, thereby enhancing the model's output accuracy. These examples act as a form of conditioning that influences the model's responses in subsequent instances.\n",
    "\n",
    "For illustrative purposes, let's delve into a hands-on example of few-shot prompting. In this exercise, the objective is to accurately incorporate a novel term into a sentence."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92d13399-340a-4120-9e73-81de1f5e0cb7",
   "metadata": {},
   "source": [
    "### Few-shot prompts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "057a326a-af7c-4d89-8ba7-eda4ed78967d",
   "metadata": {},
   "source": [
    "Though large language models can perform impressively without training, their zero-shot abilities still have limitations on more difficult tasks. Few-shot prompting enhances in-context learning by supplying model demonstrations directly in the prompt. These examples provide conditioning to guide the model's response for new inputs. As described by [Brown et al. (2020)](https://arxiv.org/abs/2005.14165), few-shot prompting can be applied to tasks like properly using novel words in sentences. With just a couple demonstrations, the model can acquire new concepts and skills without full training. This technique harnesses the few-shot capabilities of large models to achieve greater generalization and reasoning from small amounts of data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1069aa0c-21f3-4ef1-9988-2df50f70d8fc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "params = set_llama2_params(temperature=0.9)\n",
    "\n",
    "prompt = \"\"\"A \"blicket\" is a tool used for farming. An example of a sentence that uses the word blicket is:\n",
    "The farmer used a blicket to dig holes and plant seeds. \n",
    "\n",
    "\"Flooping\" refers to a dance move where you spin around. An example of a sentence that uses the word Flooping is:\"\"\"\n",
    "\n",
    "payload, response = send_prompt(params, prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a70a14a6-31de-4e79-852f-feafe6c024c8",
   "metadata": {},
   "source": [
    "You'll notice that the LLM has the ability to grasp the task with just a single example, commonly known as 1-shot learning. For tasks that are more challenging, the lab allows you to incrementally scale the number of examples or \"shots\" (such as 3-shot, 5-shot, or even 10-shot) to experiment with improving the model's performance. Below we leveage a 5-shot Few-shot prompt to create a short story."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4596d26f-b874-4af8-8983-a2e843859714",
   "metadata": {},
   "outputs": [],
   "source": [
    "params = set_llama2_params(temperature=0.5)\n",
    "\n",
    "prompt = \"\"\"\n",
    "A \"blonset\" is a tool used for cutting metal.\n",
    "An example sentence is: The blacksmith used a blonset to shape the horseshoe.\n",
    "\n",
    "A \"fendle\" is a vegetable that grows underground.\n",
    "An example sentence is: She pulled fendles from the garden to make soup.\n",
    "\n",
    "\"Vixting\" means climbing a tree very quickly.\n",
    "An example sentence is: The energetic monkeys vixted up the tall tree trunks.\n",
    "\n",
    "\"Zugging\" refers to a sport played with a small ball.\n",
    "An example sentence is: We had fun zugging the ball back and forth across the field.\n",
    "\n",
    "A \"crigit\" is a small furry pet.\n",
    "Create a short story that uses all 5 words:\"\"\"\n",
    "\n",
    "payload, response = send_prompt(params, prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e840c09-62cc-4c67-972a-b6c391d608aa",
   "metadata": {},
   "source": [
    "### Chain-of-Thought (CoT) Prompting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f86b2ee-1f83-4e30-80a8-221901b2028e",
   "metadata": {},
   "source": [
    "Chain-of-thought (CoT) prompting, proposed by [Wei et al. (2022)](https://arxiv.org/abs/2201.11903), is a technique that facilitates complex reasoning in language models by having them show intermediate steps. This method can be used with few-shot prompting, where just a few examples provide the context. The combination enables improved performance on challenging tasks that involve reasoning through multiple steps before generating a final response. By eliciting the reasoning process explicitly, CoT prompting aims to develop stronger logical thinking and rationality in language models when applying them to complex inferential problems with limited data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2812f0d5-7cad-4ce2-ac1f-c9b52ed2000d",
   "metadata": {},
   "source": [
    "### Zero-shot CoT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed755b9b-78fc-4693-9eaa-0ba6a9f5481f",
   "metadata": {},
   "source": [
    "A new approach called zero-shot chain-of-thought prompting was recently proposed by [Kojima et al. (2022)](https://arxiv.org/abs/2205.11916). This technique involves adding the phrase \"Let's think step by step\" to prompts to encourage the model to show its reasoning. We can test this method on a simple problem to see how well the model explains its logical thinking process. By explicitly cueing the model to demonstrate step-by-step reasoning, zero-shot chain-of-thought prompting aims to improve transparency and understandability without requiring training on reasoning demonstrations. This emerging technique represents an interesting way to potentially enhance rationality in large language models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8587771-1119-4020-8465-0ebf6c6c0244",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "params = set_llama2_params(temperature=0.5)\n",
    "\n",
    "prompt = \"\"\"Let's think step-by-step.\n",
    "If a standard deck of 52 playing cards has 4 suits (Hearts, Diamonds, Clubs and Spades) with 13 cards in each suit, how many total face cards (Jack, Queen, King) are there?\n",
    "Please demonstrate the reasoning.\n",
    "A:\"\"\"\n",
    "\n",
    "payload, response = send_prompt(params, prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bec1030-33cf-4645-a3ec-380271e08a1f",
   "metadata": {},
   "source": [
    "### Few-Shot CoT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bc675e3-957c-4405-a43b-0622581ca72d",
   "metadata": {},
   "source": [
    "Few-shot Chain of Thought (CoT) prompting is a technique that combines few-shot learning with intermediate reasoning steps. In few-shot learning, just a small number of examples or \"shots\" are provided to the model to demonstrate the desired behavior. Chain-of-thought prompting has the model show its step-by-step reasoning process explicitly.\n",
    "\n",
    "In Few-shot CoT, we give the model a couple of examples that demonstrate both the target skill and the reasoning chain. This provides the model with the context needed to apply similar skills and reasoning processes to new situations.\n",
    "\n",
    "In the example below we show the model a example of a multi-step math problem with reasoning steps:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1169747f-20d5-42b6-bcc7-f38410a2bc19",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "params = set_llama2_params(temperature=0.5)\n",
    "\n",
    "prompt = \"\"\"Let's think through this:\n",
    "If there were 6 oranges originally and 4 were peeled, how many unpeeled oranges are left?\n",
    "Step 1) Originally there were 6 oranges\n",
    "Step 2) 4 oranges were peeled\n",
    "Step 3) So there must be 6 - 4 = 2 unpeeled oranges left\n",
    "\n",
    "Let's think step-by-step:\n",
    "\n",
    "If David had 9 cakes and ate 4 of them, how many are left?\"\"\"\n",
    "\n",
    "payload, response = send_prompt(params, prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Few-Shot CoT with LangChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cabf06fe-d24f-465a-a645-f7839811dd1a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain import PromptTemplate\n",
    "from langchain.prompts.few_shot import FewShotPromptTemplate\n",
    "from langchain.llms import SagemakerEndpoint\n",
    "from langchain.llms.sagemaker_endpoint import LLMContentHandler\n",
    "from langchain.chains import LLMChain\n",
    "\n",
    "class ContentHandler(LLMContentHandler):\n",
    "    content_type = \"application/json\"\n",
    "    accepts = \"application/json\"\n",
    "\n",
    "    def transform_input(self, prompt, model_kwargs):\n",
    "        input_str = json.dumps({\"inputs\" : [[\n",
    "        {\"role\" : \"user\", \"content\" : prompt}]],\n",
    "        \"parameters\" : {**model_kwargs}})\n",
    "        return input_str.encode('utf-8')\n",
    "    \n",
    "    def transform_output(self, output):\n",
    "        response_json = json.loads(output.read().decode(\"utf-8\"))\n",
    "        return response_json[0][\"generation\"][\"content\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac22ba72-d651-4af9-9f96-c92cce4f8fae",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import json\n",
    "from sagemaker import session\n",
    "\n",
    "custom_attribute = 'accept_eula=true'\n",
    "\n",
    "content_handler = ContentHandler()\n",
    "\n",
    "llm=SagemakerEndpoint(\n",
    "     endpoint_name=pretrained_predictor.endpoint_name, \n",
    "     region_name=session.Session().boto_region_name, \n",
    "     model_kwargs={\"max_new_tokens\": 700, \"top_p\": 0.9, \"temperature\": 0.6},\n",
    "     endpoint_kwargs={\"CustomAttributes\": custom_attribute},\n",
    "     content_handler=content_handler\n",
    " )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e86f8bcc-921a-434a-b8ac-4a0ed2eeb57e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# create our examples\n",
    "examples = [\n",
    "    {\n",
    "        \"query\": \"How are you?\",\n",
    "        \"answer\": \"I can't complain but sometimes I still do.\"\n",
    "    }, {\n",
    "        \"query\": \"What time is it?\",\n",
    "        \"answer\": \"It's time to get a watch.\"\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec85f49f-06cc-446b-90ab-6dc64900c563",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# create a example template\n",
    "example_template = \"\"\"\n",
    "User: {query}\n",
    "AI: {answer}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5eb9dbdf-4809-4691-bbfe-4c0832757ee1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# create a prompt example from above template\n",
    "example_prompt = PromptTemplate(\n",
    "    input_variables=[\"query\", \"answer\"],\n",
    "    template=example_template\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad033dc3-ee2f-45d8-9f09-cc1356ee4637",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# now break our previous prompt into a prefix and suffix\n",
    "# the prefix is our instructions\n",
    "prefix = \"\"\"\n",
    "The following are exerpts from conversations with an AI\n",
    "assistant. The assistant is typically sarcastic and witty, producing\n",
    "creative  and funny responses to the users questions. Here are some\n",
    "examples: \n",
    "\"\"\"\n",
    "# and the suffix our user input and output indicator\n",
    "suffix = \"\"\"\n",
    "User: {query}\n",
    "AI: \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc97de13-f808-4bc8-b638-e0e64b2a2980",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# now create the few shot prompt template\n",
    "few_shot_prompt_template = FewShotPromptTemplate(\n",
    "    examples=examples,\n",
    "    example_prompt=example_prompt,\n",
    "    prefix=prefix,\n",
    "    suffix=suffix,\n",
    "    input_variables=[\"query\"],\n",
    "    example_separator=\"\\n\\n\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "207bc893-6732-4c5f-86c3-b42033402e71",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "query = \"What is the meaning of life?\"\n",
    "\n",
    "print(few_shot_prompt_template.format(query=query))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb9b7076-7914-4862-bddc-48c6c427aa83",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "llm_chain = LLMChain(\n",
    "    llm=llm,\n",
    "    prompt=few_shot_prompt_template\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a76ec8dd-0e06-447f-b5d1-b0b433405963",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "response = llm_chain.run(\n",
    "    query=\"What is the meaning of life?\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9c0dd27-0286-4161-9e52-7f44e6a716a1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(response.strip())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f0a0d94-713d-4253-9291-9cbc20d37680",
   "metadata": {},
   "source": [
    "## Clean Up"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b0a5612-7f5a-4a27-9ef9-5d9061430323",
   "metadata": {},
   "source": [
    "Once you have finished the lab exercises, you can terminate the model and SageMaker endpoint by running the code below. This will stop any further charges from accumulating for these resources. It is recommended to shut down the endpoint and delete the model at the end of the lab session, as they continue accruing charges when left running."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3717b3d-4258-43e2-94fd-2aa0272741a2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#pretrained_predictor.delete_model()\n",
    "#pretrained_predictor.delete_endpoint()"
   ]
  }
 ],
 "metadata": {
  "availableInstances": [
   {
    "_defaultOrder": 0,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.t3.medium",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 1,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.t3.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 2,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.t3.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 3,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.t3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 4,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 5,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 6,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 7,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 8,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 9,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 10,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 11,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 12,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5d.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 13,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5d.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 14,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5d.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 15,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5d.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 16,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5d.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 17,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5d.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 18,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5d.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 19,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 20,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": true,
    "memoryGiB": 0,
    "name": "ml.geospatial.interactive",
    "supportedImageNames": [
     "sagemaker-geospatial-v1-0"
    ],
    "vcpuNum": 0
   },
   {
    "_defaultOrder": 21,
    "_isFastLaunch": true,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.c5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 22,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.c5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 23,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.c5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 24,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.c5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 25,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 72,
    "name": "ml.c5.9xlarge",
    "vcpuNum": 36
   },
   {
    "_defaultOrder": 26,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 96,
    "name": "ml.c5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 27,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 144,
    "name": "ml.c5.18xlarge",
    "vcpuNum": 72
   },
   {
    "_defaultOrder": 28,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.c5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 29,
    "_isFastLaunch": true,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g4dn.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 30,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g4dn.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 31,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g4dn.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 32,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g4dn.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 33,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g4dn.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 34,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g4dn.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 35,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 61,
    "name": "ml.p3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 36,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 244,
    "name": "ml.p3.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 37,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 488,
    "name": "ml.p3.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 38,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.p3dn.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 39,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.r5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 40,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.r5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 41,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.r5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 42,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.r5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 43,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.r5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 44,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.r5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 45,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.r5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 46,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.r5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 47,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 48,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 49,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 50,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 51,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 52,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 53,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.g5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 54,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.g5.48xlarge",
    "vcpuNum": 192
   },
   {
    "_defaultOrder": 55,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 56,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4de.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 57,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.trn1.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 58,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.trn1.32xlarge",
    "vcpuNum": 128
   },
   {
    "_defaultOrder": 59,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.trn1n.32xlarge",
    "vcpuNum": 128
   }
  ],
  "instance_type": "ml.m5.large",
  "kernelspec": {
   "display_name": "Python 3 (Data Science 3.0)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-west-2:236514542706:image/sagemaker-data-science-310-v1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
