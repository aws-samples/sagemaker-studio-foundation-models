{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 216,
   "id": "dfee18e7-7ddd-4763-95bb-cc82cef6607c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "\u001b[31mERROR: Could not find a version that satisfies the requirement ChatAnyscale (from versions: none)\u001b[0m\u001b[31m\n",
      "\u001b[0m\u001b[31mERROR: No matching distribution found for ChatAnyscale\u001b[0m\u001b[31m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.1.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.2.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!python3 -m pip install llama_index ChatAnyscale chromadb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2109d131-7e24-4fde-9b7b-4ce461772d4d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import gc\n",
    "from threading import Thread\n",
    "import torch\n",
    "from transformers import (\n",
    "    AutoTokenizer, \n",
    "    AutoModelForCausalLM,\n",
    "    LlamaTokenizer,\n",
    "    BitsAndBytesConfig,\n",
    "    pipeline\n",
    ")\n",
    "from transformers import (\n",
    "    GenerationConfig, \n",
    "    TextIteratorStreamer,\n",
    "    TextStreamer\n",
    ")\n",
    "from transformers import pipeline\n",
    "from langchain.llms import HuggingFacePipeline\n",
    "from langchain import PromptTemplate, LLMChain\n",
    "from langchain import PromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "60084a92-f88c-45db-999e-933ba9c1f415",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# https://huggingface.co/NousResearch/Nous-Hermes-Llama2-13b\n",
    "MODEL_ID = \"NousResearch/Nous-Hermes-Llama2-13b\"\n",
    "# MODEL_ID = \"tiiuae/falcon-40b-instruct\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "57ca5457-b162-4530-afe8-e12d1f569e9f",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The model weights are not tied. Please use the `tie_weights` method before using the `infer_auto_device` function.\n",
      "Loading checkpoint shards: 100%|██████████| 3/3 [00:16<00:00,  5.40s/it]\n",
      "/opt/conda/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:362: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:367: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# quantization config using BnB\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16\n",
    ")\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_ID, \n",
    "    load_in_8bit=True, \n",
    "    # quantization_config=bnb_config,\n",
    "    # trust_remote_code=True,\n",
    "    torch_dtype=torch.float16, \n",
    "    device_map=\"auto\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "05c0c785-42b5-4212-8869-26db70d384dd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    MODEL_ID\n",
    ")\n",
    "tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "519a6dfe-eea5-4dd8-8bfa-fe597ac06b60",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def generate_alpaca_like_prompt(user_question, user_context):\n",
    "    \"\"\"\n",
    "    Generates a dolly Like prompt for model to respond with context \n",
    "    \"\"\"\n",
    "    instruction = f\"### Instruction:\\n{user_question}\"\n",
    "    context = f\"### Input:\\n{user_context}\" if user_context else None\n",
    "    response = f\"### Response:\\n\"\n",
    "\n",
    "    prompt = \"\\n\\n\".join([i for i in [instruction, context, response] if i is not None])\n",
    "\n",
    "    return prompt\n",
    "\n",
    "def tokenize(tokenizer, prompt):\n",
    "    \"\"\" \n",
    "    Tokenize your input prompt to provide as an input \n",
    "    to the model\n",
    "    \"\"\"\n",
    "    tokenized = tokenizer(prompt, return_tensors=\"pt\")\n",
    "    input_ids = tokenized.input_ids\n",
    "    input_ids = input_ids.to(model.device)\n",
    "    \n",
    "    return tokenized, input_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "f3cf14bc-dd12-4873-b79e-19d0e685d4e5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### Instruction:\n",
      "\n",
      "You are an assistant for question-answering tasks. You are helpful and friendly. Only answer the question using the additonal context below and if you don't see context, you just say I don't know. Use three sentences maximum, keep the answer concise and try to answer using enumeration.\n",
      "\n",
      "How is UK home office driving down crime in 2023?\n",
      "\n",
      "\n",
      "### Response:\n",
      "\n"
     ]
    }
   ],
   "source": [
    "user_question = \"How is UK home office driving down crime in 2023?\"\n",
    "\n",
    "prompt_qna_only = \"Answer the question below truthfully, briefly and polietly to the best of your knowledge. If you don't know the answer, say 'I don't know'. Answer in 2 sentences or less.\"\n",
    "\n",
    "prompt_rag_only = \"You are an assistant for question-answering tasks. You are helpful and friendly. Only answer the question using the additonal context below and if you don't see context, you just say I don't know. Use three sentences maximum, keep the answer concise and try to answer using enumeration.\"\n",
    "\n",
    "question_prompt = f\"\"\"\n",
    "{prompt_rag_only}\n",
    "\n",
    "{user_question}\n",
    "\"\"\"\n",
    "user_context = None\n",
    "\n",
    "prompt = generate_alpaca_like_prompt(\n",
    "    user_question=question_prompt, \n",
    "    user_context=user_context\n",
    ")\n",
    "print(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "fd6ced8d-cc07-49ef-9d47-be6dcf59ecc5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "_, input_ids = tokenize(\n",
    "    tokenizer=tokenizer, \n",
    "    prompt=prompt\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "7258c469-9a23-4917-a402-f1e1f3d01fea",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Setup the text streamer \n",
    "streamer = TextStreamer(tokenizer, skip_prompt=False, skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e7826796-62d9-4ad6-b285-f7f65bf51907",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/transformers/generation/utils.py:1417: UserWarning: You have modified the pretrained model configuration to control generation. This is a deprecated strategy to control generation and will be removed soon, in a future version. Please use a generation configuration file (see https://huggingface.co/docs/transformers/main_classes/text_generation )\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### Instruction:\n",
      "\n",
      "You are an assistant for question-answering tasks. You are helpful and friendly. Only answer the question using the additonal context below and if you don't see context, you just say I don't know. Use three sentences maximum, keep the answer concise and try to answer using enumeration.\n",
      "\n",
      "How is UK home office driving down crime in 2023?\n",
      "\n",
      "\n",
      "### Response:\n",
      "The UK Home Office is driving down crime in 2023 through a multi-faceted approach that includes increasing police presence, investing in community-based initiatives, and implementing tougher sentencing policies. By focusing on prevention, early intervention, and enforcement, the Home Office aims to reduce crime rates and make communities safer.\n"
     ]
    }
   ],
   "source": [
    "with torch.inference_mode():\n",
    "    outputs = model.generate(\n",
    "        input_ids=input_ids,\n",
    "        temperature=0.01,\n",
    "        top_p=0.95,\n",
    "        max_new_tokens=512,\n",
    "        streamer=streamer, \n",
    "        do_sample=True,\n",
    "        use_cache=False\n",
    "    )\n",
    "    output_text = tokenizer.decode(outputs[0], skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d4d204a-eccc-4207-b388-428fda524c94",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Llama Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "989d1f13-733e-46ef-9edf-74715b4b35a3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # Import the prompt wrapper...but for llama index\n",
    "# from llama_index.prompts.prompts import SimpleInputPrompt\n",
    "# # Create a system prompt \n",
    "# system_prompt = \"\"\"[INST] <>\n",
    "# You are a helpful, respectful and honest assistant. Always answer as \n",
    "# helpfully as possible, while being safe. Your answers should not include\n",
    "# any harmful, unethical, racist, sexist, toxic, dangerous, or illegal content.\n",
    "# Please ensure that your responses are socially unbiased and positive in nature.\n",
    "\n",
    "# If a question does not make any sense, or is not factually coherent, explain \n",
    "# why instead of answering something not correct. If you don't know the answer \n",
    "# to a question, please don't share false information.\n",
    "\n",
    "# Your goal is to provide answers relating to the financial performance of \n",
    "# the company.<>\n",
    "# \"\"\"\n",
    "# # Throw together the query wrapper\n",
    "# query_wrapper_prompt = SimpleInputPrompt(\"{query_str} [/INST]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "6efd0d68-dad6-439c-b5e4-9ed2e6aeabea",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import PromptTemplate\n",
    "\n",
    "# template_old = \"\"\"\n",
    "# <s>[INST] <<SYS>>\\nYou are an assistant for question-answering tasks. You are helpful and friendly. Only answer the question and do not provide additional information. If you don't know the answer, you just say I don't know. Use three sentences maximum and keep the answer concise.\n",
    "# <<SYS>>\\n\n",
    "# {context}\\n\n",
    "# {question} [/INST]\"\"\"\n",
    "\n",
    "template = \"\"\"\n",
    "### Instruction:\n",
    "\n",
    "You are an assistant for question-answering tasks. You are helpful and friendly. Only answer the question using the context below and if you don't see context, you just say I don't know. Use three sentences maximum, keep the answer concise and try to answer using enumeration.\n",
    "\n",
    "{question}\n",
    "\n",
    "### Input:\n",
    "{context}\n",
    "\n",
    "### Response:\n",
    "\"\"\"\n",
    "prompt_template = PromptTemplate(template=template, input_variables=['context','question'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "66a12c97-b6f5-493b-92c2-28ee68519b94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "### Instruction:\n",
      "\n",
      "You are an assistant for question-answering tasks. You are helpful and friendly. Only answer the question using the context below and if you don't see context, you just say I don't know. Use three sentences maximum, keep the answer concise and try to answer using enumeration.\n",
      "\n",
      "{question}\n",
      "\n",
      "### Input:\n",
      "{context}\n",
      "\n",
      "### Response:\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(prompt_template.template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "f5ddf860-4ecf-48cb-8408-5faf395f218b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pipe = pipeline(\n",
    "    task='text-generation',\n",
    "    model=model, \n",
    "    tokenizer=tokenizer,\n",
    "    temperature=0.8,\n",
    "    top_p=0.95,\n",
    "    max_new_tokens=512,\n",
    "    do_sample=True,\n",
    "    use_cache=False,\n",
    "    num_return_sequences=1,\n",
    "    eos_token_id=tokenizer.eos_token_id,\n",
    "    pad_token_id=tokenizer.eos_token_id,\n",
    "    streamer=TextStreamer(\n",
    "        tokenizer, \n",
    "        skip_prompt=True, \n",
    "        skip_special_tokens=True\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "95435895-ed09-4c4e-93bd-c244a490030e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain.chains import LLMChain\n",
    "from langchain.llms import HuggingFacePipeline\n",
    "\n",
    "llm=HuggingFacePipeline(\n",
    "    pipeline=pipe\n",
    ")\n",
    "\n",
    "llm_chain = LLMChain(\n",
    "    llm=llm, \n",
    "    prompt=prompt_template\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b29e9c9d-c048-4a59-a4c5-62753515a18c",
   "metadata": {},
   "source": [
    "## Doc Download and Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "7ecfb4af-7b17-4c80-b1ce-a8b1952b2e20",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import chromadb\n",
    "from langchain.vectorstores import Chroma\n",
    "import numpy as np\n",
    "from langchain.text_splitter import CharacterTextSplitter, RecursiveCharacterTextSplitter\n",
    "from langchain.document_loaders import PyPDFLoader, PyPDFDirectoryLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "56c58aff-ef09-4767-bbe3-a3bd415c2e44",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "loader = PyPDFDirectoryLoader(\"./documents/bmw/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "c3ee56e5-23d3-4be4-b38b-39acffd0bba2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "documents = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "4e14accf-5d8e-4b6d-9478-b35f5eec6488",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1024,\n",
    "    chunk_overlap=64,\n",
    ")\n",
    "docs = text_splitter.split_documents(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "6b4b7c47-33f5-478a-9558-143f515cfa28",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average length among 248 documents loaded is 1687 characters.\n",
      "After the split we have 548 documents more than the original 248.\n",
      "Average length among 548 documents (after split) is 788 characters.\n"
     ]
    }
   ],
   "source": [
    "avg_doc_length = lambda documents: sum([len(doc.page_content) for doc in documents])//len(documents)\n",
    "avg_char_count_pre = avg_doc_length(documents)\n",
    "avg_char_count_post = avg_doc_length(docs)\n",
    "print(f'Average length among {len(documents)} documents loaded is {avg_char_count_pre} characters.')\n",
    "print(f'After the split we have {len(docs)} documents more than the original {len(documents)}.')\n",
    "print(f'Average length among {len(docs)} documents (after split) is {avg_char_count_post} characters.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c944d9f0-5e2f-4cc7-9be6-15e3b2b13396",
   "metadata": {},
   "source": [
    "## Generate Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "f629d874-6ef4-436b-87dd-e7574001e2e8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "HuggingFaceBgeEmbeddings(client=SentenceTransformer(\n",
       "  (0): Transformer({'max_seq_length': 512, 'do_lower_case': True}) with Transformer model: BertModel \n",
       "  (1): Pooling({'word_embedding_dimension': 384, 'pooling_mode_cls_token': True, 'pooling_mode_mean_tokens': False, 'pooling_mode_max_tokens': False, 'pooling_mode_mean_sqrt_len_tokens': False})\n",
       "), model_name='BAAI/bge-small-en', cache_folder=None, model_kwargs={'device': 'cuda'}, encode_kwargs={'normalize_embeddings': False}, query_instruction='Represent this question for searching relevant passages: ')"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.embeddings import HuggingFaceBgeEmbeddings\n",
    "\n",
    "model_name = \"BAAI/bge-small-en\"\n",
    "model_kwargs = {'device': 'cuda'}\n",
    "encode_kwargs = {'normalize_embeddings': False}\n",
    "embeddings = HuggingFaceBgeEmbeddings(\n",
    "    model_name=model_name,\n",
    "    model_kwargs=model_kwargs,\n",
    "    encode_kwargs=encode_kwargs\n",
    ")\n",
    "embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "33c14460-c212-4c27-9827-46b80e927a99",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of the embedding:  384\n"
     ]
    }
   ],
   "source": [
    "sample_embedding = np.array(embeddings.embed_query(docs[0].page_content))\n",
    "print(\"Size of the embedding: \", sample_embedding.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "c531c6c6-5fe9-46b4-98bf-9b6e0112c65e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The LED lights up if the safety function is\n",
      "switched on.\n",
      "Safety switch for rear operation\n",
      "Press the safety switch when transport‐\n",
      "ing children in the rear; otherwise, injury may\n",
      "result if the windows are closed without super‐\n",
      "vision.◀\n",
      "Roller sunblinds\n",
      "Roller sunblind for rear window\n",
      "General information\n",
      "If you are no longer able to move the roller sun‐\n",
      "blind for the rear window after having activated\n",
      "it a number of times in a row, the system is\n",
      "blocked for a limited time to prevent overheat‐\n",
      "ing. Let the system cool.\n",
      "The roller sunblind for the rear window cannot\n",
      "be moved at low interior temperatures.\n",
      "Extending or retracting roller blind for\n",
      "rear window\n",
      "Press the button.\n",
      "Roller sunblinds for the rear side\n",
      "windows\n",
      "Pull out the roller sunblind at the loop and hook\n",
      "it onto the bracket.\n",
      "Do not open the window while the roller\n",
      "sunblind is raised.\n",
      "Do not open the window while the roller sun‐\n",
      "blind is raised; otherwise, there is a risk of\n",
      "damage at high speeds that may result in per‐\n",
      "sonal injury.◀\n"
     ]
    }
   ],
   "source": [
    "print(docs[100].page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "b8e3ad86-babe-4136-9aa6-9f0130622259",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "db = Chroma.from_documents(docs, embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "56bc567e-1ce4-4f11-8d36-f1e5b2427218",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'db' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43mdb\u001b[49m\u001b[38;5;241m.\u001b[39msimilarity_search_with_score(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWhat is the recommended tire air pressure?\u001b[39m\u001b[38;5;124m\"\u001b[39m, k\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'db' is not defined"
     ]
    }
   ],
   "source": [
    "result = db.similarity_search_with_score(\"What is the recommended tire air pressure?\", k=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "218c9bd3-a924-40e3-ab18-39cadc28d297",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Profile management\n",
      "Opening the profiles\n",
      "A different profile can be called up than the\n",
      "one associated with the remote control cur‐\n",
      "rently in use.\n",
      "1.\"Settings\"\n",
      "2.\"Profiles\"\n",
      "3.Select a profile.\n",
      "Called up profile is assigned to the remote\n",
      "control being used at the time.\n",
      "Renaming profiles\n",
      "1.\"Settings\"\n",
      "2.\"Profiles\"\n",
      "The current profile is selected.\n",
      "3.Open \"Options\".\n",
      "4.\"Rename current profile\"\n",
      "Resetting profiles\n",
      "The settings of the active profile are reset to\n",
      "their default values.\n",
      "1.\"Settings\"\n",
      "2.\"Profiles\"\n",
      "The current profile is selected.\n",
      "3.Open \"Options\".\n",
      "4.\"Reset current profile\"\n",
      "Importing profiles\n",
      "Existing settings and contacts are overwritten\n",
      "with the imported profile.\n",
      "1.\"Settings\"\n",
      "2.\"Profiles\"\n",
      "3.\"Import profile\"\n",
      "4.BMW Online: \"BMW Online\"\n",
      "USB interface: \"USB device\"Exporting profiles\n",
      "Most settings of the active profile and the\n",
      "saved contacts can be exported.\n",
      "This can be helpful for securing and retrieving\n",
      "personal settings, before delivering the vehicle\n",
      "to a workshop for example.\n",
      "1.\"Settings\"\n",
      "2.\"Profiles\"\n"
     ]
    }
   ],
   "source": [
    "print(result[0][0].page_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0712cb71-01f7-401b-854c-b38f6643af98",
   "metadata": {},
   "source": [
    "## Use RetrivalQA Chain to talk to your Documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "a91e384b-287c-434a-89c0-4d05755afeb5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain.chains import RetrievalQA\n",
    "from langchain.prompts import PromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "bcf6f5fe-3252-4d03-93dd-8d8cdc1aedec",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "llm_qa_chain = RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    chain_type='stuff',\n",
    "    retriever=db.as_retriever(\n",
    "        search_kwargs={\"k\": 2}\n",
    "    ),\n",
    "    return_source_documents=True,\n",
    "    chain_type_kwargs={\n",
    "        \"prompt\": prompt_template\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "31b06a88-2e1b-45f9-94b2-798e0663ef03",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "query = \"What are the safety features of the car?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "7dfa563b-c833-4995-862c-4c1e0d371957",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The safety features of the car include Hill start assistant, Drive-off assistant, Hints, Holder for beverages, Homepage, Hood, Horn, Hotel function, trunk lid, Hot exhaust system, HUD Head-up Display, Hydroplaning, Ignition key, Ignition off, Ignition on, Indication of a flat tire, Indicator and warning lamps, Individual air distribution, Individual settings, Instrument cluster, Instrument cluster, electronic displays, Instrument lighting, Integrated key, Ice warning, Icy roads, Identification marks, tires, Identification number, Important features in the engine compartment, iDrive, Inflation pressure, tires, Inflation pressure warning FTM, Info display, Initialize, Tire Pressure Monitor TPM, Initializing, Flat Tire Monitor FTM, and many more.\n"
     ]
    }
   ],
   "source": [
    "response = llm_qa_chain(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "f8bd9348-e27e-462d-b341-4ccb92bd592b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "query = \"What is the approximate weight of the car?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "4045d697-482d-40a6-898f-46f86291bd9e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I don't know the approximate weight of the car.\n"
     ]
    }
   ],
   "source": [
    "response = llm_qa_chain(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "f9ec8824-015d-401c-9776-9607bf287998",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "query = \"What is Blind Spot Detection?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "c33643bb-d1e1-4f63-85b7-6a26cc306d67",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Active Blind Spot Detection is a system that uses radar sensors to monitor the area behind and next to a vehicle while it is in motion at speeds above 30 mph/50 km/h. The system indicates whether there are vehicles in the blind spot or approaching from behind on the adjacent lane. If the driver attempts to change lanes after setting the turn signal, the system issues a warning if there is a vehicle in the blind spot or approaching from behind. The lamp in the exterior mirror housing lights up dimly, and the steering wheel vibrates to alert the driver. The personal responsibility of the driver is noted, as the system is not a substitute for the driver's personal judgment of the traffic situation.\n"
     ]
    }
   ],
   "source": [
    "response = llm_qa_chain(query)"
   ]
  }
 ],
 "metadata": {
  "availableInstances": [
   {
    "_defaultOrder": 0,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.t3.medium",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 1,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.t3.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 2,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.t3.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 3,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.t3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 4,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 5,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 6,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 7,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 8,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 9,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 10,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 11,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 12,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5d.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 13,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5d.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 14,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5d.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 15,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5d.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 16,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5d.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 17,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5d.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 18,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5d.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 19,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 20,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": true,
    "memoryGiB": 0,
    "name": "ml.geospatial.interactive",
    "supportedImageNames": [
     "sagemaker-geospatial-v1-0"
    ],
    "vcpuNum": 0
   },
   {
    "_defaultOrder": 21,
    "_isFastLaunch": true,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.c5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 22,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.c5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 23,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.c5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 24,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.c5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 25,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 72,
    "name": "ml.c5.9xlarge",
    "vcpuNum": 36
   },
   {
    "_defaultOrder": 26,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 96,
    "name": "ml.c5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 27,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 144,
    "name": "ml.c5.18xlarge",
    "vcpuNum": 72
   },
   {
    "_defaultOrder": 28,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.c5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 29,
    "_isFastLaunch": true,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g4dn.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 30,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g4dn.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 31,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g4dn.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 32,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g4dn.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 33,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g4dn.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 34,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g4dn.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 35,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 61,
    "name": "ml.p3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 36,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 244,
    "name": "ml.p3.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 37,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 488,
    "name": "ml.p3.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 38,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.p3dn.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 39,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.r5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 40,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.r5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 41,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.r5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 42,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.r5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 43,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.r5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 44,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.r5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 45,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.r5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 46,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.r5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 47,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 48,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 49,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 50,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 51,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 52,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 53,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.g5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 54,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.g5.48xlarge",
    "vcpuNum": 192
   },
   {
    "_defaultOrder": 55,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 56,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4de.24xlarge",
    "vcpuNum": 96
   }
  ],
  "kernelspec": {
   "display_name": "Python 3 (PyTorch 2.0.0 Python 3.10 GPU Optimized)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-east-1:081325390199:image/pytorch-2.0.0-gpu-py310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
