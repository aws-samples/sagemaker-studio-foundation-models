{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fc74abb9-e8c3-4305-ba35-fd4519548a2b",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #FFDDDD; border-left: 5px solid red; padding: 10px; color: black;\">\n",
    "    <strong>Kernel: PySpark\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6268e06-20fb-4ad7-bd47-5d51f7e59252",
   "metadata": {},
   "source": [
    "## Lab 03. Build Retrival Augmented Generation System using Amazon EMR Spark Distributed Processing and OpeSearch Vector Database\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Contents\n",
    "\n",
    "- [Overview](#overview)\n",
    "- [Connect to an Existing EMR Cluster](#connect-to-an-existing-emr-cluster)\n",
    "- [Upload Files from Local to S3](#upload-files-from-local-to-s3)\n",
    "- [Convert PDF to Text](#convert-pdf-to-text)\n",
    "- [Run Parallelized Embeddings using Amazon EMR (EC2 Spark based Processing)](#run-parallelized-embeddings-using-amazon-emr-ec2-spark-based-processing)\n",
    "- [Putting it All Together](#putting-it-all-together)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a4c7006-3446-41a6-a41a-d76875062d70",
   "metadata": {},
   "source": [
    "In this notebook we demonstrate how you can build a Retrival Augmented Generation System using the following components,\n",
    "1. Embedding Model: `BAAI/bge-base-en-v1.5`\n",
    "2. Text Generation Model: `meta-/llama2-7b-chat`\n",
    "3. Vector Database: OpenSearch as Vector Database to store embeddings\n",
    "4. StreamLit UI: A Chat Interface to talk to your documents"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9cf4391-89f7-424b-839e-5d92127aa83b",
   "metadata": {},
   "source": [
    "## Connect to an Existing EMR Cluster"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Why an empty cell you ask?\n",
    "\n",
    "Let's connect to an EMR Cluster while at this cell. Click `Cluster` button on the top right section of this JupyterLab window > Select a `Cluster` > Click Connect > Select `No Credentials` and `Voila`!\n",
    "\n",
    "<div style=\"background-color: #FFDDDD; border-left: 5px solid red; padding: 10px; color: black;\"> Stop! Please read this!\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Running `%%help` provides you will all the auto-magic commands supported by SageMaker PySpark cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7e083fe-bd83-42bb-b75b-886263d1ee25",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%help"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edcb60b5-97e5-460a-8049-c8ecbbe6b810",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Upload Files from Local to S3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we build a Retrieval Augmented Generation System, we need data! For this lab we're going to leverage the following PDF samples,\n",
    "1. Amazon SageMaker Dev Guide (1047 pages)\n",
    "2. EC2 Developer Guide (688 pages)\n",
    "3. S3 Developer Guide (546 pages)\n",
    "\n",
    "We're going to build a helper RAG bot that's able to look at developer guides and answer a user's question. You will be able to extend this notebook to build a RAG bot with your own developer guides, internal wiki pages, legacy system guides and much more. We're going to be processing a total of ~2300 pages for this example. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "Running `%%local` will ensure your code is executed on your Space's compute Instance (ex: ml.t3.medium, ml.t3.large, etc..) rather than EMR instances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9a3b0d4-0e93-45ab-ab2c-b18e3bfd8853",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%local\n",
    "import os\n",
    "import json\n",
    "import glob\n",
    "import boto3\n",
    "import sagemaker\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "007d33fd-40c4-4d3e-bab3-df14978cf658",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%local\n",
    "REGION = \"us-west-2\"\n",
    "sess = sagemaker.Session()\n",
    "default_bucket = sess.default_bucket()\n",
    "s3_client = boto3.client(\"s3\")\n",
    "\n",
    "print(f\"Using default bucket ---> {default_bucket}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b6690bd-76b5-40e9-acf9-98bb43f19733",
   "metadata": {},
   "source": [
    "The sample pdf files are places under `./AWSGuides` folder of this lab. We're going to upload this to an S3 bucket where these documents are going to be accessible to the EMR for reading and processing! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92cb76b3-e96b-4b70-bd12-5d3dacae6514",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%local\n",
    "def upload_raw_pdf_files_to_bucket(destination_bucket, destination_prefix, raw_pdf_files):\n",
    "    \n",
    "    print(f\"Uploading ---> {len(raw_pdf_files)} files!\")\n",
    "    \n",
    "    uploaded_file_s3uris = []\n",
    "    for pdf_file in tqdm(raw_pdf_files, total=len(raw_pdf_files)):\n",
    "        pdf_fname = os.path.basename(pdf_file).replace(\",\", \"\").replace(\" \", \"-\")\n",
    "        \n",
    "        pdf_dest_prefix = os.path.join(destination_prefix, pdf_fname)\n",
    "        \n",
    "        s3_client.upload_file(\n",
    "            pdf_file, \n",
    "            destination_bucket, \n",
    "            pdf_dest_prefix\n",
    "        )\n",
    "        uploaded_file_s3uris.append(f\"s3://{destination_bucket}/{pdf_dest_prefix}\")\n",
    "    \n",
    "    return uploaded_file_s3uris\n",
    "\n",
    "pdf_files_to_upload = glob.glob(\"./AWSGuides/*.pdf\")\n",
    "\n",
    "destination_prefix = \"Lab03/raw-pdfs\"\n",
    "\n",
    "files_paths_in_s3 = upload_raw_pdf_files_to_bucket(\n",
    "    destination_bucket=default_bucket, \n",
    "    destination_prefix=destination_prefix,\n",
    "    raw_pdf_files=pdf_files_to_upload\n",
    ")\n",
    "\n",
    "print(f\"Uploaded files to ---> {files_paths_in_s3}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6b03af6-cea3-46b7-8130-e7a343fa5292",
   "metadata": {},
   "source": [
    "We need to ensure that we have a synchronization of certain global variables between our local machine learning instance and a remote EMR Compute instance. For example variables like S3 paths, region information, service names, etc.. To enable this we're going to use the `%%send_to_spark` magic command. `%%send_to_spark` makes it easy for users to achieve a sync between local and EMR instances. The cells below are sending,\n",
    "1. Workflow Region Information\n",
    "2. S3 bucket name\n",
    "3. S3 documents prefix paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75247162-c86a-46fb-b61d-3fb810e42ed5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%send_to_spark -i REGION -t str -n REGION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a3b1188-b923-4393-8a79-35c9473af586",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%send_to_spark -i destination_prefix -t str -n SRC_FILE_PREFIX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7088e24-31b6-4e1b-8d07-7145c2551df3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%send_to_spark -i default_bucket -t str -n SRC_BUCKET_NAME"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9eb76b80-4ffa-453d-abc4-d63e84ef45f8",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Convert PDF to Text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "A PDF (Portable Document Format) file is a versatile file format that preserves document formatting across various platforms. Converting PDF files into text is necessary for processing because it allows for the extraction, manipulation, and analysis of the content programmatically, which is not directly feasible with the fixed-format nature of PDFs. This conversion enables Python programmers to apply text processing techniques such as data mining, searching, or natural language processing.\n",
    "\n",
    "\n",
    "**Note:** Observe the cells below are not using any magic commands, implying that these blocks of code are executed on PySpark remotely from our local SageMaker compute instance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a24cf1a1-a2ef-4e44-b95a-9d4958cac47d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import boto3\n",
    "import json\n",
    "from PyPDF2 import PdfReader\n",
    "from langchain.text_splitter import CharacterTextSplitter, RecursiveCharacterTextSplitter\n",
    "from langchain.document_loaders import PyPDFLoader, PyPDFDirectoryLoader\n",
    "import io"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d9adac1-5351-4439-af37-bab508ecdb8b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(f\"Source bucket and prefix to read pdf files ---> {SRC_BUCKET_NAME} {SRC_FILE_PREFIX}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to ensure that our sample PDF files in S3 are actually discoverable from inside our EMR Cluster. The basic way to validate this is by listing files inside an S3 Bucket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc3a4611-8edb-446a-97f3-b34562afdfc6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def list_files_in_s3_bucket_prefix(bucket_name, prefix):\n",
    "    \n",
    "    s3 = boto3.client('s3')\n",
    "\n",
    "    # Paginate through the objects in the specified bucket and prefix, and collect all keys (file paths)\n",
    "    paginator = s3.get_paginator('list_objects_v2')\n",
    "    page_iterator = paginator.paginate(Bucket=bucket_name, Prefix=prefix)\n",
    "\n",
    "    file_paths = []\n",
    "    for page in page_iterator:\n",
    "        if \"Contents\" in page:\n",
    "            for obj in page[\"Contents\"]:\n",
    "                if os.path.basename(obj[\"Key\"]):\n",
    "                    file_paths.append(obj[\"Key\"])\n",
    "\n",
    "    return file_paths\n",
    "\n",
    "all_pdf_files = list_files_in_s3_bucket_prefix(\n",
    "    bucket_name=SRC_BUCKET_NAME, \n",
    "    prefix=SRC_FILE_PREFIX\n",
    ")\n",
    "print(f\"Found {len(all_pdf_files)} files ---> {all_pdf_files}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bca227df-c358-4074-bd80-fbbe15b850ae",
   "metadata": {},
   "source": [
    "Success!\n",
    "\n",
    "Our files are discoverable from inside an EMR Cluster. Next, we're going to build a list of file references. Why? So we can parallelize the read operation across EMR nodes. Each EMR node will be responsible for reading a file from S3 and loading it into memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c20c47e-ef4d-44b9-b0e4-64fda6f4358e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "all_pdf_files = [(SRC_BUCKET_NAME, fpath) for fpath in all_pdf_files]\n",
    "type(all_pdf_files)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c20da11-468b-4038-9605-c1bc86fec182",
   "metadata": {},
   "source": [
    "If you observe, the data type for our list above is a python `List`. Spark doesnt natively parallelize a python `List` but we need to convert it into a Spark RDD which PySpark will then parallelize any task we run with an RDD downstream"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2430d20-4b34-46fc-b939-9071a0396b42",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pdfs_rdd = spark.sparkContext.parallelize(all_pdf_files)\n",
    "type(pdfs_rdd)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed1f3c93-6c8b-45a1-9e3b-973efc71457e",
   "metadata": {
    "tags": []
   },
   "source": [
    "The cell below enables Each code node reaches out a pdf file from our list of references, downloads the pdf file into memory and returns a PyPDF2 class reference for downstream workloads.\n",
    "\n",
    "---\n",
    "\n",
    "![EMR Read PDFs into Memory](./media/EMR-Doc-Read.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af4b0c93-4f4c-4870-a819-216358d863ca",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def load_pdf_from_s3_into_memory(row):\n",
    "    \"\"\"\n",
    "    Load a PDF file from an S3 bucket directly into memory.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        src_bucket_name, src_file_key = row \n",
    "        s3 = boto3.client('s3')\n",
    "        pdf_file = io.BytesIO()\n",
    "        s3.download_fileobj(src_bucket_name, src_file_key, pdf_file)\n",
    "        pdf_file.seek(0)\n",
    "        pdf_reader = PdfReader(pdf_file)\n",
    "        return (src_file_key, pdf_reader, len(pdf_reader.pages))\n",
    "    \n",
    "    except Exception as e:    \n",
    "        return (os.path.basename(src_file_key), str(e))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c536e2f8-c27c-46de-8a1f-c0d24fabb55e",
   "metadata": {},
   "source": [
    "Our vanilla list of file references has been coverted into a Spark RDD, now we can `map` each entry in the RDD/List and then `reduce` it back to the Primary node."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "387ff3c4-665a-4d9d-8c86-016c78a24e57",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pdfs_in_memory = pdfs_rdd.map(load_pdf_from_s3_into_memory).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok great! All pdf files are loaded into memory as a PDF class. Lets ensure that all pages of all pdf files are loaded into memory and werent any issues during read/load operations. \n",
    "\n",
    "PySpark cells allows users to generate static visualization as well! The example below shows how users can generate charts using `matplotlib` but you can make the charts look fanicer by styling them with more mordern static visualization libraries like `seaborn`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0acf5a1c-b0c1-440e-aaa3-deec729e4e35",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "x_labels = [pdfx.split('/')[-1] for pdfx, _, _ in pdfs_in_memory]\n",
    "y_values = [pages_count for _, _, pages_count in pdfs_in_memory]\n",
    "x = range(len(y_values))\n",
    "\n",
    "# Create a figure and a set of subplots\n",
    "fig, axs = plt.subplots(2, 1, figsize=(10, 10))\n",
    "\n",
    "# First Subplot: Bar Chart\n",
    "axs[0].bar(x, y_values, color=['red', 'green', 'blue'])\n",
    "axs[0].set_title('Bar Chart')\n",
    "axs[0].set_xticks(x)\n",
    "axs[0].set_xticklabels(x_labels, rotation=45, ha=\"right\")\n",
    "axs[0].set_ylabel('Pdf Pages Count --->')\n",
    "\n",
    "_bottom = 0\n",
    "for (pdf_name, page_count, color) in zip(x_labels, y_values, ['red', 'green', 'blue']):\n",
    "    axs[1].bar([0], [page_count], bottom=_bottom, color=color, label=pdf_name)\n",
    "    _bottom += page_count\n",
    "axs[1].set_title('Stacked Bar Chart')\n",
    "axs[1].set_xticks([0])\n",
    "axs[1].set_xticklabels(['Documents'], rotation=45, ha=\"right\")\n",
    "axs[1].set_ylabel('Stacked Pages Count --->')\n",
    "\n",
    "# Add a legend to the second subplot\n",
    "axs[1].legend()\n",
    "\n",
    "# Adjust the layout\n",
    "plt.tight_layout()\n",
    "\n",
    "# Show the plot\n",
    "plt.show()\n",
    "\n",
    "%matplot plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Every PDF document has 'n' pages to process, this task can be executed in a parallel fashion using Spark Processing. \n",
    "\n",
    "Each Document is split page by page, each page from a global reference of in memory pdfs.\n",
    "\n",
    "![PageLevelProcessingEMRPDFtoTxt](./media/PageLevelProcessingEMRPDFtoTxt.jpg)\n",
    "\n",
    "We're going to create a global pdf dict as a reference that can be accessed by all EMR core nodes when this information is required. This will come in handy when you're trying to maintain a global frame of reference during parallelization of tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "The way we achieve this parallelism at a page level is by creating a list (then converted into a PySpark RDD) with the format below,\n",
    "``` \n",
    "[\n",
    "    (/my/file1, page#1),\n",
    "    (/my/file1, page#2),\n",
    "    (/my/file1, page#3),\n",
    "    \n",
    "    (/my/file2, page#1),\n",
    "    (/my/file2, page#1),\n",
    "]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44a2aca6-3ef8-4620-b70d-b4fd99340c71",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "global_pdfs_in_mem_dict = {_key: pdf_reader for _key, pdf_reader, _ in pdfs_in_memory}\n",
    "\n",
    "docs_instances = []\n",
    "for (file_src, _, page_count) in pdfs_in_memory:\n",
    "    for pg_num in range(page_count):\n",
    "        docs_instances.append((file_src, pg_num))\n",
    "print(f\"Created {len(docs_instances)} parallel instances to process!\")\n",
    "# converts our list into an RDD\n",
    "docs_instances_rdd = spark.sparkContext.parallelize(docs_instances)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We parallize our document conversion from pdf format into text format (page by page) using our EMR core nodes. This step,\n",
    "\n",
    "`documents = docs_instances_rdd.map(extract_text_from_pdf_reader).collect()`\n",
    "\n",
    "Run a `map` and `reduce` operation on a list of values against our custom function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "364e3544-30c3-40f1-be45-84d3fc9d9133",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def extract_text_from_pdf_reader(row):\n",
    "    \"\"\" \n",
    "    Extract text from a page of the document \n",
    "    \"\"\"\n",
    "    try:\n",
    "        doc_path, page_num = row\n",
    "        page_text = global_pdfs_in_mem_dict[doc_path].pages[page_num].extract_text()\n",
    "        return page_text, doc_path, page_num\n",
    "    except Exception as e:\n",
    "        return str(e), doc_path, page_num\n",
    "    \n",
    "documents = docs_instances_rdd.map(extract_text_from_pdf_reader).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to convert our text blob from the step above into a LangChain friendly built-in `Document` class. \n",
    "\n",
    "`CustomDocument` class below is custom implementation of the same class allowing us to convert custom text blobs into a type that's identified by LangChain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19adf08d-898a-4d29-ab3a-b879aed47bde",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class CustomDocument:\n",
    "    def __init__(self, text, path, number):\n",
    "        self.page_content = text\n",
    "        self.metadata = {\n",
    "            'source': path, \n",
    "            'page': number  \n",
    "        }\n",
    "\n",
    "    def __repr__(self):\n",
    "        # This method is for representing the object in a way thats clear to a user (also can be used for debugging)\n",
    "        return f\"Document(page_content='{self.page_content}', metadata={self.metadata})\"\n",
    "\n",
    "    # Optionally, if you need a string representation of the instance that is more user-friendly, \n",
    "    # you can implement the __str__ method\n",
    "    def __str__(self):\n",
    "        return f\"Page Content: {self.page_content}\\nSource: {self.metadata['source']}\\nPage Number: {self.metadata['page']}\"\n",
    "    \n",
    "documents_custom = [\n",
    "    CustomDocument(text=text, path=doc_source, number=page_num) \n",
    "    for text, doc_source, page_num in documents\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c647d26-7e0a-4bf1-ae1c-91f543d431a4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "documents_custom[121]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bc9a7d8-8ac5-4313-b255-abd779ac7277",
   "metadata": {},
   "source": [
    "\n",
    "Chunking of text in a Retrieval-Augmented Generation (RAG) system is essential because it breaks down large text inputs into smaller, more manageable pieces. This process enables the efficient retrieval of relevant information from large databases, as RAG systems combine document retrieval with text generation to enhance response quality. Without chunking, the system might struggle with processing constraints and could miss key information due to the sheer volume of data. Thus, chunking ensures both the effectiveness and efficiency of the RAG system in handling and generating responses from extensive text sources.\n",
    "\n",
    "\n",
    "The RecursiveCharacterTextSplitter is a specialized text processing tool commonly used in RAG (Retrieval-Augmented Generation) system implementations. It is designed to efficiently handle long text inputs by recursively splitting them into smaller, manageable segments without losing context or coherence. This is crucial in RAG systems, which combine information retrieval with text generation, as it ensures that the retrieval component can effectively process and retrieve relevant information from extensive datasets. \n",
    "\n",
    "To learn more --> https://python.langchain.com/docs/modules/data_connection/document_transformers/text_splitters/recursive_text_splitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d6789c4-9b7c-4fe3-93e3-2533513ea404",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "global_text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=500,\n",
    "    chunk_overlap=50\n",
    ")\n",
    "docs = global_text_splitter.split_documents(documents_custom)\n",
    "print(f\"Total number of docs pre-split {len(documents_custom)} | after split {len(docs)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Chunking increases the number of text blobs we have to process. The number of blobs we end up with a factor of `chunk_size` and `chunk_overlap`. These are crucial parameters that govern the quality of your RAG system. Let's quickly visualize how many extra blobs are we now required to process as a result of text chunking usin `matplotlib`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47d41678-45aa-4731-9429-a5a00c1e5cd4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# create data\n",
    "plt.clf()\n",
    "\n",
    "x_labels = [\"Pre-Split\", \"Post Split\"]\n",
    "y = [len(documents_custom), len(docs)]\n",
    "x = range(len(x_labels))\n",
    "\n",
    "fig, axs = plt.subplots(1, 1, figsize=(7, 5))\n",
    "\n",
    "# First Subplot: Bar Chart\n",
    "axs.bar(x, y, color=[\"red\", \"blue\"])\n",
    "axs.set_title('Pre/Post RecursiveCharacterTextSplitter Split')\n",
    "axs.set_xticks(x)\n",
    "axs.set_xticklabels(x_labels, rotation=45, ha=\"right\")\n",
    "axs.set_ylabel('Text # to Process -->')\n",
    "\n",
    "# Add a legend to the second subplot\n",
    "axs.legend()\n",
    "\n",
    "# Adjust the layout\n",
    "plt.tight_layout()\n",
    "\n",
    "# Show the plot\n",
    "plt.show()\n",
    "\n",
    "%matplot plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95fb2184-e098-4582-90ec-ddc0fea5916a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(docs[1001])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7c1151c-e60e-478e-b342-a695fa0f3f38",
   "metadata": {},
   "source": [
    "## Run Parallelized Embeddings using Amazon EMR (EC2 Spark based Processing)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "The last component in our workflow we're going to parallelize is\n",
    "1. Tokenizing our text chunks \n",
    "2. Generation of vector embeddings using tokenized text chunks\n",
    "3. Ingestion of vector embeddings into a Vector Database (ex: OpenSearch)\n",
    "\n",
    "\n",
    "Steps 1..3 described above are represented in code below using 2 functions `generate_embeddings` and `EmbeddingsGenerator`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function `generate_embeddings` is responsible for taking in a chunk of text and generating embeddings using an Embeddings model via a Lambda `invokeEmbeddingEndpoint`. Lambda `invokeEmbeddingEndpoint` is responsible for handling the tokenziation of text chunk, parsing results and returing the embedding vector back to the code node.\n",
    "\n",
    "Class `EmbeddingsGenerator` complies with LangChain's structure for vectorestore processor. At the end of this parallelize operation. All embeddings are collected by the EMR primary node `embeddings_generated = input_text_rdd.map(generate_embeddings).collect()` and pushed to an OpenSearch Vector Database index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e40185c-15e3-4cbd-9d82-30af418d1378",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def generate_embeddings(input_text_sample):\n",
    "    \n",
    "    assert isinstance(input_text_sample, str), f\"Input must be a single string but found \" \n",
    "    \n",
    "    lambda_client = boto3.client('lambda', region_name='us-west-2') \n",
    "\n",
    "    # Prepare the data to send to the Lambda function\n",
    "    data = {\n",
    "        \"input\": input_text_sample\n",
    "    }\n",
    "\n",
    "    # Invoke the Lambda function\n",
    "    response = lambda_client.invoke(\n",
    "        FunctionName=\"invokeEmbeddingEndpoint\",\n",
    "        InvocationType=\"RequestResponse\",\n",
    "        Payload=json.dumps(data)\n",
    "    )\n",
    "\n",
    "    # Decode and load the response payload\n",
    "    response_payload = json.loads(response['Payload'].read().decode(\"utf-8\"))\n",
    "\n",
    "    # Extract status and embeddings from the response\n",
    "    status_code, embeddings = int(response_payload['statusCode']), json.loads(response_payload['body'])\n",
    "\n",
    "    return status_code, embeddings\n",
    "    \n",
    "class EmbeddingsGenerator:\n",
    "    \n",
    "    @staticmethod\n",
    "    def embed_documents(input_text, normalize=True):\n",
    "        \"\"\"\n",
    "        Generate embeddings for the provided text, invoking a Lambda function.\n",
    "        \"\"\"\n",
    "        assert isinstance(input_text, list), \"Input type must me list to embed_documents function\"\n",
    "        \n",
    "        input_text_rdd = spark.sparkContext.parallelize(input_text)\n",
    "        \n",
    "        embeddings_generated = input_text_rdd.map(generate_embeddings).collect()\n",
    "        \n",
    "        embedding_response = []\n",
    "        for s_code, embeddings in embeddings_generated:\n",
    "            if s_code == 200:\n",
    "                embedding_response.append(embeddings)\n",
    "            else:\n",
    "                pass\n",
    "        \n",
    "        return embedding_response\n",
    "    \n",
    "    @staticmethod\n",
    "    def embed_query(input_text):\n",
    "        status_code, embedding = generate_embeddings(input_text)\n",
    "        if status_code == 200:\n",
    "            return embedding\n",
    "        else: \n",
    "            None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d81eb0d-e91a-450c-b841-371f1297a28c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "response_code, sample_sentence_embedding = generate_embeddings(docs[1000].page_content)\n",
    "print(f\"Status {response_code}, Embedding size of the document --->\", len(sample_sentence_embedding))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Decide a name for your index, this is what's going to be used to query the embeddings we generate using for RAG bot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d9b0e5b-9705-4a18-8368-15618ea4146c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%local\n",
    "INDEX_NAME_OSE = \"amz-guides-index\"\n",
    "f = open(\"../studio-local-ui/indexname.txt\", \"w\")\n",
    "f.write(INDEX_NAME_OSE)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0aac1b62-9fbe-4c17-a3c7-c0d23354176a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%send_to_spark -i INDEX_NAME_OSE -t str -n INDEX_NAME_OSE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to securely access our OpenSearch cluster, to do so we use SecretsManager to securely access our OpenSearch username and password "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ce389bf-1122-4be3-b9dd-2afa03fc4b24",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%local\n",
    "def get_secret(secret_name, region_name=\"us-west-2\"):\n",
    "    # Create a Secrets Manager client\n",
    "    session = boto3.session.Session()\n",
    "    client = session.client(\n",
    "        service_name='secretsmanager',\n",
    "        region_name=region_name\n",
    "    )\n",
    "    get_secret_value_response = client.get_secret_value(\n",
    "        SecretId=secret_name\n",
    "    )\n",
    "    secrets = json.loads(get_secret_value_response['SecretString'])\n",
    "    user = secrets['username']\n",
    "    pwd = secrets['password']\n",
    "    return user, pwd\n",
    "\n",
    "# Use the function\n",
    "my_secret_name = \"OpenSearchDBSecret\"  \n",
    "user, pwd = get_secret(my_secret_name, REGION)\n",
    "print(f\"Session user and pwd ---> \", user, pwd)\n",
    "\n",
    "# write data to a local file \n",
    "f = open(\"../studio-local-ui/opensearchlogin.txt\", \"w\")\n",
    "f.write(f\"{user}|||{pwd}\")\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3284213b-5640-49be-b3f2-bf9b33b5626d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%send_to_spark -i user -t str -n user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66f1f8cb-4762-441f-9a77-fb754901e996",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%send_to_spark -i pwd -t str -n pwd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4bc4ded-16e3-4d70-8e95-2215f86bd65e",
   "metadata": {
    "tags": []
   },
   "source": [
    "In the next cell, we're going to use AWS CLI and `%local` command to execute opensearch command to determine opensearch endpoints and use that for downstream `langchain` tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf597563-e216-422c-932f-3b29d8dce57d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%local\n",
    "!echo -n \"https://$(aws es describe-elasticsearch-domain \\\n",
    "--domain-name \"$(aws es list-domain-names --query 'DomainNames[*].DomainName' --output text)\" \\\n",
    "--query 'DomainStatus.Endpoint' --output text)\" > ../studio-local-ui/opesearchurl.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%local\n",
    "OPENSEARCH_DOMAIN_URL = open(\"../studio-local-ui/opesearchurl.txt\", \"r\").read()\n",
    "print(f\"Your OpenSearch Domain URL --->\", OPENSEARCH_DOMAIN_URL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fa6df90-c8ac-4f27-ac41-ef60ea7f0bd4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%send_to_spark -i OPENSEARCH_DOMAIN_URL -t str -n OPENSEARCH_DOMAIN_URL"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "579685b0-054f-4dac-9cb4-bf22c2d1e9e1",
   "metadata": {
    "tags": []
   },
   "source": [
    "##### Now, we put it all together and run the embedding generation and ingestion processing in PySpark using Amazon EMR.\n",
    "<div style=\"background-color: #FFFF00; border-left: 5px solid yellow; padding: 10px; color: black;\">\n",
    "    Please be patient, this can take between 4-10 minutes to complete!\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "203689c0-015b-4700-8d56-3083e97d4969",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import time\n",
    "from langchain.vectorstores import OpenSearchVectorSearch\n",
    "\n",
    "start = time.time()\n",
    "docsearch = OpenSearchVectorSearch.from_documents(\n",
    "    docs, \n",
    "    EmbeddingsGenerator, \n",
    "    opensearch_url=OPENSEARCH_DOMAIN_URL,\n",
    "    bulk_size=len(docs),\n",
    "    http_auth=(user, pwd),\n",
    "    index_name=INDEX_NAME_OSE,\n",
    "    engine=\"faiss\"\n",
    ")\n",
    "\n",
    "end = time.time()\n",
    "print(f\"Total Time for ingestion: {round(end - start, 2)} secs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f47ef10c-a50e-4c01-b647-befb7e1f4c7c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "query = \"What is a Amazon SageMaker?\"\n",
    "sample_responses = docsearch.similarity_search(\n",
    "    query, \n",
    "    k=5, \n",
    "    space_type=\"cosineSimilarity\", \n",
    "    search_type=\"painless_scripting\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96fecfc5-f0cd-4064-82c8-aff9f6b3013c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sample_responses[4].page_content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "571c564f-4f9b-4f5e-9d90-37f51bc27979",
   "metadata": {},
   "source": [
    "## Putting it All Together"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b58f136-bd93-4d2d-971d-6b28259e4c7f",
   "metadata": {
    "tags": []
   },
   "source": [
    "To recap,\n",
    "\n",
    "1. We create a Spark Cluster to leverage PySpark for Distributed Data Processing at scale!\n",
    "2. We pushed some raw data into S3 (in reality, this data can be housed anywhere RedShift, S3, RDS, Dynamo, Snowflake, etc..)\n",
    "3. We Parallelized our document extraction from S3 using PySpark - our PySpark `Core` nodes were able to reach out to doc store (S3) read a file into memory for downstream processing\n",
    "4. We then split our processing at Document - at a page level and further parallelize our pdf reading process using PySpark\n",
    "5. We chunk our document corpus using `LangChain`'s `RecursiveCharacterTextSplitter`. We then convert our text into Embeddings using `BAAI/bge-base-en-v1.5` Embedding LLM Model and ingest these embeddings into OpenSearch index. - all using PySpark Parallel Processing technique\n",
    "6. Now we use `Streamlit` to interact with text generation model and document embeddings with a UI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the following cells marked with `%%bash`, these cells will install a few packages in your conda environment and spin up a new Streamlit UI that's accessible from the URL described below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f9a924c-c02d-4dbf-bc19-e84d3bada9c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "cd ../studio-local-ui\n",
    "streamlit run rag_app.py --server.runOnSave true --server.port 8502 > /dev/null"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9b135b0-aeb2-4a6f-9464-d5da7a4a77da",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #6bb07e; border-left: 5px solid #6bb07e; padding: 10px; color: black;\">\n",
    "    - Navigate to: https://example.studio.us-west-2.sagemaker.aws/jupyterlab/default/proxy/8502/\n",
    "</div>\n",
    "\n",
    "<div style=\"background-color: #6bb07e; border-left: 5px solid #6bb07e; padding: 10px; color: black;\">\n",
    "    <i>- Replace \"example\" with your your current url host `https://use_this_host.studio.us-west-2...`</i>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "availableInstances": [
   {
    "_defaultOrder": 0,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.t3.medium",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 1,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.t3.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 2,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.t3.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 3,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.t3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 4,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 5,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 6,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 7,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 8,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 9,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 10,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 11,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 12,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5d.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 13,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5d.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 14,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5d.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 15,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5d.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 16,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5d.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 17,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5d.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 18,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5d.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 19,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 20,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": true,
    "memoryGiB": 0,
    "name": "ml.geospatial.interactive",
    "supportedImageNames": [
     "sagemaker-geospatial-v1-0"
    ],
    "vcpuNum": 0
   },
   {
    "_defaultOrder": 21,
    "_isFastLaunch": true,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.c5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 22,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.c5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 23,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.c5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 24,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.c5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 25,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 72,
    "name": "ml.c5.9xlarge",
    "vcpuNum": 36
   },
   {
    "_defaultOrder": 26,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 96,
    "name": "ml.c5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 27,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 144,
    "name": "ml.c5.18xlarge",
    "vcpuNum": 72
   },
   {
    "_defaultOrder": 28,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.c5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 29,
    "_isFastLaunch": true,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g4dn.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 30,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g4dn.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 31,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g4dn.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 32,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g4dn.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 33,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g4dn.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 34,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g4dn.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 35,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 61,
    "name": "ml.p3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 36,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 244,
    "name": "ml.p3.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 37,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 488,
    "name": "ml.p3.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 38,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.p3dn.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 39,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.r5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 40,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.r5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 41,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.r5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 42,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.r5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 43,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.r5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 44,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.r5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 45,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.r5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 46,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.r5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 47,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 48,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 49,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 50,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 51,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 52,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 53,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.g5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 54,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.g5.48xlarge",
    "vcpuNum": 192
   },
   {
    "_defaultOrder": 55,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 56,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4de.24xlarge",
    "vcpuNum": 96
   }
  ],
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "python",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "pygments_lexer": "python3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
