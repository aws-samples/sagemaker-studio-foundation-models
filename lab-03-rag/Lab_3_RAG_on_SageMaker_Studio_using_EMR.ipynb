{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fc74abb9-e8c3-4305-ba35-fd4519548a2b",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #FFDDDD; border-left: 5px solid red; padding: 10px; color: black;\">\n",
    "    <strong>Kernel: PySpark\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6268e06-20fb-4ad7-bd47-5d51f7e59252",
   "metadata": {},
   "source": [
    "## Lab 03. Build Retrival Augmented Generation System using Amazon EMR Spark Distributed Processing and OpeSearch Vector Database\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Contents\n",
    "\n",
    "- [Overview](#overview)\n",
    "- [Connect to an Existing EMR Cluster](#connect-to-an-existing-emr-cluster)\n",
    "- [Upload Files from Local to S3](#upload-files-from-local-to-s3)\n",
    "- [Convert PDF to Text](#convert-pdf-to-text)\n",
    "- [Run Parallelized Embeddings using Amazon EMR (EC2 Spark based Processing)](#run-parallelized-embeddings-using-amazon-emr-ec2-spark-based-processing)\n",
    "- [Putting it All Together](#putting-it-all-together)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a4c7006-3446-41a6-a41a-d76875062d70",
   "metadata": {},
   "source": [
    "In this notebook we demonstrate how you can build a Retrival Augmented Generation System using the following components,\n",
    "1. Embedding Model: `BAAI/bge-base-en-v1.5`\n",
    "2. Text Generation Model: `meta-/llama2-7b-chat`\n",
    "3. Vector Database: OpenSearch as Vector Database to store embeddings\n",
    "4. StreamLit UI: A Chat Interface to talk to your documents"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9cf4391-89f7-424b-839e-5d92127aa83b",
   "metadata": {},
   "source": [
    "## Connect to an Existing EMR Cluster"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Why empty cell you ask?\n",
    "\n",
    "Let's connect to an EMR Cluster while at this cell. Click `Cluster` button on the top right section of this JupyterLab window > Select a `Cluster` > Click Connect > Select `No Credentials` and `Voila`!\n",
    "\n",
    "<div style=\"background-color: #FFDDDD; border-left: 5px solid red; padding: 10px; color: black;\"> Stop! Please read this!\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8c69a9a-6181-42e8-9b12-d0554b530e9b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%load_ext sagemaker_studio_analytics_extension.magics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7e083fe-bd83-42bb-b75b-886263d1ee25",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%help"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edcb60b5-97e5-460a-8049-c8ecbbe6b810",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Upload Files from Local to S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5ee95b3-53d3-4d44-9c6a-11fa2d4efeff",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%local\n",
    "!python3 -m pip install setuptools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "183cdac4-ca2e-4eee-9939-8b4c3c64af78",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%local\n",
    "!python3 -m pip install sagemaker==2.192.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9a3b0d4-0e93-45ab-ab2c-b18e3bfd8853",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%local\n",
    "import os\n",
    "import json\n",
    "import glob\n",
    "import boto3\n",
    "import sagemaker\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "007d33fd-40c4-4d3e-bab3-df14978cf658",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%local\n",
    "REGION = \"us-west-2\"\n",
    "sess = sagemaker.Session()\n",
    "default_bucket = sess.default_bucket()\n",
    "s3_client = boto3.client(\"s3\")\n",
    "\n",
    "print(f\"Using default bucket ---> {default_bucket}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b6690bd-76b5-40e9-acf9-98bb43f19733",
   "metadata": {},
   "source": [
    "A few sample files are available in directory under ./AWSGuides/, these are sample documents we'll be using to build our RAG application."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92cb76b3-e96b-4b70-bd12-5d3dacae6514",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%local\n",
    "def upload_raw_pdf_files_to_bucket(destination_bucket, destination_prefix, raw_pdf_files):\n",
    "    \n",
    "    print(f\"Uploading ---> {len(raw_pdf_files)} files!\")\n",
    "    \n",
    "    uploaded_file_s3uris = []\n",
    "    for pdf_file in tqdm(raw_pdf_files, total=len(raw_pdf_files)):\n",
    "        pdf_fname = os.path.basename(pdf_file).replace(\",\", \"\").replace(\" \", \"-\")\n",
    "        \n",
    "        pdf_dest_prefix = os.path.join(destination_prefix, pdf_fname)\n",
    "        \n",
    "        s3_client.upload_file(\n",
    "            pdf_file, \n",
    "            destination_bucket, \n",
    "            pdf_dest_prefix\n",
    "        )\n",
    "        uploaded_file_s3uris.append(f\"s3://{destination_bucket}/{pdf_dest_prefix}\")\n",
    "    \n",
    "    return uploaded_file_s3uris\n",
    "\n",
    "pdf_files_to_upload = glob.glob(\"./AWSGuides/*.pdf\")\n",
    "\n",
    "destination_prefix = \"Lab03/raw-pdfs\"\n",
    "\n",
    "files_paths_in_s3 = upload_raw_pdf_files_to_bucket(\n",
    "    destination_bucket=default_bucket, \n",
    "    destination_prefix=destination_prefix,\n",
    "    raw_pdf_files=pdf_files_to_upload\n",
    ")\n",
    "\n",
    "print(f\"Uploaded files to ---> {files_paths_in_s3}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6b03af6-cea3-46b7-8130-e7a343fa5292",
   "metadata": {},
   "source": [
    "Let's send these variables from our local instance to Pyspark Primary node using a simple \n",
    "\n",
    "`%%send_to_spark` command"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75247162-c86a-46fb-b61d-3fb810e42ed5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%send_to_spark -i REGION -t str -n REGION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a3b1188-b923-4393-8a79-35c9473af586",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%send_to_spark -i destination_prefix -t str -n SRC_FILE_PREFIX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7088e24-31b6-4e1b-8d07-7145c2551df3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%send_to_spark -i default_bucket -t str -n SRC_BUCKET_NAME"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9eb76b80-4ffa-453d-abc4-d63e84ef45f8",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Convert PDF to Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a24cf1a1-a2ef-4e44-b95a-9d4958cac47d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import boto3\n",
    "import json\n",
    "from PyPDF2 import PdfReader\n",
    "from langchain.text_splitter import CharacterTextSplitter, RecursiveCharacterTextSplitter\n",
    "from langchain.document_loaders import PyPDFLoader, PyPDFDirectoryLoader\n",
    "import io"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d9adac1-5351-4439-af37-bab508ecdb8b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(f\"Source bucket and prefix to read pdf files ---> {SRC_BUCKET_NAME} {SRC_FILE_PREFIX}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc3a4611-8edb-446a-97f3-b34562afdfc6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def list_files_in_s3_bucket_prefix(bucket_name, prefix):\n",
    "    \n",
    "    s3 = boto3.client('s3')\n",
    "\n",
    "    # Paginate through the objects in the specified bucket and prefix, and collect all keys (file paths)\n",
    "    paginator = s3.get_paginator('list_objects_v2')\n",
    "    page_iterator = paginator.paginate(Bucket=bucket_name, Prefix=prefix)\n",
    "\n",
    "    file_paths = []\n",
    "    for page in page_iterator:\n",
    "        if \"Contents\" in page:\n",
    "            for obj in page[\"Contents\"]:\n",
    "                if os.path.basename(obj[\"Key\"]):\n",
    "                    file_paths.append(obj[\"Key\"])\n",
    "\n",
    "    return file_paths\n",
    "\n",
    "all_pdf_files = list_files_in_s3_bucket_prefix(\n",
    "    bucket_name=SRC_BUCKET_NAME, \n",
    "    prefix=SRC_FILE_PREFIX\n",
    ")\n",
    "print(f\"Found {len(all_pdf_files)} files ---> {all_pdf_files}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bca227df-c358-4074-bd80-fbbe15b850ae",
   "metadata": {},
   "source": [
    "Let's prep a list to process files along with bucket names "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c20c47e-ef4d-44b9-b0e4-64fda6f4358e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "all_pdf_files = [(SRC_BUCKET_NAME, fpath) for fpath in all_pdf_files]\n",
    "type(all_pdf_files)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c20da11-468b-4038-9605-c1bc86fec182",
   "metadata": {},
   "source": [
    "Let's convert our list to a spark RDD for parallelization of our list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2430d20-4b34-46fc-b939-9071a0396b42",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pdfs_rdd = spark.sparkContext.parallelize(all_pdf_files)\n",
    "type(pdfs_rdd)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed1f3c93-6c8b-45a1-9e3b-973efc71457e",
   "metadata": {
    "tags": []
   },
   "source": [
    "Each code node reaches out a pdf file from our list, downloads the pdf file into memory and returns a PyPDF2 class reference for downstream workloads\n",
    "\n",
    "![EMR Read PDFs into Memory](./media/EMR-Doc-Read.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af4b0c93-4f4c-4870-a819-216358d863ca",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def load_pdf_from_s3_into_memory(row):\n",
    "    \"\"\"\n",
    "    Load a PDF file from an S3 bucket directly into memory.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        src_bucket_name, src_file_key = row \n",
    "        s3 = boto3.client('s3')\n",
    "        pdf_file = io.BytesIO()\n",
    "        s3.download_fileobj(src_bucket_name, src_file_key, pdf_file)\n",
    "        pdf_file.seek(0)\n",
    "        pdf_reader = PdfReader(pdf_file)\n",
    "        return (src_file_key, pdf_reader, len(pdf_reader.pages))\n",
    "    \n",
    "    except Exception as e:    \n",
    "        return (os.path.basename(src_file_key), str(e))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c536e2f8-c27c-46de-8a1f-c0d24fabb55e",
   "metadata": {},
   "source": [
    "Let's concurrently load pdf files into memory using rdd map and collect the results back to our Primary Node"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "387ff3c4-665a-4d9d-8c86-016c78a24e57",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pdfs_in_memory = pdfs_rdd.map(load_pdf_from_s3_into_memory).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0acf5a1c-b0c1-440e-aaa3-deec729e4e35",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "x_labels = [pdfx.split('/')[-1] for pdfx, _, _ in pdfs_in_memory]\n",
    "y_values = [pages_count for _, _, pages_count in pdfs_in_memory]\n",
    "x = range(len(y_values))\n",
    "\n",
    "# Create a figure and a set of subplots\n",
    "fig, axs = plt.subplots(2, 1, figsize=(10, 10))\n",
    "\n",
    "# First Subplot: Bar Chart\n",
    "axs[0].bar(x, y_values, color=['red', 'green', 'blue'])\n",
    "axs[0].set_title('Bar Chart')\n",
    "axs[0].set_xticks(x)\n",
    "axs[0].set_xticklabels(x_labels, rotation=45, ha=\"right\")\n",
    "axs[0].set_ylabel('Pdf Pages Count --->')\n",
    "\n",
    "_bottom = 0\n",
    "for (pdf_name, page_count, color) in zip(x_labels, y_values, ['red', 'green', 'blue']):\n",
    "    axs[1].bar([0], [page_count], bottom=_bottom, color=color, label=pdf_name)\n",
    "    _bottom += page_count\n",
    "axs[1].set_title('Stacked Bar Chart')\n",
    "axs[1].set_xticks([0])\n",
    "axs[1].set_xticklabels(['Documents'], rotation=45, ha=\"right\")\n",
    "axs[1].set_ylabel('Stacked Pages Count --->')\n",
    "\n",
    "# Add a legend to the second subplot\n",
    "axs[1].legend()\n",
    "\n",
    "# Adjust the layout\n",
    "plt.tight_layout()\n",
    "\n",
    "# Show the plot\n",
    "plt.show()\n",
    "\n",
    "%matplot plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c822bd81-bf8e-43ab-94dd-4185750baff1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class CustomDocument:\n",
    "    def __init__(self, text, path, number):\n",
    "        self.page_content = text\n",
    "        self.metadata = {\n",
    "            'source': path, \n",
    "            'page': number  \n",
    "        }\n",
    "\n",
    "    def __repr__(self):\n",
    "        # This method is for representing the object in a way thatâ€™s clear to a human (also can be used for debugging)\n",
    "        return f\"Document(page_content='{self.page_content}', metadata={self.metadata})\"\n",
    "\n",
    "    # Optionally, if you need a string representation of the instance that is more user-friendly, \n",
    "    # you can implement the __str__ method\n",
    "    def __str__(self):\n",
    "        return f\"Page Content: {self.page_content}\\nSource: {self.metadata['source']}\\nPage Number: {self.metadata['page']}\"\n",
    "    \n",
    "def extract_text_from_pdf_reader(row):\n",
    "    \"\"\" \n",
    "    Extract text from a page of the document \n",
    "    \"\"\"\n",
    "    try:\n",
    "        doc_path, page_num = row\n",
    "        page_text = global_pdfs_in_mem_dict[doc_path].pages[page_num].extract_text()\n",
    "        return page_text, doc_path, page_num\n",
    "    except Exception as e:\n",
    "        return str(e), doc_path, page_num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44a2aca6-3ef8-4620-b70d-b4fd99340c71",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "global_pdfs_in_mem_dict = {_key: pdf_reader for _key, pdf_reader, _ in pdfs_in_memory}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6709d8df-5cdf-48be-9f16-93e5b7a8f031",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "docs_instances = []\n",
    "for (file_src, _, page_count) in pdfs_in_memory:\n",
    "    for pg_num in range(page_count):\n",
    "        docs_instances.append((file_src, pg_num))\n",
    "print(f\"Created {len(docs_instances)} parallel instances to process!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "364e3544-30c3-40f1-be45-84d3fc9d9133",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "docs_instances_rdd = spark.sparkContext.parallelize(docs_instances)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "064c5d70-cfb8-4d34-86a6-0091ee8a2a20",
   "metadata": {},
   "source": [
    "Every PDF document has 'n' pages to process, this task can be executed in a parallel fashion using Spark Processing. \n",
    "\n",
    "Each Document is split page by page, each page from a global reference of in memory pdfs.\n",
    "\n",
    "![PageLevelProcessingEMRPDFtoTxt](./media/PageLevelProcessingEMRPDFtoTxt.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19adf08d-898a-4d29-ab3a-b879aed47bde",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "documents = docs_instances_rdd.map(extract_text_from_pdf_reader).collect()\n",
    "documents_custom = [\n",
    "    CustomDocument(text=text, path=doc_source, number=page_num) \n",
    "    for text, doc_source, page_num in documents\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c647d26-7e0a-4bf1-ae1c-91f543d431a4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "documents_custom[121]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bc9a7d8-8ac5-4313-b255-abd779ac7277",
   "metadata": {},
   "source": [
    "We split pages using a reference chunk size, chunk size is an experimental value. To learn more about chunk size and how RecursiveCharacterTextSplitter, see: https://python.langchain.com/docs/modules/data_connection/document_transformers/text_splitters/recursive_text_splitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d6789c4-9b7c-4fe3-93e3-2533513ea404",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "global_text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=500,\n",
    "    chunk_overlap=50\n",
    ")\n",
    "docs = global_text_splitter.split_documents(documents_custom)\n",
    "print(f\"Total number of docs pre-split {len(documents_custom)} | after split {len(docs)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47d41678-45aa-4731-9429-a5a00c1e5cd4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# create data\n",
    "plt.clf()\n",
    "\n",
    "x_labels = [\"Pre-Split\", \"Post Split\"]\n",
    "y = [len(documents_custom), len(docs)]\n",
    "x = range(len(x_labels))\n",
    "\n",
    "fig, axs = plt.subplots(1, 1, figsize=(7, 5))\n",
    "\n",
    "# First Subplot: Bar Chart\n",
    "axs.bar(x, y, color=[\"red\", \"blue\"])\n",
    "axs.set_title('Pre/Post RecursiveCharacterTextSplitter Split')\n",
    "axs.set_xticks(x)\n",
    "axs.set_xticklabels(x_labels, rotation=45, ha=\"right\")\n",
    "axs.set_ylabel('Text # to Process -->')\n",
    "\n",
    "# Add a legend to the second subplot\n",
    "axs.legend()\n",
    "\n",
    "# Adjust the layout\n",
    "plt.tight_layout()\n",
    "\n",
    "# Show the plot\n",
    "plt.show()\n",
    "\n",
    "%matplot plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95fb2184-e098-4582-90ec-ddc0fea5916a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(docs[1001])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7c1151c-e60e-478e-b342-a695fa0f3f38",
   "metadata": {},
   "source": [
    "## Run Parallelized Embeddings using Amazon EMR (EC2 Spark based Processing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e40185c-15e3-4cbd-9d82-30af418d1378",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def generate_embeddings(input_text_sample):\n",
    "    \n",
    "    assert isinstance(input_text_sample, str), f\"Input must be a single string but found \" \n",
    "    \n",
    "    lambda_client = boto3.client('lambda', region_name='us-west-2') \n",
    "\n",
    "    # Prepare the data to send to the Lambda function\n",
    "    data = {\n",
    "        \"input\": input_text_sample\n",
    "    }\n",
    "\n",
    "    # Invoke the Lambda function\n",
    "    response = lambda_client.invoke(\n",
    "        FunctionName=\"invokeEmbeddingEndpoint\",\n",
    "        InvocationType=\"RequestResponse\",\n",
    "        Payload=json.dumps(data)\n",
    "    )\n",
    "\n",
    "    # Decode and load the response payload\n",
    "    response_payload = json.loads(response['Payload'].read().decode(\"utf-8\"))\n",
    "\n",
    "    # Extract status and embeddings from the response\n",
    "    status_code, embeddings = int(response_payload['statusCode']), json.loads(response_payload['body'])\n",
    "\n",
    "    return status_code, embeddings\n",
    "    \n",
    "class EmbeddingsGenerator:\n",
    "    \n",
    "    @staticmethod\n",
    "    def embed_documents(input_text, normalize=True):\n",
    "        \"\"\"\n",
    "        Generate embeddings for the provided text, invoking a Lambda function.\n",
    "        \"\"\"\n",
    "        assert isinstance(input_text, list), \"Input type must me list to embed_documents function\"\n",
    "        \n",
    "        input_text_rdd = spark.sparkContext.parallelize(input_text)\n",
    "        \n",
    "        embeddings_generated = input_text_rdd.map(generate_embeddings).collect()\n",
    "        \n",
    "        embedding_response = []\n",
    "        for s_code, embeddings in embeddings_generated:\n",
    "            if s_code == 200:\n",
    "                embedding_response.append(embeddings)\n",
    "            else:\n",
    "                pass\n",
    "        \n",
    "        return embedding_response\n",
    "    \n",
    "    @staticmethod\n",
    "    def embed_query(input_text):\n",
    "        status_code, embedding = generate_embeddings(input_text)\n",
    "        if status_code == 200:\n",
    "            return embedding\n",
    "        else: \n",
    "            None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d81eb0d-e91a-450c-b841-371f1297a28c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "response_code, sample_sentence_embedding = generate_embeddings(docs[1000].page_content)\n",
    "print(f\"Status {response_code}, Embedding size of the document --->\", len(sample_sentence_embedding))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d9b0e5b-9705-4a18-8368-15618ea4146c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%local\n",
    "INDEX_NAME_OSE = \"amz-guides-index\"\n",
    "f = open(\"../studio-local-ui/indexname.txt\", \"w\")\n",
    "f.write(INDEX_NAME_OSE)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0aac1b62-9fbe-4c17-a3c7-c0d23354176a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%send_to_spark -i INDEX_NAME_OSE -t str -n INDEX_NAME_OSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ce389bf-1122-4be3-b9dd-2afa03fc4b24",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%local\n",
    "def get_secret(secret_name, region_name=\"us-west-2\"):\n",
    "    # Create a Secrets Manager client\n",
    "    session = boto3.session.Session()\n",
    "    client = session.client(\n",
    "        service_name='secretsmanager',\n",
    "        region_name=region_name\n",
    "    )\n",
    "    get_secret_value_response = client.get_secret_value(\n",
    "        SecretId=secret_name\n",
    "    )\n",
    "    secrets = json.loads(get_secret_value_response['SecretString'])\n",
    "    user = secrets['username']\n",
    "    pwd = secrets['password']\n",
    "    return user, pwd\n",
    "\n",
    "# Use the function\n",
    "my_secret_name = \"OpenSearchSecret-workshop-studio-v2-cfn\"  \n",
    "user, pwd = get_secret(my_secret_name, REGION)\n",
    "print(f\"Session user and pwd ---> \", user, pwd)\n",
    "\n",
    "# write data to a local file \n",
    "f = open(\"../studio-local-ui/opensearchlogin.txt\", \"w\")\n",
    "f.write(f\"{user}|||{pwd}\")\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3284213b-5640-49be-b3f2-bf9b33b5626d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%send_to_spark -i user -t str -n user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66f1f8cb-4762-441f-9a77-fb754901e996",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%send_to_spark -i pwd -t str -n pwd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f8fee38-ffa8-451e-87b1-96ea050fdb42",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #FFFF00; border-left: 5px solid yellow; padding: 10px; color: black;\">\n",
    "    Please navigate to your AWS Management Console to find your OpenSearch Domain Endpoint URL\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4bc4ded-16e3-4d70-8e95-2215f86bd65e",
   "metadata": {
    "tags": []
   },
   "source": [
    "Let's find your OpenSearch domain URL to connect, switch over to your AWS Management Console and search for `Amazon OpenSearch Service`. You should see a OpenSearch domain active and in `Green` status listed as `opensearchservi-xxxx`. \n",
    "1.  Click on your OpenSearch Domain\n",
    "2. Find `Domain endpoint`\n",
    "3. Copy the URL: https://xxx.es.amazonaws.com"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf597563-e216-422c-932f-3b29d8dce57d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%local\n",
    "OPENSEARCH_DOMAIN_URL = \"https://search-opensearchservi-yoetoghxcrcw-v4bi7r4eb6jvbidmjg3bbohw4q.us-west-2.es.amazonaws.com\"\n",
    "f = open(\"../studio-local-ui/opesearchurl.txt\", \"w\")\n",
    "f.write(OPENSEARCH_DOMAIN_URL)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fa6df90-c8ac-4f27-ac41-ef60ea7f0bd4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%send_to_spark -i OPENSEARCH_DOMAIN_URL -t str -n OPENSEARCH_DOMAIN_URL"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "579685b0-054f-4dac-9cb4-bf22c2d1e9e1",
   "metadata": {
    "tags": []
   },
   "source": [
    "This step below parallelizes the following operations using EMR Spark,\n",
    "1. Takes text chunks of documents - ivokes an embedding endpoint to encode our data chunks\n",
    "2. Ingest embeddings + text + text meta data into OpenSearch database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "203689c0-015b-4700-8d56-3083e97d4969",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import time\n",
    "from langchain.vectorstores import OpenSearchVectorSearch\n",
    "\n",
    "start = time.time()\n",
    "docsearch = OpenSearchVectorSearch.from_documents(\n",
    "    docs, \n",
    "    EmbeddingsGenerator, \n",
    "    opensearch_url=OPENSEARCH_DOMAIN_URL,\n",
    "    bulk_size=len(docs),\n",
    "    http_auth=(user, pwd),\n",
    "    index_name=INDEX_NAME_OSE,\n",
    "    engine=\"faiss\"\n",
    ")\n",
    "\n",
    "end = time.time()\n",
    "print(f\"Total Time for ingestion: {round(end - start, 2)} secs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f47ef10c-a50e-4c01-b647-befb7e1f4c7c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "query = \"What is a Amazon SageMaker?\"\n",
    "sample_responses = docsearch.similarity_search(\n",
    "    query, \n",
    "    k=5, \n",
    "    space_type=\"cosineSimilarity\", \n",
    "    search_type=\"painless_scripting\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96fecfc5-f0cd-4064-82c8-aff9f6b3013c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sample_responses[4].page_content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "571c564f-4f9b-4f5e-9d90-37f51bc27979",
   "metadata": {},
   "source": [
    "## Putting it All Together"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b58f136-bd93-4d2d-971d-6b28259e4c7f",
   "metadata": {
    "tags": []
   },
   "source": [
    "To recap,\n",
    "\n",
    "1. We create a Spark Cluster to leverage PySpark for Distributed Data Processing at scale!\n",
    "2. We pushed some raw data into S3 (in reality, this data can be housed anywhere RedShift, S3, RDS, Dynamo, Snowflake, etc..)\n",
    "3. We Parallelized our document extraction from S3 using PySpark - our PySpark `Core` nodes were able to reach out to doc store (S3) read a file into memory for downstream processing\n",
    "4. We then split our processing at Document - at a page level and further parallelize our pdf reading process using PySpark\n",
    "5. We chunk our document corpus using `LangChain`'s `RecursiveCharacterTextSplitter`. We then convert our text into Embeddings using `BAAI/bge-base-en-v1.5` Embedding LLM Model and ingest these embeddings into OpenSearch index. - all using PySpark Parallel Processing technique\n",
    "6. Now we use `Streamlit` to interact with text generation model and document embeddings with a UI"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fefed40c",
   "metadata": {},
   "source": [
    "Now, let's host the app. In order to do this, we will connect to the System terminal. Navigate to the `File` > `New` and `Terminal` to launch a new `System Terminal`\n",
    "\n",
    "Then run the following commands using `System Terminal`:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1826a0e8",
   "metadata": {},
   "source": [
    "`cd ~/sagemaker-studio-foundation-models/studio-local-ui`\n",
    "\n",
    "`streamlit run rag_app.py --server.runOnSave true`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8b20ffa-29b8-4d2b-be5f-6133167e6d3b",
   "metadata": {},
   "source": [
    "OR You can run bash commands from inside a notebook cell as below,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f9a924c-c02d-4dbf-bc19-e84d3bada9c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "cd ../studio-local-ui\n",
    "streamlit run rag_app.py --server.runOnSave true"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9b135b0-aeb2-4a6f-9464-d5da7a4a77da",
   "metadata": {},
   "source": [
    "##### Navigate to https://example.studio.us-west-2.sagemaker.aws/jupyterlab/default/proxy/8501/ \n",
    "\n",
    "Replace \"example\" with your your current url hash `https://use_this_hash.studio.us-west-2...`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c85edf8-6b7b-4a2b-8c08-06fa48e85b39",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "availableInstances": [
   {
    "_defaultOrder": 0,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.t3.medium",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 1,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.t3.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 2,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.t3.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 3,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.t3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 4,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 5,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 6,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 7,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 8,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 9,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 10,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 11,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 12,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5d.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 13,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5d.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 14,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5d.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 15,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5d.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 16,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5d.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 17,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5d.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 18,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5d.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 19,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 20,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": true,
    "memoryGiB": 0,
    "name": "ml.geospatial.interactive",
    "supportedImageNames": [
     "sagemaker-geospatial-v1-0"
    ],
    "vcpuNum": 0
   },
   {
    "_defaultOrder": 21,
    "_isFastLaunch": true,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.c5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 22,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.c5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 23,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.c5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 24,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.c5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 25,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 72,
    "name": "ml.c5.9xlarge",
    "vcpuNum": 36
   },
   {
    "_defaultOrder": 26,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 96,
    "name": "ml.c5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 27,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 144,
    "name": "ml.c5.18xlarge",
    "vcpuNum": 72
   },
   {
    "_defaultOrder": 28,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.c5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 29,
    "_isFastLaunch": true,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g4dn.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 30,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g4dn.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 31,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g4dn.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 32,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g4dn.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 33,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g4dn.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 34,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g4dn.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 35,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 61,
    "name": "ml.p3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 36,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 244,
    "name": "ml.p3.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 37,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 488,
    "name": "ml.p3.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 38,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.p3dn.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 39,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.r5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 40,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.r5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 41,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.r5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 42,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.r5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 43,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.r5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 44,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.r5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 45,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.r5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 46,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.r5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 47,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 48,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 49,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 50,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 51,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 52,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 53,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.g5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 54,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.g5.48xlarge",
    "vcpuNum": 192
   },
   {
    "_defaultOrder": 55,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 56,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4de.24xlarge",
    "vcpuNum": 96
   }
  ],
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "PySpark",
   "language": "python",
   "name": "pysparkkernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "python",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "pyspark",
   "pygments_lexer": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
