{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d780de3a-f72e-42a8-a51d-942297ec149a",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #FFDDDD; border-left: 5px solid red; padding: 10px; color: black;\">\n",
    "    <strong>Kernel: Python 3 (ipykernel)\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d1697f4-24f3-4c1b-99d2-fa89a3340a1e",
   "metadata": {},
   "source": [
    "## Lab 1: Setup a LLM Playground on SageMaker Studio\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4ca32fa-e8ae-4820-8107-58fbaa910953",
   "metadata": {},
   "source": [
    "__Large Language Model (LLM) with `Llama2`, `LangChain`, and `Streamlit`.__\n",
    "\n",
    "In this lab, we learn how to use SageMaker to download, provision, and send prompts to a Large Language Model, `Llama 2`. We create an agent using `LangChain`, and tie everything together by creating a UI and text input using `Streamlit` to make our own hosted chatbot interface."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e978109e-a89a-4a35-9ce4-0c9062a30ecd",
   "metadata": {},
   "source": [
    "## Contents\n",
    "\n",
    "- [Model License information](#Model-License-information)\n",
    "- [Download and host Llama2 model](#Download-and-host-Llama2-model)\n",
    "  - [Set up](#Set-up)\n",
    "- [Sending prompts](#Sending-prompts)\n",
    "  - [Supported Parameters](#Supported-Parameters)\n",
    "  - [Notes](#Notes)\n",
    "- [Building an agent with LangChain](#Building-an-agent-with-LangChain)\n",
    "- [LangChain Tools](#LangChain-Tools)\n",
    "- [Developing and deploying the UI with Streamlit](#Developing-and-deploying-the-UI-with-Streamlit)\n",
    "- [Tearing down resources](#Tearing-down-resources)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88c309ff-c81e-4cf0-915b-a2ff544bd634",
   "metadata": {},
   "source": [
    "### Model License information\n",
    "---\n",
    "\n",
    "To perform inference on these models, you need to pass `custom_attributes='accept_eula=true'` as part of header. This means you have read and accept the end-user-license-agreement (EULA) of the model. EULA can be found in model card description or from https://ai.meta.com/resources/models-and-libraries/llama-downloads/. By default, this notebook sets `custom_attribute='accept_eula=false'`, so all inference requests will fail until you explicitly change this custom attribute.\n",
    "\n",
    "Note: Custom_attributes used to pass EULA are key/value pairs. The key and value are separated by `'='` and pairs are separated by `';'`. If the user passes the same key more than once, the last value is kept and passed to the script handler (i.e., in this case, used for conditional logic). For example, if `'accept_eula=false; accept_eula=true'` is passed to the server, then `'accept_eula=true'` is kept and passed to the script handler.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "357c23c0-6ffd-4057-a7b4-62571a220cc0",
   "metadata": {},
   "source": [
    "### Connect to a Hosted Llama2 Model\n",
    "---\n",
    "\n",
    "#### Set up\n",
    "\n",
    "We begin by installing and upgrading necessary packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "461893c9-9c6f-4a16-98d8-195e26b76800",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install \"langchain==0.2.16\" \"streamlit==1.38.0\" wikipedia \"numexpr==2.8.7\" faiss-cpu opensearch-py==2.3.2 -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "54d4ac5c-918c-48de-a8bc-3b9000a944c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sagemaker.config INFO - Not applying SDK defaults from location: /etc/xdg/sagemaker/config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /home/sagemaker-user/.config/sagemaker/config.yaml\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import boto3\n",
    "import sagemaker\n",
    "from sagemaker import serializers, deserializers\n",
    "from ipywidgets import Dropdown\n",
    "from IPython.display import display, Markdown\n",
    "from typing import Dict, List\n",
    "from datetime import datetime\n",
    "\n",
    "import sys\n",
    "sys.path.append(os.path.dirname(os.getcwd()))\n",
    "from utils.helpers import pretty_print_html "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "200c188f-ec7d-4153-99ca-2c4fc4e98731",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dea0e47f77fb4f25a9ec82278d32464a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Dropdown(description='**Please accept Llama2 EULA to continue:**', index=1, layout=Layout(width='max-content')‚Ä¶"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "eula_dropdown = Dropdown(\n",
    "    options=[\"True\", \"False\"],\n",
    "    value=\"False\",\n",
    "    description=\"**Please accept Llama2 EULA to continue:**\",\n",
    "    style={\"description_width\": \"initial\"},\n",
    "    layout={\"width\": \"max-content\"},\n",
    ")\n",
    "display(eula_dropdown)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fd75c6c5-2001-47a8-b240-f551158bdabb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"font-family: monospace; background-color: #f8f8f8; padding: 10px; border-radius: 5px; border: 1px solid #0077b6;\">Your Llama2 EULA attribute is set to: accept_eula=true</pre>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "custom_attribute = f'accept_eula={eula_dropdown.value.lower()}'\n",
    "pretty_print_html(f\"Your Llama2 EULA attribute is set to: {custom_attribute}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09e95e69-598b-46a5-985a-3c49f51f06ad",
   "metadata": {},
   "source": [
    "#### Connect to an Hosted Llama Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a1d666a5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"font-family: monospace; background-color: #f8f8f8; padding: 10px; border-radius: 5px; border: 1px solid #0077b6;\">Using Llama Endpoint: meta-llama31-8b-instruct-tg-ep in region: us-east-1</pre>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "endpoint_name = \"meta-llama31-8b-instruct-tg-ep\" \n",
    "boto_region = boto3.Session().region_name\n",
    "\n",
    "pretty_print_html(f\"Using Llama Endpoint: {endpoint_name} in region: {boto_region}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7b3227db",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sess = sagemaker.session.Session(\n",
    "    boto_session=boto3.Session(region_name=boto_region)\n",
    ")\n",
    "smr_client = boto3.client(\n",
    "    \"sagemaker-runtime\", \n",
    "    region_name=boto_region\n",
    ")\n",
    "\n",
    "pretrained_predictor = sagemaker.Predictor(\n",
    "    endpoint_name=endpoint_name,\n",
    "    sagemaker_session=sess,\n",
    "    serializer=serializers.JSONSerializer(),\n",
    "    deserializer=deserializers.JSONDeserializer(),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "543d54d9-447e-4f95-9ae2-2aafdd99c0c2",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Sending prompts\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e64ac3c-0974-419a-8ad6-1f360c812d50",
   "metadata": {},
   "source": [
    "Next, we invoke the endpoint hosting our Llama 2 LLM with some queries. To guess the best results, however, it is important to be aware of the adjustable parameters of this model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aeccd9af-d022-4126-afa2-303eee5da85a",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Supported Parameters\n",
    "This model supports the following inference payload parameters:\n",
    "\n",
    "* **max_new_tokens:** Model generates text until the output length (excluding the input context length) reaches max_new_tokens. If specified, it must be a positive integer.\n",
    "* **temperature:** Controls the randomness in the output. Higher temperature results in output sequence with low-probability words and lower temperature results in output sequence with high-probability words. If `temperature` -> 0, it results in greedy decoding. If specified, it must be a positive float.\n",
    "* **top_p:** In each step of text generation, sample from the smallest possible set of words with cumulative probability `top_p`. If specified, it must be a float between 0 and 1.\n",
    "\n",
    "We'll begin with 512, 0.9, and 0.6 for these respectively, though feel free to alter these are we do to see how this may affect the LLM output.\n",
    "\n",
    "You may specify any subset of the parameters mentioned above while invoking an endpoint. \n",
    "\n",
    "#### Notes\n",
    "- If `max_new_tokens` is not defined, the model may generate up to the maximum total tokens allowed, which is 4K for these models. This may result in endpoint query timeout errors, so it is recommended to set `max_new_tokens` when possible. For 7B, 13B, and 70B models, we recommend to set `max_new_tokens` no greater than 1500, 1000, and 500 respectively, while keeping the total number of tokens less than 4K.\n",
    "- In order to support a 4k context length, this model has restricted query payloads to only utilize a batch size of 1. Payloads with larger batch sizes will receive an endpoint error prior to inference.\n",
    "- This model only supports 'system', 'user' and 'assistant' roles, starting with 'system', then 'user' and alternating (u/a/u/a/u...)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f74c26e1-e8a8-42b8-8af9-f2674ff2181c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def set_meta_llama_params(\n",
    "    max_new_tokens=512,\n",
    "    top_p=0.9,\n",
    "    temperature=0.6,\n",
    "):\n",
    "    \"\"\" set Llama parameters \"\"\"\n",
    "    llama_params = {}\n",
    "    llama_params['max_new_tokens'] = max_new_tokens\n",
    "    llama_params['top_p'] = top_p\n",
    "    llama_params['temperature'] = temperature\n",
    "    return llama_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "69ed9015-86a6-4c8e-aca5-6e53c252cfec",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def print_dialog(inputs, payload, response):\n",
    "    dialog_output = []\n",
    "    for msg in inputs:\n",
    "        dialog_output.append(f\"**{msg['role'].upper()}**: {msg['content']}\\n\")\n",
    "    dialog_output.append(f\"**ASSISTANT**: {response['generated_text']}\")\n",
    "    dialog_output.append(\"\\n---\\n\")\n",
    "    \n",
    "    display(Markdown('\\n'.join(dialog_output)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "8fa6d0c7-5b37-4725-864d-80ea7873b6f4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def format_messages(messages: List[Dict[str, str]]) -> List[str]:\n",
    "    \"\"\"\n",
    "    Format messages for Llama 3+ chat models.\n",
    "    \n",
    "    The model only supports 'system', 'user' and 'assistant' roles, starting with 'system', then 'user' and \n",
    "    alternating (u/a/u/a/u...). The last message must be from 'user'.\n",
    "    \"\"\"\n",
    "    # auto assistant suffix\n",
    "    # messages.append({\"role\": \"assistant\"})\n",
    "    \n",
    "    output = \"<|begin_of_text|>\"\n",
    "    # Adding the inferred prefix\n",
    "    _system_prefix = f\"\\n\\nCutting Knowledge Date: December 2023\\nToday Date: {datetime.now().strftime('%d %b %Y')}\\n\\n\"\n",
    "    for i, entry in enumerate(messages):\n",
    "        output += f\"<|start_header_id|>{entry['role']}<|end_header_id|>\"\n",
    "        if i == 0:\n",
    "            output += f\"{_system_prefix}{entry['content']}<|eot_id|>\"\n",
    "        elif i >= 1 and 'content' in entry:\n",
    "            output += f\"\\n\\n{entry['content']}<|eot_id|>\"\n",
    "    output += \"<|start_header_id|>assistant<|end_header_id|>\\n\"\n",
    "    return output\n",
    "\n",
    "\n",
    "def send_prompt(params, prompt, instruction=\"\"):\n",
    "\n",
    "    pre_instruction = \"You are a helpful ai assistant. Keep your answers short and less than 5 sentences and only talkative when required!\"\n",
    "    if not instruction:\n",
    "        instruction = pre_instruction\n",
    "    \n",
    "    # default 'system', 'user' and 'assistant' prompt format\n",
    "    base_input = [\n",
    "        {\"role\": \"system\", \"content\": instruction},\n",
    "        {\"role\": \"user\", \"content\": prompt},\n",
    "    ]\n",
    "    # convert s/u/a format \n",
    "    optz_input = format_messages(base_input)\n",
    "    payload = {\n",
    "        \"inputs\": optz_input,\n",
    "        \"parameters\": params\n",
    "    }\n",
    "    response = pretrained_predictor.predict(payload)\n",
    "    print_dialog(base_input, payload, response)\n",
    "    return payload, response"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddaf6cc8-d3ae-4d66-96b4-83c6a688120f",
   "metadata": {},
   "source": [
    "With functions defined for the printing of the dialog and the prompt sending, let's begin sending queries to our Llama 2 LLM!\n",
    "\n",
    "Note that we can also adjust the parameters supplied of the model, which we do by altering the `top_p parameter`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "fa756656-460e-4b22-bd8f-7a5a9beff415",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "**SYSTEM**: You are a helpful ai assistant. Keep your answers short and less than 5 sentences and only talkative when required!\n",
       "\n",
       "**USER**: What is the recipe of a pumpkin pie?\n",
       "\n",
       "**ASSISTANT**: To make a classic pumpkin pie, you'll need:\n",
       "\n",
       "- 1 cup of pumpkin puree\n",
       "- 1 cup of heavy cream\n",
       "- 1/2 cup of sugar\n",
       "- 1/2 teaspoon of salt\n",
       "- 1/2 teaspoon of ground cinnamon\n",
       "- 1/4 teaspoon of ground nutmeg\n",
       "- 1/4 teaspoon of ground ginger\n",
       "- 2 large eggs\n",
       "- 1 pie crust (homemade or store-bought)\n",
       "\n",
       "Mix the pumpkin puree, heavy cream, sugar, salt, cinnamon, nutmeg, and ginger in a bowl. Add the eggs and mix well. Pour the mixture into the pie crust and bake at 425¬∞F (220¬∞C) for 15 minutes, then reduce heat to 350¬∞F (180¬∞C) and bake for an additional 30-40 minutes.\n",
       "\n",
       "---\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 5.16 ms, sys: 690 Œºs, total: 5.85 ms\n",
      "Wall time: 6.05 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "params = set_meta_llama_params(top_p=0.4)\n",
    "payload, response = send_prompt(params, prompt=\"What is the recipe of a pumpkin pie?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "7f17ee20-1e48-4069-af69-f22ccc37c016",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "**SYSTEM**: always answer with Haiku\n",
       "\n",
       "**USER**: How do I learn to play the guitar?\n",
       "\n",
       "**ASSISTANT**: Fingers on the frets\n",
       "Practice every single day\n",
       "Music in my soul\n",
       "\n",
       "---\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 4.67 ms, sys: 0 ns, total: 4.67 ms\n",
      "Wall time: 609 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "params = set_meta_llama_params(top_p=0.6)\n",
    "payload, response = send_prompt(params, prompt=\"How do I learn to play the guitar?\", instruction=\"always answer with Haiku\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "75b17e6c-4366-4b28-8427-0f6017697c0b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "**SYSTEM**: always answer with emojis\n",
       "\n",
       "**USER**: What's a good strategy for chess?\n",
       "\n",
       "**ASSISTANT**: ü§î\n",
       "\n",
       "1. **Control the Center**: The center of the board is the most important area in chess. Try to control as many central squares as possible with your pawns and pieces. This gives you more mobility and makes it harder for your opponent to maneuver. üìç\n",
       "\n",
       "2. **Develop Your Pieces**: Move your pieces out from their starting positions and develop them towards the center of the board. This helps you control more squares and attack your opponent's position. üöÄ\n",
       "\n",
       "3. **Protect Your King**: Keep your king safe by castling (moving your king to the side of the board and placing your rook in front) and placing pieces in front to block potential attacks. üè∞\n",
       "\n",
       "4. **Pawn Structure**: Manage your pawns carefully, as they can either support or weaken your position. Try to create pawn chains (rows of pawns) to block your opponent's pieces and create barriers. üöß\n",
       "\n",
       "5. **Analyze Your Opponent's Moves**: Observe your opponent's strategy and adjust your plan accordingly. Look for weaknesses in their position and try to exploit them. üîç\n",
       "\n",
       "6. **Be Patient**: Chess is a game of strategy, not a sprint. Take your time to think and plan your moves carefully. Avoid making impulsive decisions that might put you at risk. ‚è∞\n",
       "\n",
       "7. **Endgame Play**: In the endgame, focus on promoting your pawns to queens and rooks, as they are the most powerful pieces. Try to create a passed pawn (a pawn that has no opposing pawn on the same file) to promote it to a queen. üèÜ\n",
       "\n",
       "---\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 5.17 ms, sys: 0 ns, total: 5.17 ms\n",
      "Wall time: 11.9 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "params = set_meta_llama_params(top_p=0.8)\n",
    "payload, response = send_prompt(params, prompt=\"What's a good strategy for chess?\", instruction=\"always answer with emojis\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "c7a7140f-aba9-4466-89ef-39569f1f73e5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "**SYSTEM**: You are a helpful ai assistant. Keep your answers short and less than 5 sentences and only talkative when required!\n",
       "\n",
       "**USER**: What are the top 5 things to do in Tokyo?\n",
       "\n",
       "**ASSISTANT**: Tokyo offers a wide range of activities. Here are the top 5 things to do in Tokyo:\n",
       "\n",
       "1. **Visit the Tokyo Skytree**: At 634 meters tall, it's the tallest tower in the world, offering breathtaking views of the city.\n",
       "2. **Explore the Shibuya Crossing**: This famous intersection is known for its busiest and most colorful street scene in the world.\n",
       "3. **Visit the Meiji Shrine**: Dedicated to the deified spirits of Emperor Meiji and his wife, Empress Shoken, this shrine is a serene oasis in the midst of the bustling city.\n",
       "4. **Walk through the Asakusa district**: This historic district is home to the famous Senso-ji Temple, a colorful and lively area filled with traditional shops and food stalls.\n",
       "5. **Visit the Tsukiji Outer Market**: While the inner market has moved to a new location, the outer market still offers a fascinating glimpse into Tokyo's seafood culture and fresh sushi.\n",
       "\n",
       "---\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 6.27 ms, sys: 0 ns, total: 6.27 ms\n",
      "Wall time: 6.96 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "params = set_meta_llama_params(top_p=0.6)\n",
    "tokyo_payload, tokyo_response = send_prompt(params, prompt=\"What are the top 5 things to do in Tokyo?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37ba9ead-8c0d-448b-96eb-525e19239860",
   "metadata": {},
   "source": [
    "Because we are interacting with the llama2 **chat** LLM, we can input a previous prompt with a further question in a conversation manner. \n",
    "\n",
    "Also, because we are capturing the payload and response for each inference to our endpoint, we can feed this back into our LLM as part of our next prompt, in order to continue the conversation. In the following output we can see the requests and repsonses from the user, and the assistant:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "34179c7a-7f2d-4392-a404-d8fb026af9ca",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "**SYSTEM**: You are a helpful ai assistant. Keep your answers short and less than 5 sentences and only talkative when required!\n",
       "\n",
       "**USER**: What are the top 5 things to do in Tokyo?\n",
       "\n",
       "**ASSISTANT**: Tokyo offers a wide range of activities. Here are the top 5 things to do in Tokyo:\n",
       "\n",
       "1. **Visit the Tokyo Skytree**: At 634 meters tall, it's the tallest tower in the world, offering breathtaking views of the city.\n",
       "2. **Explore the Shibuya Crossing**: This famous intersection is known for its busiest and most colorful street scene in the world.\n",
       "3. **Visit the Meiji Shrine**: Dedicated to the deified spirits of Emperor Meiji and his wife, Empress Shoken, this shrine is a serene oasis in the midst of the bustling city.\n",
       "4. **Walk through the Asakusa district**: This historic district is home to the famous Senso-ji Temple, a colorful and lively area filled with traditional shops and food stalls.\n",
       "5. **Visit the Tsukiji Outer Market**: While the inner market has moved to a new location, the outer market still offers a fascinating glimpse into Tokyo's seafood culture and fresh sushi.\n",
       "\n",
       "**USER**: What is so great about #1?\n",
       "\n",
       "**ASSISTANT**: **Tokyo Skytree** is great because it offers:\n",
       "\n",
       "- **Panoramic views**: From its observation decks, you can see the entire city, including famous landmarks like the Tokyo Tower and Mount Fuji on a clear day.\n",
       "- **Interactive exhibits**: The tower has interactive exhibits and a museum that showcase Tokyo's history, culture, and technology.\n",
       "- **Shopping and dining**: The complex surrounding the tower has a variety of shops, restaurants, and cafes to explore.\n",
       "\n",
       "---\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 16.4 ms, sys: 594 Œºs, total: 17 ms\n",
      "Wall time: 3.5 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "base_input = [\n",
    "    {\n",
    "        \"role\": \"system\", \n",
    "        \"content\": \"You are a helpful ai assistant. Keep your answers short and less than 5 sentences and only talkative when required!\"\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"user\", \n",
    "        \"content\": \"What are the top 5 things to do in Tokyo?\"},\n",
    "    {\n",
    "        \"role\": \"assistant\",\n",
    "        \"content\": tokyo_response['generated_text'],\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"user\", \n",
    "        \"content\": \"What is so great about #1?\"  # <<---- Your follow up question here!\n",
    "    },\n",
    "]\n",
    "optz_input = format_messages(base_input)\n",
    "\n",
    "payload = {\n",
    "    \"inputs\": optz_input,\n",
    "    \"parameters\": {\n",
    "        \"max_new_tokens\": 512, \n",
    "        \"top_p\": 0.9, \n",
    "        \"temperature\": 0.6\n",
    "    }\n",
    "}\n",
    "response = pretrained_predictor.predict(payload, custom_attributes=custom_attribute)\n",
    "print_dialog(base_input, payload, response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e52bc81-276b-46ef-99ce-dbad16d67219",
   "metadata": {},
   "source": [
    "---\n",
    "### Building an agent with LangChain\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a072df6-54f4-4a89-9a99-d1621db93d48",
   "metadata": {},
   "source": [
    "We now have a LLM that can continue conversations in a chat interface! However, there is a more effective option than manually capturing the request and response for each inference request and feeding this back into the model.\n",
    "\n",
    "[LangChain](https://www.langchain.com/) is a framework that helps us simplify this process. We can use LangChain to send prompts to our LLM, store chat histroy, and feed this back into the model in order to have a conversation.\n",
    "\n",
    "LangChain also allows us to define a content header to transform the inputs and outputs to the LLM, which we will do in the next cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "b99ae448-4543-486d-b239-78dda69b5d86",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from typing import Dict\n",
    "from langchain.llms import SagemakerEndpoint\n",
    "from langchain.llms.sagemaker_endpoint import LLMContentHandler\n",
    "\n",
    "class ContentHandler(LLMContentHandler):\n",
    "    content_type = \"application/json\"\n",
    "    accepts = \"application/json\"\n",
    "\n",
    "    def transform_input(self, prompt, model_kwargs):\n",
    "        base_input = [{\"role\" : \"user\", \"content\" : prompt}]\n",
    "        optz_input = format_messages(base_input)\n",
    "        input_str = json.dumps({\n",
    "            \"inputs\" : optz_input, \n",
    "            \"parameters\" : {**model_kwargs}\n",
    "        })\n",
    "        return input_str.encode('utf-8')\n",
    "    \n",
    "    def transform_output(self, output):\n",
    "        response_json = json.loads(output.read().decode(\"utf-8\"))\n",
    "        return response_json[\"generated_text\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89cda634-f183-47f8-8657-c50538f5e97a",
   "metadata": {},
   "source": [
    "We can then pass the SageMaker endpoint we previoiusly provisioned into a LangChain `SageMaker Endpoint` object, which allows LangChain to interact with out Llama 2 LLM. We are also passing in parameters which we defined previously."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "d5eeff19-1f21-4e53-b866-0a03df3676a1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import json\n",
    "from sagemaker import session\n",
    "\n",
    "content_handler = ContentHandler()\n",
    "\n",
    "llm=SagemakerEndpoint(\n",
    "     endpoint_name=pretrained_predictor.endpoint_name, \n",
    "     region_name=session.Session().boto_region_name, \n",
    "     model_kwargs={\n",
    "         \"max_new_tokens\": 400, \n",
    "         \"top_p\": 0.9, \n",
    "         \"temperature\": 0.6\n",
    "     },\n",
    "     endpoint_kwargs={\n",
    "         \"CustomAttributes\": custom_attribute\n",
    "     },\n",
    "     content_handler=content_handler\n",
    " )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93c06a94-ff1c-48c7-8257-e104fdf23827",
   "metadata": {},
   "source": [
    "We can now create a chat prompt template that LangChain will pass to our LLM. The LangChain [ChatPromptTemplate](https://python.langchain.com/docs/modules/model_io/prompts/prompt_templates/#chatprompttemplate) object allows us to do this.\n",
    "\n",
    "We also have [ConversationBufferMemory](https://api.python.langchain.com/en/latest/memory/langchain.memory.buffer.ConversationBufferMemory.html) and [LLMChain](https://docs.langchain.com/docs/components/chains/llm-chain) objects. The former allows to store the conversation memory, and the latter brings together the Chat Prompt Template, LLM, and Conversation Buffer Memory. We also set `verbose` to `True`, allowing us, in this case, to see the conversation history up until this point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "37e00237-6040-4a13-9992-563d3c74a112",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain.prompts import (\n",
    "    ChatPromptTemplate,\n",
    "    MessagesPlaceholder,\n",
    "    SystemMessagePromptTemplate,\n",
    "    HumanMessagePromptTemplate,\n",
    ")\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.memory import ConversationBufferWindowMemory\n",
    "\n",
    "# Prompt \n",
    "prompt = ChatPromptTemplate(\n",
    "    messages=[\n",
    "        SystemMessagePromptTemplate.from_template(\n",
    "            \"Assistant is a chatbot having a conversation with a human. Assistant is informative and polite, and only answers the question asked.\"\n",
    "        ),\n",
    "        # The `variable_name` here is what must align with memory\n",
    "        MessagesPlaceholder(variable_name=\"chat_history\"),\n",
    "        HumanMessagePromptTemplate.from_template(\"{question}\")\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Notice that we set`return_messages=True` to fit into the MessagesPlaceholder\n",
    "# Notice that `\"chat_history\"` aligns with the MessagesPlaceholder name\n",
    "memory = ConversationBufferWindowMemory(\n",
    "    memory_key=\"chat_history\",\n",
    "    return_messages=True, \n",
    "    k=2\n",
    ")\n",
    "\n",
    "conversation = LLMChain(\n",
    "    llm=llm,\n",
    "    prompt=prompt,\n",
    "    verbose=True,\n",
    "    memory=memory\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d91bcec1-bf84-4c2c-bee0-a4ef0e613e5f",
   "metadata": {},
   "source": [
    "Now we have our conversation LLM Chain, LangChain will pass our query, as well as the history and chat prompt template, to the LLM. This is great for a chatbot interface, as we'll demonstrate now. For each of the following three cells' output, you'll notice the conversation history after the text `Entering new LLMChain chain...`, and the query response after `Finished chain.`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "151d3c37-ef28-4352-8319-d83344d6803a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Notice that we just pass in the `question` variables - `chat_history` gets populated by memory\n",
    "def simple_conversation(question):\n",
    "    print(conversation({\"question\": question})['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "5bf54e80-cdd2-4244-b8f5-846a58c8698b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2785/514790715.py:3: LangChainDeprecationWarning: The method `Chain.__call__` was deprecated in langchain 0.1.0 and will be removed in 1.0. Use invoke instead.\n",
      "  print(conversation({\"question\": question})['text'])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mSystem: Assistant is a chatbot having a conversation with a human. Assistant is informative and polite, and only answers the question asked.\n",
      "Human: hi!\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "Hello! It's nice to meet you. Is there something I can help you with or would you like to chat?\n"
     ]
    }
   ],
   "source": [
    "simple_conversation('hi!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "d87fe2ce-8129-4ed8-b2f7-39588d6e9429",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mSystem: Assistant is a chatbot having a conversation with a human. Assistant is informative and polite, and only answers the question asked.\n",
      "Human: hi!\n",
      "AI: Hello! It's nice to meet you. Is there something I can help you with or would you like to chat?\n",
      "Human: How can I travel from New York to Los Angeles?\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "There are several ways to travel from New York to Los Angeles, depending on your budget, time constraints, and personal preferences. Here are a few options:\n",
      "\n",
      "1. **Flying:** The fastest way to reach Los Angeles from New York is by flying. You can take a direct flight from one of New York's three major airports (JFK, LGA, or EWR) to Los Angeles International Airport (LAX). Flight duration is approximately 5 hours.\n",
      "\n",
      "2. **Driving:** If you prefer a road trip, you can drive from New York to Los Angeles. The distance is approximately 2,796 miles and takes around 40-50 hours with normal traffic conditions. You can take I-80 W and I-10 W to reach Los Angeles.\n",
      "\n",
      "3. **Train:** You can also take the train from New York's Penn Station to Los Angeles' Union Station. The journey takes around 67 hours, depending on the type of train and route you choose. Amtrak offers this service.\n",
      "\n",
      "4. **Bus:** Taking the bus is another option, with companies like Greyhound and Megabus offering routes from New York to Los Angeles. The journey takes around 64-72 hours, depending on the route and traffic conditions.\n",
      "\n",
      "5. **Cruise:** If you prefer a more leisurely journey, you can take a cruise from New York to Los Angeles. This option is more expensive and takes around 7-10 days, depending on the cruise line and route.\n",
      "\n",
      "Which option sounds most appealing to you?\n"
     ]
    }
   ],
   "source": [
    "simple_conversation(\"How can I travel from New York to Los Angeles?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "cba810dc-4dce-4537-b52a-585bca0af5dc",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mSystem: Assistant is a chatbot having a conversation with a human. Assistant is informative and polite, and only answers the question asked.\n",
      "Human: hi!\n",
      "AI: Hello! It's nice to meet you. Is there something I can help you with or would you like to chat?\n",
      "Human: How can I travel from New York to Los Angeles?\n",
      "AI: There are several ways to travel from New York to Los Angeles, depending on your budget, time constraints, and personal preferences. Here are a few options:\n",
      "\n",
      "1. **Flying:** The fastest way to reach Los Angeles from New York is by flying. You can take a direct flight from one of New York's three major airports (JFK, LGA, or EWR) to Los Angeles International Airport (LAX). Flight duration is approximately 5 hours.\n",
      "\n",
      "2. **Driving:** If you prefer a road trip, you can drive from New York to Los Angeles. The distance is approximately 2,796 miles and takes around 40-50 hours with normal traffic conditions. You can take I-80 W and I-10 W to reach Los Angeles.\n",
      "\n",
      "3. **Train:** You can also take the train from New York's Penn Station to Los Angeles' Union Station. The journey takes around 67 hours, depending on the type of train and route you choose. Amtrak offers this service.\n",
      "\n",
      "4. **Bus:** Taking the bus is another option, with companies like Greyhound and Megabus offering routes from New York to Los Angeles. The journey takes around 64-72 hours, depending on the route and traffic conditions.\n",
      "\n",
      "5. **Cruise:** If you prefer a more leisurely journey, you can take a cruise from New York to Los Angeles. This option is more expensive and takes around 7-10 days, depending on the cruise line and route.\n",
      "\n",
      "Which option sounds most appealing to you?\n",
      "Human: Can you tell me more about the first option?\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "The first option is flying from New York to Los Angeles. Here are some more details to consider:\n",
      "\n",
      "**Airports:** As I mentioned earlier, there are three major airports in New York: John F. Kennedy International Airport (JFK), LaGuardia Airport (LGA), and Newark Liberty International Airport (EWR). You can fly from any of these airports to Los Angeles International Airport (LAX).\n",
      "\n",
      "**Flight duration:** As I mentioned earlier, the flight duration from New York to Los Angeles is approximately 5 hours. However, please note that this can vary depending on several factors such as flight route, airline, and weather conditions.\n",
      "\n",
      "**Flight frequency:** There are numerous flights available from New York to Los Angeles, with many airlines operating multiple flights per day. Some popular airlines that operate this route include American Airlines, Delta Air Lines, United Airlines, and Alaska Airlines.\n",
      "\n",
      "**Cost:** The cost of flights from New York to Los Angeles can vary greatly depending on the airline, time of year, and how far in advance you book. On average, you can expect to pay between $200-$500 for a one-way economy ticket.\n",
      "\n",
      "**Tips:**\n",
      "\n",
      "* Be sure to check for any travel restrictions or requirements, such as COVID-19 testing or vaccination certificates.\n",
      "* Book your flight in advance to secure the best prices.\n",
      "* Consider flying into a different airport in Los Angeles, such as Long Beach Airport (LGB) or Burbank Airport (BUR), if you have specific plans or preferences.\n",
      "* Check for any layovers or connections, and plan accordingly.\n",
      "\n",
      "Is there anything specific you'd like to know about flying from New York to Los Angeles?\n"
     ]
    }
   ],
   "source": [
    "simple_conversation(\"Can you tell me more about the first option?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a8f1f43-c23f-47e5-9044-8200541490b6",
   "metadata": {
    "tags": []
   },
   "source": [
    "___\n",
    "### LangChain Tools\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3c32f20-bf77-42be-ae34-5d14fedafea8",
   "metadata": {},
   "source": [
    "LangChain, further, has [tools](https://python.langchain.com/docs/modules/agents/tools/) which it can use to send API requests to perform various tasks which it may not have been able to do in isolation, such as make a search request or check the weather. Today, we will be using a math, and a Wikipedia tool, though please see a more complete list [here](https://js.langchain.com/docs/api/tools/). It is also possible to [create your own tool](https://python.langchain.com/docs/modules/agents/tools/custom_tools).\n",
    "\n",
    "We also use LangChain [agents](https://docs.langchain.com/docs/components/agents/). Agents are especially powerful where there is not a predetermined chain of calls, like we've had above so far. It is possible to have an unknown chain that depends on the user's input. In these types of chains, there is a ‚Äúagent‚Äù which has access to a suite of tools. Depending on the user input, the agent can then decide which, if any, of these tools to call. An agent could call multiple LLM Chains that we defined above, each with their own tools. They can also be extended with custom logic to allow for retries, and error handling.\n",
    "\n",
    "Defining our two tools, as well as the LangChain agent, will give us a model that will able to determine whether it needs to use Wikipedia or a math tool, or whether it is able to answer a question on its own. If it needs the tool, it will make a request to the tool, receive the response, and then return that response to the user.\n",
    "\n",
    "We also define an Output Parser, which is a method of parsing the output from the prompt. If the LLM produces output uses certain headers, we can enable complex interactions where variables are generated by the LLM in their response and passed into the next step of the chain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "446b88e0-2343-45e9-acdf-6eaca826e540",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain.agents import AgentOutputParser\n",
    "from langchain.agents.conversational_chat.prompt import FORMAT_INSTRUCTIONS\n",
    "from langchain.output_parsers.json import parse_json_markdown\n",
    "from langchain.schema import AgentAction, AgentFinish\n",
    "\n",
    "class OutputParser(AgentOutputParser):\n",
    "    def get_format_instructions(self) -> str:\n",
    "        return FORMAT_INSTRUCTIONS\n",
    "\n",
    "    def parse(self, text: str) -> AgentAction | AgentFinish:\n",
    "        try:\n",
    "            # this will work IF the text is a valid JSON with action and action_input\n",
    "            response = parse_json_markdown(text)\n",
    "            action, action_input = response[\"step\"], response[\"step_input\"]\n",
    "            if action == \"Final Answer\":\n",
    "                # this means the agent is finished so we call AgentFinish\n",
    "                return AgentFinish({\"output\": action_input}, text)\n",
    "            else:\n",
    "                # otherwise the agent wants to use an action, so we call AgentAction\n",
    "                return AgentAction(action, action_input, text)\n",
    "        except Exception:\n",
    "            # sometimes the agent will return a string that is not a valid JSON\n",
    "            # often this happens when the agent is finished\n",
    "            # so we just return the text as the output\n",
    "            return AgentFinish({\"output\": text}, text)\n",
    "\n",
    "    @property\n",
    "    def _type(self) -> str:\n",
    "        return \"conversational_chat\"\n",
    "\n",
    "# initialize output parser for agent\n",
    "parser = OutputParser()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbb731e7-34a2-407b-b4bf-70fc386c4b27",
   "metadata": {},
   "source": [
    "We initialize the agent with the tools we have defined above, the [agent type](https://python.langchain.com/docs/modules/agents/agent_types/), as well as the LLM, memory, and output parser we defined above. Again we set `Verbose` to `True`, which in this case will allow us to see if and how the agent calls a tool it has access to."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "8f4b4462-e2c0-4390-a55f-8744c640b441",
   "metadata": {},
   "outputs": [],
   "source": [
    "memory = ConversationBufferWindowMemory(\n",
    "    memory_key=\"chat_history\", \n",
    "    k=8, \n",
    "    return_messages=True, \n",
    "    output_key=\"output\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "6fcfa5a5-8c68-451b-91e4-dae42030afa3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain.agents import initialize_agent\n",
    "from langchain.agents import AgentOutputParser, load_tools\n",
    "\n",
    "llm=SagemakerEndpoint(\n",
    "     endpoint_name=pretrained_predictor.endpoint_name, \n",
    "     region_name=session.Session().boto_region_name, \n",
    "     model_kwargs={\n",
    "         \"max_new_tokens\": 400, \n",
    "         \"top_p\": 0.1, \n",
    "         \"temperature\": 0.2\n",
    "     },\n",
    "     endpoint_kwargs={\"CustomAttributes\": custom_attribute},\n",
    "     content_handler=content_handler\n",
    " )\n",
    "\n",
    "# equip agents with tools\n",
    "tools = load_tools([\"llm-math\", \"wikipedia\"], llm=llm)\n",
    "\n",
    "# initialize agent\n",
    "agent = initialize_agent(\n",
    "    agent=\"chat-conversational-react-description\",\n",
    "    memory=memory,\n",
    "    max_iterations=2,\n",
    "    llm=llm,\n",
    "    handle_parsing_errors=\"Check your output and make sure it conforms. It must be entirely in JSON!\",\n",
    "    tools=tools,\n",
    "    verbose=True,\n",
    "    agent_kwargs={\n",
    "        \"output_parser\": parser\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97e8bf5a-75d1-43f4-98c3-d620184fe8ec",
   "metadata": {},
   "source": [
    "We also provide a background prompt to the model. This provides the LLM with instructions of the tools it has access to, when to use which, and how to use each. This allows the LLM to firstly know when to use a tool (as opposed to answering in isolation 'by itself'), but also allows the LangChain agent to create a request to the tool the LLM has identified, before returning to the LLM to respond in a natural language way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "ec5959a7-70bb-4e38-81b6-e47fc6e71b65",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# create the system message\n",
    "system_message = \"\"\"<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nAssistant is a expert JSON builder designed to assist with a wide range of tasks.\n",
    "\n",
    "Assistant is able to respond to the User and use tools using JSON strings that contain \"step\" and \"step_input\" parameters.\n",
    "\n",
    "All of Assistant's communication is performed using this JSON format.\n",
    "\n",
    "Assistant can also use tools by responding to the user with tool use instructions in the same \"step\" and \"step_input\" JSON format. Tools available to Assistant are:\n",
    "\n",
    "- \"Calculator\": Useful for when you need to answer questions about math.\n",
    "  - To use the calculator tool, Assistant should write like so:\n",
    "    ```json\n",
    "    {{\"step\": \"Calculator\",\n",
    "      \"step_input\": \"sqrt(4)\"}}\n",
    "    ```\n",
    "\n",
    "- \"wikipedia\": Useful when you need a summary of a person, place, historical event, or other subject. Input is typically a noun, like a person, place, historical event, or another subject.\n",
    "  - To use the wikipedia tool, Assistant should format the JSON like the following before getting the response and returning to the user:\n",
    "    ```json\n",
    "    {{\"step\": \"wikipedia\",\n",
    "      \"step_input\": \"Statue of Liberty\"}}\n",
    "\n",
    "When Assistant responds with JSON they make sure to enclose the JSON with three back ticks.\n",
    "\n",
    "Here are some previous conversations between the Assistant and User:\n",
    "\n",
    "User: Hey how are you today?\n",
    "Assistant: ```json\n",
    "{{\"step\": \"Final Answer\",\n",
    " \"step_input\": \"I'm good thanks, how are you?\"}}\n",
    "\\```\n",
    "User: I'm great, what is the square root of 4?\n",
    "Assistant: ```json\n",
    "{{\"step\": \"Calculator\",\n",
    " \"step_input\": \"sqrt(4)\"}}\n",
    "\\```\n",
    "User: Who is the President of the United States of America?\n",
    "Assistant: ```json\n",
    "{{\"step\": \"wikipedia\",\n",
    " \"step_input\": \"President of United States of America\"}}\n",
    "\\```\n",
    "User: What is 9 cubed?\n",
    "Assistant: ```\n",
    "{{\"step\": \"Calculator\",\n",
    " \"step_input\": \"9**3\"}}\n",
    "\\```\n",
    "User: 729\n",
    "Assistant: ```\n",
    "{{\"step\": \"Final Answer\",\n",
    " \"step_input\": \"The answer to your question is 729.\"}}\n",
    "\\```\n",
    "User: Can you tell me about the Statue of Liberty?\n",
    "Assistant: ```\n",
    "{{\"step\": \"wikipedia\",\n",
    " \"step_input\": \"Statue of Liberty\"}}\n",
    "\\```\n",
    "User: What is the square root of 81?\n",
    "Assistant: ```\n",
    "{{\"step\": \"Calculator\",\n",
    " \"step_input\": \"sqrt(81)\"}}\n",
    "\\```\n",
    "User: 9\n",
    "Assistant: ```\n",
    "{{\"step\": \"Final Answer\",\n",
    " \"step_input\": \"The answer to your question is 9.\"}}\n",
    "\\```\n",
    "\n",
    "Here is the latest conversation between Assistant and User.<|eot_id|>\"\"\"\n",
    "\n",
    "few_shot = agent.agent.create_prompt(\n",
    "    system_message=system_message,\n",
    "    tools=tools\n",
    ")\n",
    "agent.agent.llm_chain.prompt = few_shot\n",
    "\n",
    "human_msg = \"<|start_header_id|>user<|end_header_id|>\\nRespond to the following in JSON with 'step' and 'step_input' values\\nUser: {input}\"\n",
    "\n",
    "agent.agent.llm_chain.prompt.messages[2].prompt.template = human_msg"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "962a1862-e5bf-4ff7-b36e-fee0f9afddda",
   "metadata": {},
   "source": [
    "We can now send some prompts to the LLM and see when/how it uses the tools!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "e4117358-0847-4296-8e28-79e170752f1d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def agent_conversation(question):\n",
    "    print(agent(question))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "4adfd0b0-6a04-4d1b-9667-1b1b1232f62c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m```json\n",
      "{\"step\": \"Final Answer\",\n",
      " \"step_input\": \"I'm good thanks, how are you?\"}\n",
      "```\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "{'input': 'hey how are you today?', 'chat_history': [HumanMessage(content='hey how are you today?'), AIMessage(content=\"I'm good thanks, how are you?\"), HumanMessage(content='Tell me about the Empire Statue Building'), AIMessage(content='Agent stopped due to iteration limit or time limit.'), HumanMessage(content='hey how are you today?'), AIMessage(content=\"I'm good thanks, how are you?\"), HumanMessage(content='Tell me about the Empire Statue Building'), AIMessage(content='Agent stopped due to iteration limit or time limit.')], 'output': \"I'm good thanks, how are you?\"}\n"
     ]
    }
   ],
   "source": [
    "agent_conversation(\"hey how are you today?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "3ed7fdf3-3d27-4f18-b7a1-ef6287e4ac2c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m```json\n",
      "{\"step\": \"wikipedia\",\n",
      " \"step_input\": \"Empire State Building\"}\n",
      "```\u001b[0m\n",
      "Observation: \u001b[33;1m\u001b[1;3mPage: Empire State Building\n",
      "Summary: The Empire State Building is a 102-story Art Deco skyscraper in the Midtown South neighborhood of Manhattan in New York City. The building was designed by Shreve, Lamb & Harmon and built from 1930 to 1931. Its name is derived from \"Empire State\", the nickname of the state of New York. The building has a roof height of 1,250 feet (380 m) and stands a total of 1,454 feet (443.2 m) tall, including its antenna. The Empire State Building was the world's tallest building until the first tower of the World Trade Center was topped out in 1970; following the September 11 attacks in 2001, the Empire State Building was New York City's tallest building until it was surpassed in 2012 by One World Trade Center. As of 2022, the building is the seventh-tallest building in New York City, the ninth-tallest completed skyscraper in the United States, and the 54th-tallest in the world.\n",
      "The site of the Empire State Building, on the west side of Fifth Avenue between West 33rd and 34th Streets, was developed in 1893 as the Waldorf‚ÄìAstoria Hotel. In 1929, Empire State Inc. acquired the site and devised plans for a skyscraper there. The design for the Empire State Building was changed fifteen times until it was ensured to be the world's tallest building. Construction started on March 17, 1930, and the building opened thirteen and a half months afterward on May 1, 1931. Despite favorable publicity related to the building's construction, because of the Great Depression and World War II, its owners did not make a profit until the early 1950s.\n",
      "The building's Art Deco architecture, height, and observation decks have made it a popular attraction. Around four million tourists from around the world annually visit the building's 86th- and 102nd-floor observatories; an additional indoor observatory on the 80th floor opened in 2019. The Empire State Building is an international cultural icon: it has been featured in more than 250 television series and films since the film King Kong was released in 1933. The building's size has been used as a standard of reference to describe the height and length of other structures. A symbol of New York City, the building has been named as one of the Seven Wonders of the Modern World by the American Society of Civil Engineers. It was ranked first on the American Institute of Architects' List of America's Favorite Architecture in 2007. Additionally, the Empire State Building and its ground-floor interior were designated city landmarks by the New York City Landmarks Preservation Commission in 1980, and were added to the National Register of Historic Places as a National Historic Landmark in 1986.\n",
      "\n",
      "Page: 1945 Empire State Building B-25 crash\n",
      "Summary: On July 28, 1945, a B-25 Mitchell bomber of the United States Army Air Forces crashed into the north side of the Empire State Building in New York City while flying in thick fog. The crash killed fourteen people (three crewmen and eleven people in the building), and an estimated twenty-four others were injured. Damage caused by the crash was estimated at US$1 million (equivalent to about $17 million in 2023), although the building's structural integrity was not compromised.\n",
      "\n",
      "\n",
      "\n",
      "Page: Empire State\n",
      "Summary: The Empire State is a nickname for the U.S. state of New York, adopted in the 1800s. It has been incorporated into the names of several state buildings and events.\n",
      "The source of the nickname is unknown and has puzzled many historians; as American writer Paul Eldridge put it, \"Who was the merry wag who crowned the State ... [as the Empire State]? New York would certainly raise a monument to his memory, but he made his grandiose gesture and vanished forever.\"\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3m```json\n",
      "{\"step\": \"Final Answer\",\n",
      " \"step_input\": \"The Empire State Building is a 102-story Art Deco skyscraper in the Midtown South neighborhood of Manhattan in New York City. It was the world's tallest building until the first tower of the World Trade Center was topped out in 1970, and it is currently the seventh-tallest building in New York City, the ninth-tallest completed skyscraper in the United States, and the 54th-tallest in the world.\"}\n",
      "```\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "{'input': 'Tell me about the Empire Statue Building', 'chat_history': [HumanMessage(content='hey how are you today?'), AIMessage(content=\"I'm good thanks, how are you?\"), HumanMessage(content='Tell me about the Empire Statue Building'), AIMessage(content='Agent stopped due to iteration limit or time limit.'), HumanMessage(content='hey how are you today?'), AIMessage(content=\"I'm good thanks, how are you?\"), HumanMessage(content='Tell me about the Empire Statue Building'), AIMessage(content='Agent stopped due to iteration limit or time limit.'), HumanMessage(content='hey how are you today?'), AIMessage(content=\"I'm good thanks, how are you?\")], 'output': \"The Empire State Building is a 102-story Art Deco skyscraper in the Midtown South neighborhood of Manhattan in New York City. It was the world's tallest building until the first tower of the World Trade Center was topped out in 1970, and it is currently the seventh-tallest building in New York City, the ninth-tallest completed skyscraper in the United States, and the 54th-tallest in the world.\"}\n"
     ]
    }
   ],
   "source": [
    "agent_conversation(\"Tell me about the Empire Statue Building\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "e384c14e-afb6-4c8c-8a9f-055d4c09c650",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m```json\n",
      "{\"step\": \"Calculator\",\n",
      " \"step_input\": \"4**2.1\"}\n",
      "```\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3mAnswer:  55.78856902770393\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3m```json\n",
      "{\"step\": \"Final Answer\",\n",
      " \"step_input\": \"The answer to your question is 55.78856902770393.\"}\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "{'input': 'what is 4 to the power of 2.1?', 'chat_history': [HumanMessage(content='hey how are you today?'), AIMessage(content=\"I'm good thanks, how are you?\"), HumanMessage(content='Tell me about the Empire Statue Building'), AIMessage(content='Agent stopped due to iteration limit or time limit.'), HumanMessage(content='hey how are you today?'), AIMessage(content=\"I'm good thanks, how are you?\"), HumanMessage(content='Tell me about the Empire Statue Building'), AIMessage(content='Agent stopped due to iteration limit or time limit.'), HumanMessage(content='hey how are you today?'), AIMessage(content=\"I'm good thanks, how are you?\"), HumanMessage(content='Tell me about the Empire Statue Building'), AIMessage(content=\"The Empire State Building is a 102-story Art Deco skyscraper in the Midtown South neighborhood of Manhattan in New York City. It was the world's tallest building until the first tower of the World Trade Center was topped out in 1970, and it is currently the seventh-tallest building in New York City, the ninth-tallest completed skyscraper in the United States, and the 54th-tallest in the world.\")], 'output': 'The answer to your question is 55.78856902770393.'}\n"
     ]
    }
   ],
   "source": [
    "agent_conversation(\"what is 4 to the power of 2.1?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "63240f64-374e-4553-997f-863327ffbfc2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m```json\n",
      "{\"step\": \"Calculator\",\n",
      " \"step_input\": \"sqrt(64)\"}\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3mAnswer:  8.0\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3m```json\n",
      "{\"step\": \"Final Answer\",\n",
      " \"step_input\": \"The square root of 64 is 8.\"}\n",
      "```\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "{'input': 'What is the square root of 64?', 'chat_history': [HumanMessage(content='hey how are you today?'), AIMessage(content=\"I'm good thanks, how are you?\"), HumanMessage(content='Tell me about the Empire Statue Building'), AIMessage(content='Agent stopped due to iteration limit or time limit.'), HumanMessage(content='hey how are you today?'), AIMessage(content=\"I'm good thanks, how are you?\"), HumanMessage(content='Tell me about the Empire Statue Building'), AIMessage(content='Agent stopped due to iteration limit or time limit.'), HumanMessage(content='hey how are you today?'), AIMessage(content=\"I'm good thanks, how are you?\"), HumanMessage(content='Tell me about the Empire Statue Building'), AIMessage(content=\"The Empire State Building is a 102-story Art Deco skyscraper in the Midtown South neighborhood of Manhattan in New York City. It was the world's tallest building until the first tower of the World Trade Center was topped out in 1970, and it is currently the seventh-tallest building in New York City, the ninth-tallest completed skyscraper in the United States, and the 54th-tallest in the world.\"), HumanMessage(content='what is 4 to the power of 2.1?'), AIMessage(content='The answer to your question is 55.78856902770393.')], 'output': 'The square root of 64 is 8.'}\n"
     ]
    }
   ],
   "source": [
    "agent_conversation(\"What is the square root of 64?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a0af730-0b89-4783-948e-3208b2e8218c",
   "metadata": {},
   "source": [
    "___\n",
    "### Developing and deploying the UI with Streamlit\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d1119d4-9620-49f6-9616-1f184dfd54d4",
   "metadata": {},
   "source": [
    "Let's bring all of this together and host our chatbot interface!\n",
    "\n",
    "For this we will use [Streamlit](https://streamlit.io/). Streamlit is an open-source Python library that allows you to create and deploy web applications. It can be deployed from our local machine, or from the Cloud. Today, we will deploy it directly from SageMaker Studio.\n",
    "\n",
    "The file `chat_app.py` (`../studio-local-ui/`) brings together all of what we have discussed so far. It initializes a LangChain Agent, with the tools and conversation memory we spoke about previously. It connects to our same Llama 2 LLM."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5537e33d-b843-484d-b9a8-6451f71937ec",
   "metadata": {},
   "source": [
    "The majority of this code you will be familiar with from the notebook so far. The rest uses the [Streamlit library](https://docs.streamlit.io/library/api-reference), as well as [LangChain Streamlit packages](https://python.langchain.com/docs/integrations/memory/streamlit_chat_message_history). \n",
    "\n",
    "It is one of the last lines of the file, `response = agent(prompt, callbacks=[st_cb])` that sends the prompt to the agent, as well as specifies the [StreamlitCallbackHandler](https://python.LangChain.com/docs/integrations/callbacks/streamlit) which can display the reasoning and actions in the streamlit app. By default we are not showing this in the conversation, and have a regex that filers out too much of the conversation history and thought process, though in order to see comment out the line at the end `response = re.sub(\"\\{.*?\\}\",\"\",response[\"output\"])`.\n",
    "\n",
    "We are also using [st.chat_message](https://docs.streamlit.io/library/api-reference/chat/st.chat_message) to handle the chat message container, and [st.write](https://docs.streamlit.io/library/api-reference/write-magic/st.write) to return this, along with the previous conversation, back to the UI."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7742ca6-962f-41cc-afd0-03a38dbb2f6a",
   "metadata": {
    "tags": []
   },
   "source": [
    "We can [build Streamlit apps in SageMaker Studio](https://aws.amazon.com/blogs/machine-learning/build-streamlit-apps-in-amazon-sagemaker-studio/). We will do this by hosting the app on the Jupyter Server. \n",
    "\n",
    "Firstly, let's write the output of our SageMaker endpoint to a text file so it can be read by the `app.py`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "e7a84017-a71f-499d-8903-88fc4055676e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "f = open(\"../studio-local-ui/endpoint_name.txt\", \"w\")\n",
    "f.write(pretrained_predictor.endpoint_name)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "c292de59-8e0b-4260-b8c6-325a1e1f9744",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "f = open(\"../studio-local-ui/custom_attribute.txt\", \"w\")\n",
    "f.write(custom_attribute)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9de74a99-c6ce-4fa2-bb0b-abe8512ece55",
   "metadata": {},
   "source": [
    "Run the following cells marked with `%%bash`, these cells will install a few packages in your conda environment and spin up a new Streamlit UI that's accessible from the URL described below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "4fde565f-7728-4524-8ff4-196081dbb932",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading package lists...\n",
      "Building dependency tree...\n",
      "Reading state information...\n",
      "The following additional packages will be installed:\n",
      "  libjq1 libonig5\n",
      "The following NEW packages will be installed:\n",
      "  jq libjq1 libonig5\n",
      "0 upgraded, 3 newly installed, 0 to remove and 0 not upgraded.\n",
      "Need to get 357 kB of archives.\n",
      "After this operation, 1087 kB of additional disk space will be used.\n",
      "Get:1 http://archive.ubuntu.com/ubuntu jammy/main amd64 libonig5 amd64 6.9.7.1-2build1 [172 kB]\n",
      "Get:2 http://archive.ubuntu.com/ubuntu jammy/main amd64 libjq1 amd64 1.6-2.1ubuntu3 [133 kB]\n",
      "Get:3 http://archive.ubuntu.com/ubuntu jammy/main amd64 jq amd64 1.6-2.1ubuntu3 [52.5 kB]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "debconf: delaying package configuration, since apt-utils is not installed\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetched 357 kB in 1s (611 kB/s)\n",
      "Selecting previously unselected package libonig5:amd64.\n",
      "(Reading database ... 13790 files and directories currently installed.)\n",
      "Preparing to unpack .../libonig5_6.9.7.1-2build1_amd64.deb ...\n",
      "Unpacking libonig5:amd64 (6.9.7.1-2build1) ...\n",
      "Selecting previously unselected package libjq1:amd64.\n",
      "Preparing to unpack .../libjq1_1.6-2.1ubuntu3_amd64.deb ...\n",
      "Unpacking libjq1:amd64 (1.6-2.1ubuntu3) ...\n",
      "Selecting previously unselected package jq.\n",
      "Preparing to unpack .../jq_1.6-2.1ubuntu3_amd64.deb ...\n",
      "Unpacking jq (1.6-2.1ubuntu3) ...\n",
      "Setting up libonig5:amd64 (6.9.7.1-2build1) ...\n",
      "Setting up libjq1:amd64 (1.6-2.1ubuntu3) ...\n",
      "Setting up jq (1.6-2.1ubuntu3) ...\n",
      "Processing triggers for libc-bin (2.35-0ubuntu3.8) ...\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "sudo apt-get install -yq jq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69e1e9ee-7fd6-4e2d-8352-ff827daf4eab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=====>  Launch Streamlit: https://tu6mkavngfsynef.studio.us-east-1.sagemaker.aws/jupyterlab/default/proxy/8501/\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sagemaker-user/sagemaker-studio-foundation-models/studio-local-ui/chat_app.py:1: LangChainDeprecationWarning: Importing StreamlitCallbackHandler from /opt/conda/lib/python3.10/site-packages/langchain/callbacks/__init__.py is deprecated. Please replace deprecated imports:\n",
      "\n",
      ">> from /opt/conda/lib/python3.10/site-packages/langchain/callbacks/__init__.py import StreamlitCallbackHandler\n",
      "\n",
      "with new imports of:\n",
      "\n",
      ">> from langchain_community.callbacks.streamlit import StreamlitCallbackHandler\n",
      "You can use the langchain cli to **automatically** upgrade many imports. Please see documentation here <https://python.langchain.com/v0.2/docs/versions/v0_2/>\n",
      "  from langchain.callbacks import StreamlitCallbackHandler\n",
      "/home/sagemaker-user/sagemaker-studio-foundation-models/studio-local-ui/chat_app.py:2: LangChainDeprecationWarning: Importing StreamlitChatMessageHistory from langchain.memory.chat_message_histories is deprecated. Please replace deprecated imports:\n",
      "\n",
      ">> from langchain.memory.chat_message_histories import StreamlitChatMessageHistory\n",
      "\n",
      "with new imports of:\n",
      "\n",
      ">> from langchain_community.chat_message_histories import StreamlitChatMessageHistory\n",
      "You can use the langchain cli to **automatically** upgrade many imports. Please see documentation here <https://python.langchain.com/v0.2/docs/versions/v0_2/>\n",
      "  from langchain.memory.chat_message_histories import StreamlitChatMessageHistory\n",
      "/opt/conda/lib/python3.10/site-packages/langchain/llms/__init__.py:549: LangChainDeprecationWarning: Importing LLMs from langchain is deprecated. Importing from langchain will no longer be supported as of langchain==0.2.0. Please import from langchain-community instead:\n",
      "\n",
      "`from langchain_community.llms import SagemakerEndpoint`.\n",
      "\n",
      "To install langchain-community run `pip install -U langchain-community`.\n",
      "  warnings.warn(\n",
      "/home/sagemaker-user/sagemaker-studio-foundation-models/studio-local-ui/chat_app.py:4: LangChainDeprecationWarning: Importing LLMContentHandler from langchain.llms is deprecated. Please replace deprecated imports:\n",
      "\n",
      ">> from langchain.llms import LLMContentHandler\n",
      "\n",
      "with new imports of:\n",
      "\n",
      ">> from langchain_community.llms.sagemaker_endpoint import LLMContentHandler\n",
      "You can use the langchain cli to **automatically** upgrade many imports. Please see documentation here <https://python.langchain.com/v0.2/docs/versions/v0_2/>\n",
      "  from langchain.llms.sagemaker_endpoint import LLMContentHandler\n",
      "/home/sagemaker-user/sagemaker-studio-foundation-models/studio-local-ui/chat_app.py:6: LangChainDeprecationWarning: Importing load_tools from langchain.agents is deprecated. Please replace deprecated imports:\n",
      "\n",
      ">> from langchain.agents import load_tools\n",
      "\n",
      "with new imports of:\n",
      "\n",
      ">> from langchain_community.agent_toolkits.load_tools import load_tools\n",
      "You can use the langchain cli to **automatically** upgrade many imports. Please see documentation here <https://python.langchain.com/v0.2/docs/versions/v0_2/>\n",
      "  from langchain.agents import load_tools, AgentOutputParser, initialize_agent, ConversationalChatAgent, AgentExecutor\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "cd ../studio-local-ui\n",
    "DOMAIN_ID=$(jq -r '.DomainId' /opt/ml/metadata/resource-metadata.json)\n",
    "SPACE_NAME=$(jq -r '.SpaceName' /opt/ml/metadata/resource-metadata.json)\n",
    "STREAMLIT_URL=$(aws sagemaker describe-space --domain-id $DOMAIN_ID --space-name $SPACE_NAME | jq -r '.Url')\n",
    "\n",
    "echo \"=====>  Launch Streamlit: $STREAMLIT_URL/proxy/8501/\"\n",
    "\n",
    "streamlit run chat_app.py --server.runOnSave true --server.port 8501 > /dev/null"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdd0397a-5440-44b8-838a-7b2b543d014c",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #6bb07e; border-left: 5px solid #6bb07e; padding: 10px; color: black;\">\n",
    "    - Navigate to: https://example.studio.us-east-1.sagemaker.aws/jupyterlab/default/proxy/8501/\n",
    "</div>\n",
    "\n",
    "<div style=\"background-color: #6bb07e; border-left: 5px solid #6bb07e; padding: 10px; color: black;\">\n",
    "    <i>- Replace \"example\" with your your current url host `https://use_this_host.studio.us-east-1...`</i>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "388a24fb",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #FFDDDD; border-left: 5px solid red; padding: 10px; color: black;\">\n",
    "    Please *interrupt* the above cell to stop Streamlit app\n",
    "</div>\n",
    "\n",
    "Navigate to `Kernel` > `Interrupt Kernel` \n",
    "\n",
    "OR\n",
    "\n",
    "Use the `Stop` Button from the toolbar to interrupt your kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8eb84700-d9b8-4e59-b1ef-7e390ca34596",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "availableInstances": [
   {
    "_defaultOrder": 0,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.t3.medium",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 1,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.t3.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 2,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.t3.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 3,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.t3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 4,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 5,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 6,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 7,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 8,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 9,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 10,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 11,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 12,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5d.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 13,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5d.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 14,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5d.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 15,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5d.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 16,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5d.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 17,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5d.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 18,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5d.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 19,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 20,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": true,
    "memoryGiB": 0,
    "name": "ml.geospatial.interactive",
    "supportedImageNames": [
     "sagemaker-geospatial-v1-0"
    ],
    "vcpuNum": 0
   },
   {
    "_defaultOrder": 21,
    "_isFastLaunch": true,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.c5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 22,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.c5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 23,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.c5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 24,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.c5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 25,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 72,
    "name": "ml.c5.9xlarge",
    "vcpuNum": 36
   },
   {
    "_defaultOrder": 26,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 96,
    "name": "ml.c5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 27,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 144,
    "name": "ml.c5.18xlarge",
    "vcpuNum": 72
   },
   {
    "_defaultOrder": 28,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.c5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 29,
    "_isFastLaunch": true,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g4dn.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 30,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g4dn.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 31,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g4dn.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 32,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g4dn.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 33,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g4dn.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 34,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g4dn.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 35,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 61,
    "name": "ml.p3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 36,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 244,
    "name": "ml.p3.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 37,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 488,
    "name": "ml.p3.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 38,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.p3dn.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 39,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.r5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 40,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.r5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 41,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.r5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 42,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.r5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 43,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.r5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 44,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.r5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 45,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.r5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 46,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.r5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 47,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 48,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 49,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 50,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 51,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 52,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 53,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.g5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 54,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.g5.48xlarge",
    "vcpuNum": 192
   },
   {
    "_defaultOrder": 55,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 56,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4de.24xlarge",
    "vcpuNum": 96
   }
  ],
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
