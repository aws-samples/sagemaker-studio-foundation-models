{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "63303307-35a6-4252-bcad-0fdb4d68773e",
   "metadata": {
    "tags": []
   },
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "⚠️ We will use ml.g5.2xlarge to run this notebook\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f4d4a742-3d41-4e80-a99f-c6cc345ea93d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.1.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.2.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!python3 -m pip install -q accelerate==0.20.3 transformers==4.33.0 gradio bitsandbytes accelerate google-search-results sentencepiece"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9b2ce1e5-525a-4a57-8eb5-20d4e3061278",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import gc\n",
    "from threading import Thread\n",
    "import torch\n",
    "from transformers import (\n",
    "    AutoTokenizer, \n",
    "    AutoModelForCausalLM,\n",
    "    BitsAndBytesConfig\n",
    ")\n",
    "from transformers import GenerationConfig, TextIteratorStreamer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "72979938-47b6-4df2-88e3-9372da4ae604",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "MODEL_ID = \"NousResearch/Nous-Hermes-Llama2-13b\"\n",
    "# MODEL_ID = \"tiiuae/falcon-40b-instruct\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d21e638c-c51e-43a7-85a7-13ff97cc715a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The model weights are not tied. Please use the `tie_weights` method before using the `infer_auto_device` function.\n",
      "Loading checkpoint shards: 100%|██████████| 3/3 [01:27<00:00, 29.12s/it]\n",
      "/opt/conda/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:362: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:367: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# quantization config using BnB\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16\n",
    ")\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_ID, \n",
    "    # load_in_8bit=True, \n",
    "    quantization_config=bnb_config,\n",
    "    # trust_remote_code=True,\n",
    "    device_map=\"auto\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fd738b94-91ff-4ed1-9c3d-bd3e8c6efc0d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    MODEL_ID\n",
    ")\n",
    "tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b76ca9d4-f659-4ae2-80ab-d44add5afe65",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def generate_dolly_like_prompt(user_question, user_context):\n",
    "    \"\"\"\n",
    "    Generates a dolly Like prompt for model to respond with context \n",
    "    \"\"\"\n",
    "    prefix = \"You are an assistant for question-answering tasks. You are helpful, friendly and only answer the question you are asked.\"\n",
    "    instruction = f\"### Instruction:\\n{prefix}\\n\\n{user_question}\"\n",
    "    context = f\"### Input:\\n{user_context}\" if user_context else None\n",
    "    response = f\"### Response:\\n\"\n",
    "\n",
    "    prompt = \"\\n\\n\".join([i for i in [instruction, context, response] if i is not None])\n",
    "\n",
    "    return prompt\n",
    "\n",
    "def tokenize(tokenizer, prompt):\n",
    "    \"\"\" \n",
    "    Tokenize your input prompt to provide as an input \n",
    "    to the model\n",
    "    \"\"\"\n",
    "    tokenized = tokenizer(prompt, return_tensors=\"pt\")\n",
    "    input_ids = tokenized.input_ids\n",
    "    input_ids = input_ids.to(model.device)\n",
    "    \n",
    "    return tokenized, input_ids"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ec2ff70-20a0-40ed-835c-9f01e6f255d8",
   "metadata": {},
   "source": [
    "### Convert Input into a Prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "da922091-cd58-4a88-a16b-5f6462e98175",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "user_question = \"How can learn to drive a car?\"\n",
    "user_context = None\n",
    "\n",
    "prompt = generate_dolly_like_prompt(\n",
    "    user_question=user_question, \n",
    "    user_context=user_context\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "31ae39e8-23dc-4d06-9bc1-25c41009c7f8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### Instruction:\n",
      "You are an assistant for question-answering tasks. You are helpful, friendly and only answer the question you are asked.\n",
      "\n",
      "How can learn to drive a car?\n",
      "\n",
      "### Response:\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13ec47f2-70b3-4575-bbaf-22fb54981779",
   "metadata": {},
   "source": [
    "### Tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f544b5f3-36e1-4e69-b824-4043abb8c5a9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "_, input_ids = tokenize(\n",
    "    tokenizer=tokenizer, \n",
    "    prompt=prompt\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1f9acc57-856d-4754-aa58-cc22ce1fe0ba",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[    1,   835,  2799,  4080, 29901,    13,  3492,   526,   385, 20255,\n",
      "           363,  1139, 29899, 12011,   292,  9595, 29889,   887,   526,  8444,\n",
      "         29892, 19780,   322,   871,  1234,   278,  1139,   366,   526,  4433,\n",
      "         29889,    13,    13,  5328,   508,  5110,   304,  7899,   263,  1559,\n",
      "         29973,    13,    13,  2277, 29937, 13291, 29901,    13]],\n",
      "       device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "print(input_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c88f7174-cb2e-4a52-9b29-cbefa35865ab",
   "metadata": {},
   "source": [
    "## Generate Response as a Blocking Process"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca02059f-3ba9-4df9-ba30-2d4e4396c933",
   "metadata": {},
   "source": [
    "When you prompt a model to generate a response, it takes times, sometimes several minutes based on the size of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "dc7c5973-ba71-48d5-b58d-d39c823d00d0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/transformers/generation/utils.py:1417: UserWarning: You have modified the pretrained model configuration to control generation. This is a deprecated strategy to control generation and will be removed soon, in a future version. Please use a generation configuration file (see https://huggingface.co/docs/transformers/main_classes/text_generation )\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "with torch.inference_mode():\n",
    "    outputs = model.generate(\n",
    "        input_ids=input_ids,\n",
    "        temperature=0.01,\n",
    "        top_p=0.95,\n",
    "        max_new_tokens=512,\n",
    "        do_sample=True,\n",
    "        use_cache=False,\n",
    "        num_return_sequences=1 # generate multiple responses from the model with values > 1\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c1ce9f0a-492e-4a0c-8721-b05e9dd4216a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#################### Response: 1 ####################\n",
      "\n",
      "### Instruction:\n",
      "You are an assistant for question-answering tasks. You are helpful, friendly and only answer the question you are asked.\n",
      "\n",
      "How can learn to drive a car?\n",
      "\n",
      "### Response:\n",
      " To learn to drive a car, you can follow these steps:\n",
      "\n",
      "1. Research and understand the laws and requirements for obtaining a driver's license in your area.\n",
      "2. Find a reputable driving school or instructor who can teach you the basics of driving and help you develop good driving habits.\n",
      "3. Practice driving in a safe and controlled environment, such as an empty parking lot or a driving range.\n",
      "4. Gradually move on to driving on quiet streets and in low-traffic areas to gain confidence and experience.\n",
      "5. Take a driver's education course if one is available in your area.\n",
      "6. Once you feel comfortable, take a driving test to obtain your driver's license.\n",
      "\n",
      "Remember to always use caution and be attentive while driving, and to always follow traffic laws and safety guidelines. Good luck!\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "responses =[]\n",
    "for i, _output in enumerate(outputs): \n",
    "    response = tokenizer.decode(\n",
    "        _output, \n",
    "        skip_special_tokens=True\n",
    "    )\n",
    "    print(f\"#################### Response: {i+1} ####################\\n\")\n",
    "    print(response)\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cf7a45d-a235-4360-9c30-524b03865bdb",
   "metadata": {},
   "source": [
    "How can we improve the process without having the users wait several seconds to minutes for a response? Maybe we can stream response from the model "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f976045-5681-44a5-a280-316928675ed7",
   "metadata": {},
   "source": [
    "## Stream you outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "72b1ef28-6fca-4eb5-bdbc-0ceaf91b24be",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# create a model generator config\n",
    "generation_config = GenerationConfig(\n",
    "    temperature=0.5,\n",
    "    top_p=0.95,\n",
    "    max_new_tokens=512,\n",
    "    do_sample=True,\n",
    "    use_cache=False,\n",
    "    num_return_sequences=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e9b25406-6c8b-401d-82f9-aa6c4f87d189",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# streaming handler to handle tokens output from the model\n",
    "streamer = TextIteratorStreamer(\n",
    "    tokenizer, \n",
    "    skip_prompt=False, \n",
    "    skip_special_tokens=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b95b59a9-4879-40db-ac87-dc5ce5070f10",
   "metadata": {},
   "source": [
    "### Generate a new prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5df4a3ef-d65d-4da5-91e0-d814abaf6b24",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "user_question = \"How can I learn to cook?\"\n",
    "\n",
    "prompt = generate_dolly_like_prompt(\n",
    "    user_question=user_question, \n",
    "    user_context=None\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c996bb72-5526-4b7d-b20f-3d3424edf471",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### Instruction:\n",
      "You are an assistant for question-answering tasks. You are helpful, friendly and only answer the question you are asked.\n",
      "\n",
      "How can I learn to cook?\n",
      "\n",
      "### Response:\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2dcba101-03b8-4cd2-a1b6-974f77f9df33",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "_tokenized, _input_ids = tokenize(\n",
    "    tokenizer=tokenizer, \n",
    "    prompt=prompt\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "90e4d6fb-34ac-40fb-bead-c83db4bb8669",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "generate_kwargs = dict(\n",
    "    input_ids=_input_ids,\n",
    "    generation_config=generation_config,\n",
    "    return_dict_in_generate=True,\n",
    "    eos_token_id=tokenizer.eos_token_id,\n",
    "    pad_token_id=tokenizer.eos_token_id,\n",
    "    bos_token_id=tokenizer.bos_token_id,\n",
    "    attention_mask=_tokenized.attention_mask,\n",
    "    output_scores=True,\n",
    "    streamer=streamer,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25ec49d8-14a4-4d20-8e61-0ac3d40a2eeb",
   "metadata": {},
   "source": [
    "### Stream you Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a1486be9-3924-4401-abcf-574f659fbd12",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### Instruction:\n",
      "You are an assistant for question-answering tasks. You are helpful, friendly and only answer the question you are asked.\n",
      "\n",
      "How can I learn to cook?\n",
      "\n",
      "### Response:\n",
      "To learn to cook, start by mastering basic techniques and recipes. Practice regularly and experiment with different ingredients and flavors. Consider taking cooking classes or watching online tutorials to learn new skills and get inspiration."
     ]
    }
   ],
   "source": [
    "thread = Thread(\n",
    "    target=model.generate, \n",
    "    kwargs=generate_kwargs\n",
    ")\n",
    "\n",
    "thread.start()\n",
    "for new_text in streamer:\n",
    "    print(new_text, end=\"\")\n",
    "\n",
    "thread.join()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "94dfc604-5ff4-4970-bea5-dc6fa6092018",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# https://modal.com/docs/guide/ex/falcon_bitsandbytes\n",
    "class StreamingAgent:\n",
    "    def __init__(\n",
    "        self, \n",
    "        model, \n",
    "        tokenizer, \n",
    "        model_name=None\n",
    "    ):\n",
    "        \n",
    "        if model_name is not None: \n",
    "            print(f\"Downloading and Loading {model_name}!\")\n",
    "            self.model_name = model_name\n",
    "            self.quantization_config = BitsAndBytesConfig(\n",
    "                load_in_4bit=True,\n",
    "                bnb_4bit_use_double_quant=True,\n",
    "                bnb_4bit_quant_type=\"nf4\",\n",
    "                bnb_4bit_compute_dtype=torch.bfloat16\n",
    "            )\n",
    "\n",
    "            # load model into memory locally\n",
    "            self._model = AutoModelForCausalLM.from_pretrained(\n",
    "                self.model_name, \n",
    "                quantization_config=self.quantization_config,\n",
    "                device_map=\"auto\"\n",
    "            )\n",
    "            self._model.eval()\n",
    "            self.local_model = torch.compile(self._model)\n",
    "            \n",
    "            # load tokenizer into memroy\n",
    "            self.tokenizer = AutoTokenizer.from_pretrained(self.model_name)\n",
    "            self.tokenizer.pad_token = self.tokenizer.eos_token\n",
    "            \n",
    "        else:\n",
    "            print(\"Loading local model provided by a user ...\")\n",
    "            self.local_model = torch.compile(model)\n",
    "            # user provided tokenizer\n",
    "            self.tokenizer = tokenizer\n",
    "            \n",
    "    \n",
    "    def prompt_template(self, user_question):\n",
    "        \n",
    "        user_context = \"Answer the question truthfully, honestly and to the point. Also, try to be funny when you answer the question.\"\n",
    "\n",
    "        instruction = f\"### Instruction\\n{user_question}\"\n",
    "        context = f\"### Context\\n{user_context}\" if user_context else None\n",
    "        response = f\"### Answer\\n\"\n",
    "\n",
    "        prompt = \"\\n\\n\".join([i for i in [instruction, context, response] if i is not None])\n",
    "        \n",
    "        return prompt\n",
    "\n",
    "    def generate(self, user_query: str):\n",
    "        \n",
    "        # keep track of when prompt template is going to served right back \n",
    "        self._counter = 0\n",
    "        \n",
    "        prompt = self.prompt_template(user_query)\n",
    "\n",
    "        tokenized = self.tokenizer(prompt, return_tensors=\"pt\")\n",
    "        input_ids = tokenized.input_ids\n",
    "        input_ids = input_ids.to(self.local_model.device)\n",
    "\n",
    "        generation_config = GenerationConfig(\n",
    "            temperature=1.5,\n",
    "            top_k=120,\n",
    "            top_p=0.9,\n",
    "            max_new_tokens=512,\n",
    "            do_sample=True,\n",
    "            num_return_sequences=1\n",
    "        )\n",
    "\n",
    "        streamer = TextIteratorStreamer(\n",
    "            self.tokenizer, \n",
    "            skip_special_tokens=True\n",
    "        )\n",
    "        \n",
    "        generate_kwargs = dict(\n",
    "            input_ids=input_ids,\n",
    "            generation_config=generation_config,\n",
    "            return_dict_in_generate=True,\n",
    "            eos_token_id=self.tokenizer.eos_token_id,\n",
    "            pad_token_id=self.tokenizer.eos_token_id,\n",
    "            bos_token_id=self.tokenizer.bos_token_id,\n",
    "            attention_mask=tokenized.attention_mask,\n",
    "            output_scores=True,\n",
    "            streamer=streamer,\n",
    "        )\n",
    "\n",
    "        thread = Thread(\n",
    "            target=self.local_model.generate, \n",
    "            kwargs=generate_kwargs\n",
    "        )\n",
    "        \n",
    "        thread.start()\n",
    "        for new_text in streamer:\n",
    "            \n",
    "            if self._counter > 0:\n",
    "                yield new_text\n",
    "                self._counter += 1\n",
    "            else:\n",
    "                self._counter += 1\n",
    "\n",
    "        thread.join()\n",
    "        self._counter = 0\n",
    "    \n",
    "    def delete_model(self):\n",
    "        del self.local_model\n",
    "        del self._model\n",
    "        del self.tokenizer\n",
    "        gc.collect()\n",
    "        torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "973b0be1-ceee-4216-a1a7-0cab6a7d5b84",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading local model provided by a user ...\n"
     ]
    }
   ],
   "source": [
    "stream_agent = StreamingAgent(\n",
    "    model=model, \n",
    "    tokenizer=tokenizer,\n",
    "    model_name=None\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3c58f2b-50f7-4ca7-9d7e-d10f7322ad34",
   "metadata": {},
   "source": [
    "## Build a local UI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d8eced85-979c-4ca1-bfa2-fe84eab89a9d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on local URL:  http://127.0.0.1:7860\n",
      "Running on public URL: https://b9cfe9d1391a7cc533.gradio.live\n",
      "\n",
      "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from Terminal to deploy to Spaces (https://huggingface.co/spaces)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"https://b9cfe9d1391a7cc533.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# https://github.com/gradio-app/gradio/blob/main/demo/chatbot_simple/run.py\n",
    "import gradio as gr\n",
    "import random\n",
    "import time\n",
    "\n",
    "with gr.Blocks() as demo:\n",
    "    chatbot = gr.Chatbot()\n",
    "    msg = gr.Textbox()\n",
    "    clear = gr.Button(\"Clear\")\n",
    "\n",
    "    def user(user_message, history):\n",
    "        return \"\", history + [[user_message, None]]\n",
    "\n",
    "    def bot(history):\n",
    "        last_user_message = history[-1][0]\n",
    "        print(\"Asking the model: \", last_user_message)\n",
    "        bot_message = stream_agent.generate(last_user_message)\n",
    "        history[-1][1] = \"\"\n",
    "        \n",
    "        for pred_words in stream_agent.generate(last_user_message):\n",
    "            if pred_words:\n",
    "                history[-1][1] += pred_words\n",
    "                yield history\n",
    "\n",
    "    msg.submit(\n",
    "        user, [msg, chatbot], [msg, chatbot], \n",
    "        queue=False\n",
    "    ).then(\n",
    "        bot, chatbot, chatbot\n",
    "    )\n",
    "    clear.click(lambda: None, None, chatbot, queue=False)\n",
    "    \n",
    "demo.queue().launch(\n",
    "    share=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed0e87c0-ae51-4684-b93f-a899976997d4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "availableInstances": [
   {
    "_defaultOrder": 0,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.t3.medium",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 1,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.t3.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 2,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.t3.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 3,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.t3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 4,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 5,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 6,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 7,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 8,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 9,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 10,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 11,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 12,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5d.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 13,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5d.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 14,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5d.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 15,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5d.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 16,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5d.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 17,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5d.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 18,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5d.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 19,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 20,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": true,
    "memoryGiB": 0,
    "name": "ml.geospatial.interactive",
    "supportedImageNames": [
     "sagemaker-geospatial-v1-0"
    ],
    "vcpuNum": 0
   },
   {
    "_defaultOrder": 21,
    "_isFastLaunch": true,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.c5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 22,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.c5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 23,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.c5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 24,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.c5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 25,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 72,
    "name": "ml.c5.9xlarge",
    "vcpuNum": 36
   },
   {
    "_defaultOrder": 26,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 96,
    "name": "ml.c5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 27,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 144,
    "name": "ml.c5.18xlarge",
    "vcpuNum": 72
   },
   {
    "_defaultOrder": 28,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.c5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 29,
    "_isFastLaunch": true,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g4dn.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 30,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g4dn.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 31,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g4dn.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 32,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g4dn.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 33,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g4dn.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 34,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g4dn.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 35,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 61,
    "name": "ml.p3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 36,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 244,
    "name": "ml.p3.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 37,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 488,
    "name": "ml.p3.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 38,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.p3dn.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 39,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.r5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 40,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.r5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 41,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.r5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 42,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.r5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 43,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.r5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 44,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.r5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 45,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.r5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 46,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.r5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 47,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 48,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 49,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 50,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 51,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 52,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 53,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.g5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 54,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.g5.48xlarge",
    "vcpuNum": 192
   },
   {
    "_defaultOrder": 55,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 56,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4de.24xlarge",
    "vcpuNum": 96
   }
  ],
  "instance_type": "ml.g5.12xlarge",
  "kernelspec": {
   "display_name": "Python 3 (PyTorch 2.0.0 Python 3.10 GPU Optimized)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-east-1:081325390199:image/pytorch-2.0.0-gpu-py310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
