{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "22d9b500-44e6-41ee-970a-0d16a08e4240",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb169844-679d-4277-a14e-0ce7f85cb4eb",
   "metadata": {},
   "source": [
    "In this notebook, we demonstrate how to **improve responses of a large language model (LLM)** on a **task** based on specific **criteria**, using Direct Preference Optimization (DPO), SageMaker Studio, and SageMaker GroundTruth.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f79e48ab-3dc3-4f1b-a112-100ce7d101da",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "<b>Important:</b> You need a ml.g5.48xlarge instance to run this notebook in SageMaker Studio. If you are on the latest version of Studio, your Jupyter Lab space should have 100GB of EBS storage. \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9900fb0f-8d02-4a7f-80a7-6f8501bb8c7c",
   "metadata": {},
   "source": [
    "\n",
    "***\n",
    "**_Scenario_**:\n",
    "You are a data scientist working for the fictional digital bank \"Example Bank\". You are develping a customer facing chatbot powered by the **meta-llama/Meta-Llama-3-8B-Instruct** that answers common customer questions about Example Bank. As part of your work, you want to ensure that if that the model receives non-legitimate questions (toxic, off-topic etc.) it responds in an acceptable way that is aligned to Example Bank's brand and core values. \n",
    "\n",
    "You will use DPO to align the model's responses to non-legitimate questions to the organisation's brand.  This notebook walks you through the steps to achieve that using SageMaker Studio and SageMaker GroundTruth:\n",
    "1. **Load the meta-llama/Meta-Llama-3-8B-Instruct model** on the SageMaker Studio notebook.\n",
    "2.  **Collect initial model responses** for common and toxic questions.\n",
    "3.  **Set-up the workflow for gathering human preference data** using SageMaker GroundTruth\n",
    "4. **Gather human preference data** by presenting the model's responses to human raters and asking them to rank these responses based on how aligned they are to the organisation's brand.\n",
    "5. **Process the collected human feedback** so that it can be used to improve the model performance.\n",
    "6. **Train the language model using DPO**: Use the preprocessed feedback data to fine-tune or retrain the large language model using the Hugging Face **[trl](#https://huggingface.co/docs/trl/en/index)** library and **[DPO Trainer](#https://huggingface.co/docs/trl/en/dpo_trainer)**\n",
    "7. **Evaluate the fine-tuned model**: Test the fine-tuned model on a held-out evaluation dataset to assess its performance and ensure that it has improved in terms of helpfulness and other desired characteristics.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "You can apply the same approach for different LLMs, tasks, and criteria (e.g. helpfulness, accuracy etc.)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67ea2664-bd11-4c5b-87cf-34ff9e50b3e5",
   "metadata": {},
   "source": [
    "# 00. Set-up"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cdf3f66-aecb-4a80-9365-e4939b489710",
   "metadata": {},
   "source": [
    "First, let's set-up our environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9713230f-6b59-452a-b683-e39e63091c5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile requirements.txt\n",
    "sagemaker>=2.175.0\n",
    "transformers==4.39.3\n",
    "accelerate==0.28.0\n",
    "datasets==2.13.0\n",
    "langchain==0.0.305\n",
    "sentence_transformers\n",
    "bitsandbytes==0.43.0\n",
    "torch==2.2.1\n",
    "aiofiles\n",
    "peft==0.8.2\n",
    "trl==0.9.4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1911952-d0cd-4d58-9750-ebf184fb0ba3",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -U -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad2ab5c4-391a-423f-9fc4-59702d31bc7f",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "<b>Important:</b> Once the previous step is complete, make sure you restart the kernel before proceeding.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9c1cf13-c5e1-4160-acb7-f28555bed80d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import os\n",
    "import sagemaker\n",
    "import boto3\n",
    "import datetime\n",
    "from transformers import pipeline\n",
    "import json\n",
    "import asyncio\n",
    "import aiofiles\n",
    "from datasets import Dataset, load_dataset\n",
    "\n",
    "from peft import (\n",
    "    get_peft_model,\n",
    "    LoraConfig,\n",
    "    prepare_model_for_kbit_training,\n",
    ")\n",
    "import bitsandbytes as bnb\n",
    "from tqdm import tqdm\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    BitsAndBytesConfig,\n",
    "    TrainingArguments,\n",
    "    AutoModelForSequenceClassification\n",
    ")\n",
    "from IPython.core.display import display, HTML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5246a3e4-06c5-4099-bb38-57ef5d0c5689",
   "metadata": {},
   "outputs": [],
   "source": [
    "sess = sagemaker.Session()\n",
    "role = sagemaker.get_execution_role()\n",
    "\n",
    "s3 = boto3.client('s3')\n",
    "sm_client = boto3.client('sagemaker')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "912d7ef1-db51-44d8-b2f5-eb9b5e2391a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import getpass\n",
    "hf_access_token = getpass.getpass(\"Huggingface API Token:\")\n",
    "os.environ['HF_TOKEN'] = hf_access_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "818ac09b-dc8a-4095-a2dd-a0053c316b45",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model_id = \"meta-llama/Meta-Llama-3-8B-Instruct\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c61b064f-b192-4d86-a4d4-b97f14a89828",
   "metadata": {},
   "outputs": [],
   "source": [
    "cache_dir = \"/mnt/sagemaker-nvme\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0c5a636-c6fb-4dc0-8681-ce58390aae0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_files_path = \"/home/sagemaker-user/sample-files\"\n",
    "os.makedirs(sample_files_path, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fd89cfc-0718-436c-be63-00e2860bbe5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# optionally set global temperature and top_p\n",
    "gl_temperature = 1.0\n",
    "gl_top_p = 0.95"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b27c0782-771c-466e-877e-374ef6db296e",
   "metadata": {},
   "source": [
    "Now we are ready to get started with our development"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d914efd8-1b76-4a13-9caa-c22a40cf2138",
   "metadata": {},
   "source": [
    "# 01. Load `meta-llama/Meta-Llama-3-8B-Instruct` in the notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52a24cd7-c66e-46d1-a9df-84a0fb918ea9",
   "metadata": {},
   "source": [
    "### Load the model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5442135-3dd2-44f2-a9df-7d405b2f69d0",
   "metadata": {},
   "source": [
    "We download the `meta-llama/Meta-Llama-3-8B-Instruct` model from the HuggingFace Hub and load the model into memory in bf16 format. Optionally you can use BitsAndyBytes to quantize the model to 4 bits (or half a byte per parameter) to reduce its RAM requirements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8035959-ce2a-46c9-8b17-39c0eabc564c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    base_model_id,\n",
    "    token=hf_access_token,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    device_map=\"auto\",\n",
    "    cache_dir=cache_dir\n",
    ")\n",
    "\n",
    "model.config.use_cache = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ca02b24-c28b-42e7-a0f2-dda20c6ed929",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    base_model_id, \n",
    "    token=hf_access_token, \n",
    "    cache_dir=cache_dir\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a959cec4-63a8-4cec-9c73-370f64aeca90",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# 02. Collect initial model responses for common and toxic questions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da97c98d-8398-45d6-aaea-498e271d9fdd",
   "metadata": {},
   "source": [
    "Now that we have the LLM loaded in memory, we can use it to collect model responses for our scenario.\n",
    "First, let's define the brand and core values of Example Bank and send a sample prompt to the model to ensure it is working as expected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70c2c035-79cb-4afb-9b2e-492356115670",
   "metadata": {},
   "outputs": [],
   "source": [
    "company_name = \"Example Bank\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2351cbff-2e18-422e-b962-1de4c82520b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "company_context = \"\"\"Example Bank is a next-generation digital bank on a mission to revolutionize the banking experience. Founded in 2020, we are committed to leveraging cutting-edge technology to make banking simple, accessible, and transparent for everyone. In Example Bank, we believe that banking should be seamless, intuitive, and tailored to the needs of modern consumers. Our founders, seasoned professionals from the tech and finance industries, set out to create a bank that puts people first, empowering them to take control of their finances with ease. At Example Bank, we envision a world where banking is no longer a chore but a delightful experience. We are dedicated to breaking down barriers and democratizing access to financial services. Our goal is to empower individuals and businesses alike by providing them with the tools and resources they need to thrive in an increasingly digital landscape.\n",
    "\n",
    "Our values:\n",
    "- Innovation: We embrace cutting-edge technologies and continuously seek out innovative solutions to deliver the best possible banking experience. We are a digital-only bank, which means we don't have any physical branches. Instead, we offer all of our services online or through our mobile app. This allows us to keep our costs low and pass the savings on to our customers.\n",
    "- Transparency: We are committed to being direct and honest with our customers. We believe that transparency is key to building trust, and we want our customers to feel confident that they are making informed decisions about their money. That's why we provide clear and concise information about our products and services, and we are always available to answer any questions our customers may have.\n",
    "- Accessibility: Our services are designed to be inclusive and user-friendly, catering to a diverse range of customers, regardless of their financial backgrounds.\n",
    "- Security: We prioritize the safety and security of our customers' data and assets, employing state-of-the-art encryption and cybersecurity measures.\n",
    "\n",
    "In addition to our core values, Example Bank offers a range of innovative financial products and services:\n",
    "\n",
    "- Loans: Whether you’re looking to buy a home, start a business, or finance a major purchase, our flexible loan options are designed to meet your needs. With competitive interest rates and a simple application process, obtaining a loan has never been easier.\n",
    "- Credit Cards: Our credit cards come with a host of benefits including cashback rewards, low-interest rates, and no annual fees. Manage your spending effortlessly with real-time notifications and intuitive budgeting tools.\n",
    "- Mobile Apps: Our user-friendly apps on the Google Play Store and Apple App Store offer a seamless banking experience. From checking balances to transferring funds, our apps ensure you have complete control of your finances at your fingertips.\n",
    "- Savings and Investments: Grow your wealth with our high-yield savings accounts and a variety of investment options. Our financial advisors are available to help you make informed decisions tailored to your financial goals.\n",
    "- Customer Support: We provide 24/7 customer support to assist with any inquiries or issues. Our dedicated team is always ready to help, ensuring you receive the best possible service at all times.\n",
    "\n",
    "At Example Bank, we are committed to enhancing your financial well-being through innovation, transparency, and unparalleled service. Join us today and experience the future of banking.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d92621c5-be8b-41b3-aa93-7440220f6c3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "content = \"\"\"\n",
    "{}\\nYou are a customer service agent at Example bank. You are polite and helpful. You should answer a question only based on the context above,. Answer the following customer question in one or two sentences: How do I rob a bank?\n",
    "\"\"\".format(company_context)\n",
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": content}\n",
    "]\n",
    "\n",
    "input_ids = tokenizer.apply_chat_template(\n",
    "    messages, \n",
    "    return_tensors=\"pt\"\n",
    ").to(\"cuda\")\n",
    "\n",
    "terminators = [\n",
    "    tokenizer.eos_token_id,\n",
    "    tokenizer.convert_tokens_to_ids(\"<|eot_id|>\")\n",
    "]\n",
    "\n",
    "outputs = model.generate(\n",
    "    input_ids, \n",
    "    max_new_tokens=120, \n",
    "    temperature=gl_temperature,\n",
    "    top_p=gl_top_p,\n",
    "    eos_token_id=terminators\n",
    "\n",
    ")\n",
    "print(tokenizer.decode(outputs[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ed46b6f-3219-408e-9929-19da9c90eed2",
   "metadata": {},
   "source": [
    "We have a list of common questions received by call centers in financial organisations together with a list of toxic and off-topic questions. We invoke the model 4 times per question and save the model prompts and outputs in a json file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45349bb5-04a1-4c51-aed5-0f612a197b42",
   "metadata": {},
   "outputs": [],
   "source": [
    "questions = 'example_bank_questions.txt'\n",
    "llm_responses = os.path.join(sample_files_path, 'llm_responses.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b283c8c9-c852-438c-8fdc-1af29bf70e7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from timeit import default_timer as timer\n",
    "import tqdm.asyncio\n",
    "\n",
    "\n",
    "async def invoke_model(question, context):\n",
    "    pipe = pipeline(\"text-generation\", model=model, tokenizer=tokenizer)\n",
    "    messages = [\n",
    "        {\"role\": \"user\", \"content\": f\"{context}: {question}\"}\n",
    "    ]\n",
    "\n",
    "    terminators = [\n",
    "        tokenizer.eos_token_id,\n",
    "        tokenizer.convert_tokens_to_ids(\"<|eot_id|>\")\n",
    "    ]\n",
    "\n",
    "    response = pipe(\n",
    "        messages, \n",
    "        max_new_tokens=120, \n",
    "        do_sample=True,\n",
    "        temperature=gl_temperature, \n",
    "        top_p=gl_top_p, \n",
    "        eos_token_id=terminators\n",
    "    )[0]['generated_text'][-1]\n",
    "    return response['content']\n",
    "\n",
    "async def process_lines(file_path):\n",
    "    results = []\n",
    "    context = f\"\"\"{company_context} You are a customer service agent for {company_name} Sometimes you are smart with your answers. Answer the following customer question in one or two sentences:\n",
    "    \"\"\"\n",
    "    async with aiofiles.open(file_path, 'r') as file:\n",
    "        lines = [line async for line in file]\n",
    "        # Initialize tqdm with the number of lines to process\n",
    "        for line in tqdm.asyncio.tqdm(lines, desc=\"Processing Question Bank\"):\n",
    "            start = timer()\n",
    "            # print(f\"processing prompt: {line}\")\n",
    "            responses = await asyncio.gather(*[invoke_model(line, context) for _ in range(4)])\n",
    "            result = {\n",
    "                'context': context,\n",
    "                'question': line.strip(),\n",
    "                'responses': responses\n",
    "            }\n",
    "            end = timer()\n",
    "            # print(f\"Total time taken: {end - start} seconds\\n-----\")\n",
    "            results.append(result)\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4d63c08-59aa-4da7-ad4f-c7ee670626d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = await process_lines(questions)\n",
    "\n",
    "with open(llm_responses, 'w') as file:\n",
    "    json.dump(\n",
    "        results, \n",
    "        file, \n",
    "        indent=4\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08f53543-0017-495d-a22f-50813e30db22",
   "metadata": {
    "tags": []
   },
   "source": [
    "# 03. Set-up the SageMaker GroundTruth labeling job for gathering human preference data "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7efa75e2-b09f-4e78-a124-4c167566deec",
   "metadata": {},
   "source": [
    "Next, we want to invite our team of rankers to review the LLM responses and provide feedback on which responses align better with our organisation's brand. To do that, we will follow the steps to set up a [SageMaker GroundTruth](#https://docs.aws.amazon.com/sagemaker/latest/dg/data-label.html) labeling job."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3f0bd56-3795-4c75-81ba-627d41b9c2f3",
   "metadata": {},
   "source": [
    "### Step 1: IAM role and Amazon S3 bucket set-up "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e283be1-5ec8-4f5b-980c-f9084ded047e",
   "metadata": {},
   "source": [
    "To use SageMaker GroundTruth, the IAM execution role you are using to run this notebook **must have the AWS managed policy [AmazonSageMakerGroundTruthExecution](https://console.aws.amazon.com/iam/home#policies/arn:aws:iam::aws:policy/AmazonSageMakerGroundTruthExecution) attached**. \\\n",
    "Run the following code-block to see your IAM execution role name. You can add the policy to the IAM role using the AWS console. For more details, see [Adding and removing IAM identity permissions](https://docs.aws.amazon.com/IAM/latest/UserGuide/access_policies_manage-attach-detach.html#add-policies-console) in the AWS documentation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab724982",
   "metadata": {},
   "outputs": [],
   "source": [
    "role_name = role.split(\"/\")[-1]\n",
    "print(\"********************************************************************************\")\n",
    "print(\"The IAM execution role name:\", role_name)\n",
    "print(\"The IAM execution role ARN:\", role)\n",
    "print(\"********************************************************************************\")\n",
    "print(\n",
    "    \"IMPORTANT: Make sure this execution role has the AWS Managed policy AmazonGroundTruthExecution attached.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c7b0512-a7ce-4f74-9b78-79d91a733534",
   "metadata": {},
   "source": [
    "***\n",
    "In this notebook we use the default S3 bucket to save all the inputs and outputs of the SageMaker GroundTruth job. If you choose to use a different S3 bucket, make sure that the IAM role has the right permissions to access it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71a0695d-5b69-42db-abc5-52bb49d27cd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "bucket = sess.default_bucket()\n",
    "print(bucket)\n",
    "prefix=\"studio-rlhf\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d50616e-9bc1-4c70-9670-310ae07bf726",
   "metadata": {},
   "source": [
    "### Step 2: Specify the work team"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cd893ac",
   "metadata": {},
   "source": [
    "The work team consists of the group of people who provide the human feedback. If you don't have an existing work team, you can create one using the AWS console, outside of this notebook. For details and exact steps see [Create an Amazon Cognito Workforce Using the Labeling Workforces Page](#https://docs.aws.amazon.com/sagemaker/latest/dg/sms-workforce-create-private-console.html#create-workforce-sm-console). When you are ready to proceed, enter the **ARN of the work team** in the following cell."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0cf5cd8-043e-4675-944c-e216784ff326",
   "metadata": {},
   "source": [
    "\n",
    "Note:  If you want to preview the worker UI and execute the labeling task youself, you will need to create a private work team, add yourself as a worker to this team, and provide the work team ARN below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f61b02a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "WORKTEAM_ARN = \"arn:aws:sagemaker:us-east-1:477886989750:workteam/private-crowd/uk-based-team\"\n",
    "\n",
    "print(f\"This notebook will use the work team ARN: {WORKTEAM_ARN}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d14a918a",
   "metadata": {},
   "source": [
    "### Step 3: Create the instruction template"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53c45b88-e34f-4343-bb1f-f9550e3c22fd",
   "metadata": {},
   "source": [
    "The instruction template dictates what is be displayed on the UI at the time when the human raters review the model responses and provide feedback. It contains instructions to help them perform their task accurately. \n",
    "\n",
    "The template below advises human raters to review the question and LLM responses and rank the latter based on their alignment to the organisation's brand."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb47c2db-1455-41ae-9fcc-c416f047eadf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display, HTML\n",
    "\n",
    "\n",
    "def make_template(save_fname=\"instructions.template\"):\n",
    "    template = \"\"\"\n",
    "    <html>\n",
    "  <head>\n",
    "    <meta charset=\"UTF-8\" />\n",
    "    <link rel=\"stylesheet\" href=\"https://assets.crowd.aws/css/gen-ai-components.css\" />\n",
    "    <link rel=\"icon\" href=\"data:image/svg+xml,<svg xmlns=%22http://www.w3.org/2000/svg%22 viewBox=%220 0 100 100%22><text y=%22.9em%22 font-size=%2290%22>&#129351;</text></svg>\" />\n",
    "    <title>Text Ranking Tool</title>\n",
    "    <script src=\"https://assets.crowd.aws/gen-ai-components.js\"></script>\n",
    "  </head>\n",
    "\n",
    "  <body>\n",
    "    <div>\n",
    "      <crowd-text-ranking\n",
    "        crowd-form-element-id=\"crowd-form-submit\"\n",
    "        instructions='Rank the following responses from a language model according to their alignment to the organisation's brand.'\n",
    "        ordinal-ranking-dimensions='[{\"name\":\"BrandValue\",\"allowTie\":true}]'\n",
    "        text='{{ task.input.source }}'\n",
    "        responses='{{ task.input.responses | to_json }}' />\n",
    "    </div>\n",
    "    <crowd-form id=\"crowd-form-submit\" style=\"display: none\"></crowd-form>\n",
    "    <script src=\"https://assets.crowd.aws/crowd-html-elements.js\"></script>\n",
    "  </body>\n",
    "</html>\n",
    "\n",
    "    \"\"\"\n",
    "    with open(save_fname, \"w\") as f:\n",
    "        f.write(template)\n",
    "        \n",
    "make_template(save_fname=f\"{sample_files_path}/instructions.template\")\n",
    "result = s3.upload_file(f\"{sample_files_path}/instructions.template\", bucket, \"{}/instructions.template\".format(prefix))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "396919e5-b315-4e9d-a5ee-ad19d9f06e53",
   "metadata": {},
   "source": [
    "Once the template is ready, we upload it to the Amazon S3 bucket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37ad8c23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The path in Amazon S3 to your worker task template or human task UI\n",
    "HUMAN_UI = []\n",
    "\n",
    "UI_TEMPLATE_S3_URI = f\"s3://{bucket}/{prefix}/instructions.template\"\n",
    "HUMAN_UI.append(UI_TEMPLATE_S3_URI)\n",
    "UI_CONFIG_PARAM = \"UiTemplateS3Uri\"\n",
    "\n",
    "print(f\"{UI_CONFIG_PARAM} resource that will be used: {HUMAN_UI[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bedf8904-6a36-4e35-b010-f99682d356d9",
   "metadata": {},
   "source": [
    "### Step 4: Preprocess the input data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ae09410-ffb4-4aa1-8228-d545ce5983dc",
   "metadata": {},
   "source": [
    "Before we create the labeling job, we need to ensure that the input data is in the format expected by GroundTruth. We use the prompts and responses we collected from our model in the **_dataset-for-GT.json_** file to create the manifest file **_inp-manifest-trank.json_**. Each row in the manifest file contains an object(prompt-response pair). We upload the file to Amazon S3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c82a1b0-d3dc-4a1c-b9e1-ee013370f9bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "json_file = open(llm_responses)\n",
    "dataset = json.load(json_file)\n",
    "sources = [{\"source\": item[\"question\"], \"responses\": item[\"responses\"]} for item in dataset]\n",
    "\n",
    "# Open a file for writing\n",
    "with open(f\"{sample_files_path}/inp-manifest-trank.json\", \"w\") as file:\n",
    "    for obj in sources:\n",
    "        # Convert the object to a JSON string and write it to the file\n",
    "        json_str = json.dumps(obj)\n",
    "        file.write(json_str + \"\\n\")\n",
    "model_responses_path = f\"{prefix}/inp-manifest-trank.json\"\n",
    "s3.upload_file(f\"{sample_files_path}/inp-manifest-trank.json\", bucket, model_responses_path)\n",
    "model_responses_s3_uri = 's3://' + bucket + '/' + model_responses_path\n",
    "model_responses_s3_uri"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4983d2bb-e54f-41bb-907a-4f9571a0af9c",
   "metadata": {},
   "source": [
    "### Step 5: Create the labeling job"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97fb84d1-9a7f-4a42-bb04-188d6cfc8d92",
   "metadata": {},
   "source": [
    "Now we are ready to create the labeling job."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d105bcf-2dff-4f66-a2ed-d7d0c015d047",
   "metadata": {},
   "outputs": [],
   "source": [
    "now = datetime.datetime.now()\n",
    "timestamp_str = now.strftime(\"%Y%m%d-%H%M%S\")\n",
    "labeling_job_name = \"passthrough-text-ranking\" + timestamp_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1174604f",
   "metadata": {},
   "outputs": [],
   "source": [
    "sm_client.create_labeling_job(\n",
    "    LabelingJobName=labeling_job_name,\n",
    "    LabelAttributeName='label',\n",
    "    InputConfig={\n",
    "        'DataSource': {\n",
    "            'S3DataSource': {\n",
    "                'ManifestS3Uri': model_responses_s3_uri\n",
    "            }\n",
    "        }\n",
    "    },\n",
    "    OutputConfig={\n",
    "        'S3OutputPath': 's3://{}/{}/output/'.format(bucket,prefix) #Enter S3 URI of Output folder\n",
    "    },\n",
    "    RoleArn=role, \n",
    "    HumanTaskConfig={\n",
    "        'WorkteamArn': WORKTEAM_ARN,\n",
    "        'UiConfig':{\n",
    "            'UiTemplateS3Uri': UI_TEMPLATE_S3_URI\n",
    "        },\n",
    "        'PreHumanTaskLambdaArn': 'arn:aws:lambda:us-east-1:432418664414:function:PRE-PassThrough',\n",
    "        'TaskKeywords': [\n",
    "            'QnA',\n",
    "        ],\n",
    "        'TaskTitle': 'Rank LLM responses',\n",
    "        'TaskDescription': \"Rank the responses provided by the LLM\",\n",
    "        'NumberOfHumanWorkersPerDataObject': 1,\n",
    "        'TaskTimeLimitInSeconds': 60*30,\n",
    "        'TaskAvailabilityLifetimeInSeconds': 60*60*24*10,\n",
    "        'MaxConcurrentTaskCount': 100,\n",
    "        'AnnotationConsolidationConfig': {\n",
    "            'AnnotationConsolidationLambdaArn': 'arn:aws:lambda:us-east-1:432418664414:function:ACS-PassThrough'\n",
    "        } \n",
    "    })"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b995a3a",
   "metadata": {},
   "source": [
    "#### Monitoring Labeling Job Status\n",
    "\n",
    "We track the status of the ongoing labeling job. It is essential to monitor the job's progress and wait for its completion by the annotators. Once the labeling job is finished, we can then proceed to gather feedback from the annotators. This process ensures that we only collect feedback after the entire job is completed, thereby maintaining the accuracy and reliability of the feedback collected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b59715d",
   "metadata": {},
   "outputs": [],
   "source": [
    "sm_client.describe_labeling_job(LabelingJobName=labeling_job_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86b4ae17-e767-48be-890a-99e976648e4e",
   "metadata": {},
   "source": [
    "### Step 6: Gather human feedback through the labeling portal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "341e0984",
   "metadata": {},
   "outputs": [],
   "source": [
    "workforce = sm_client.describe_workforce(WorkforceName=\"default\")\n",
    "worker_portal_url = 'https://' + workforce[\"Workforce\"][\"SubDomain\"]\n",
    "\n",
    "\n",
    "# Display the URL and instructions\n",
    "display(HTML(f\"\"\"\n",
    "<body>\n",
    "<h1>04. Gather human preference data</h1>\n",
    "<p>Please complete the human evaluation tasks available in the labeling portal.</p>\n",
    "<p><a href=\"{worker_portal_url}\">{worker_portal_url}</a>\n",
    "<p><b>Ensure all tasks are completed before proceeding to the next steps in this notebook.<b></p>\n",
    "<body>\n",
    "\"\"\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "594d3cc5-9c5c-4fa0-b6a1-3774823e3785",
   "metadata": {},
   "source": [
    "The Ground Truth output data from your labeling job is saved in the Amazon S3 bucket that you specified during the job creation process. Specifically, the a\\nnotations directory contains the actual annotations provided by the workers during the labeling job.\n",
    "\n",
    "Let's download and save the rankings per item. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49df3652-75a3-4926-9ae3-62b7c4008cd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "labeling_job_name=\"passthrough-text-ranking20240409-211449\"\n",
    "prefix = f\"studio-rlhf/output/{labeling_job_name}/annotations/worker-response/iteration-1/\"\n",
    "output_file = f\"{sample_files_path}/gt-rankings.json\"\n",
    "\n",
    "# List objects within the Iteration directory\n",
    "paginator = s3.get_paginator('list_objects_v2')\n",
    "page_iterator = paginator.paginate(Bucket=bucket, Prefix=prefix)\n",
    "data = []\n",
    "# Iterate through each object within each page\n",
    "for page in page_iterator:\n",
    "    if \"Contents\" in page:\n",
    "        for obj in page['Contents']:\n",
    "            key = obj['Key']\n",
    "            # Skip the directory itself, if listed\n",
    "            if not key.endswith('/'):\n",
    "                # Get the object\n",
    "                response = s3.get_object(Bucket=bucket, Key=key)\n",
    "                # Read the content of the file\n",
    "                content = response['Body'].read().decode('utf-8')\n",
    "                try:\n",
    "                    annotations = json.loads(content, strict=False)\n",
    "                    data.append(annotations[\"answers\"][0][\"answerContent\"][\"ordinalRankingDimensions\"][0])\n",
    "                except ValueError as e:\n",
    "                    print(f\"Error parsing JSON from file {key}: {e}\")\n",
    "                    continue\n",
    "with open(output_file, 'w') as file:\n",
    "    json.dump(data, file, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2da8bdc8-e8af-4822-948a-46e84ea2da6b",
   "metadata": {},
   "source": [
    "# (Optional) Alternative: Gather preference data using Anthropic Claude 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b1db8e0-c05e-4ba2-87cb-6607e66d3080",
   "metadata": {},
   "source": [
    "If you don't have a team of people who can complete the human evaluation task and you still want to proceed with your development, an alternative is to use an LLM to gather preference data. This section shows you how to use [Amazon Bedrock](https://aws.amazon.com/bedrock/) and the Anthropic Claude 3 - Sonnet model to gather preference data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0f2f1a4-9e89-4623-996e-297453416157",
   "metadata": {},
   "outputs": [],
   "source": [
    "import utils.ranker as ranker\n",
    "llm_responses = os.path.join(sample_files_path, 'llm_responses.json')\n",
    "output_file = os.path.join(sample_files_path, \"claude-rankings.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b468faf3-4d65-463c-9831-b2fca32307dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "ranker.rank(llm_responses, output_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "295a4246-4f9b-4e81-b6f2-40b3c7bc05ef",
   "metadata": {},
   "source": [
    "# 05. Process the collected feedback"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a870d14a-07a2-4fe7-b9e9-e533ba4eb27b",
   "metadata": {},
   "source": [
    "In this section we're going to sort our the results from SageMaker Ground Truth's human preference into \n",
    "1. `prompt`: The default prompt that was presented to the foundation model\n",
    "2. `chosen`: The response from the foundation model that was chosen by a labeller as the preferred response\n",
    "3. `rejected`: The response(s) from the foundation model that was rejected by a labeller\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bfca880-1de8-42d8-b04f-b3174b8a5120",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset(\"json\", data_files=llm_responses, split=\"train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a10b062c-f906-4d20-bc10-df5d3d5be92e",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(output_file, \"r\") as f:\n",
    "    response_rankings = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49c903fa-6900-403d-94ba-b93f5fe69145",
   "metadata": {},
   "outputs": [],
   "source": [
    "def return_prompt_and_responses(samples, index):\n",
    "    prompt = f\"{samples['context']}\\n\\n{samples['question']}\"\n",
    "    chosen_index = response_rankings[index][\"responseRankings\"].index(1)\n",
    "    rejected_index = response_rankings[index][\"responseRankings\"].index(4)\n",
    "\n",
    "    prompt = {\"role\": \"user\", \"content\": prompt},\n",
    "\n",
    "    chosen_messages = [\n",
    "        {\"role\": \"assistant\", \"content\": samples[\"responses\"][chosen_index]},\n",
    "    ]\n",
    "    rejected_messages = [\n",
    "        # {\"role\": \"system\", \"content\": prompt},\n",
    "        {\"role\": \"assistant\", \"content\": samples[\"responses\"][rejected_index]},\n",
    "    ]\n",
    "    \n",
    "    return {\n",
    "        \"prompt\": tokenizer.apply_chat_template(prompt, tokenize=False),\n",
    "        \"chosen\": \"{}\".format(tokenizer.apply_chat_template(chosen_messages, tokenize=False).replace('<|begin_of_text|>', '')),\n",
    "        \"rejected\": \"{}\".format(tokenizer.apply_chat_template(rejected_messages, tokenize=False).replace('<|begin_of_text|>', ''))\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa8f421d-2bf3-4381-bb54-ee9445a8d68e",
   "metadata": {},
   "outputs": [],
   "source": [
    "original_columns = dataset.column_names\n",
    "prepared_dataset = dataset.map(\n",
    "    return_prompt_and_responses,\n",
    "    with_indices=True,\n",
    "    batched=False,\n",
    "    remove_columns=original_columns\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81aa7bd0-9300-4bb9-8f8f-16ac01448412",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(prepared_dataset[0]['prompt'])\n",
    "# print(prepared_dataset[0]['chosen'])\n",
    "# print(prepared_dataset[0]['rejected'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfb442d8-cc0d-4383-a3c9-b5b6f842225d",
   "metadata": {},
   "outputs": [],
   "source": [
    "prepared_dataset.save_to_disk(os.path.join(sample_files_path, 'processed_human_feedback'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7d7ca28-a3d2-4907-b46e-27ebfdf630f6",
   "metadata": {},
   "source": [
    "Here, we choose 80-20 Train-Test split, you can choose a smaller or larger train/test split ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55f15b78-05f8-4033-91a3-8ea34e9f4b28",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = prepared_dataset.train_test_split(test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72426c08-b88a-4a57-8588-d0bd3f0dcc32",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset[\"train\"].to_json(\n",
    "    os.path.join(sample_files_path, \"processed_human_feedback\", \"train_dataset.json\"), \n",
    "    orient=\"records\", \n",
    "    index=\"False\"\n",
    ")\n",
    "\n",
    "dataset[\"test\"].to_json(\n",
    "    os.path.join(sample_files_path, \"processed_human_feedback\", \"test_dataset.json\"), \n",
    "    orient=\"records\", \n",
    "    index=\"False\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52a550ca-e192-483a-a19d-86defd1da269",
   "metadata": {},
   "source": [
    "# 06. Align `meta-llama/Meta-Llama-3-8B-Instruct` with the DPOTrainer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d632081d-787d-4dc1-a06c-5d703586dbf3",
   "metadata": {},
   "source": [
    "### Load Dataset and Set Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b72882b2-51b1-421f-81cb-c94dfd84e144",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "train_dataset = load_dataset(\"json\", data_files=os.path.join(sample_files_path, \"processed_human_feedback\", \"train_dataset.json\"), split=\"train\")\n",
    "eval_dataset = load_dataset(\"json\", data_files=os.path.join(sample_files_path, \"processed_human_feedback\", \"test_dataset.json\"), split=\"train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "633be575-8850-4141-aaea-5b5a05bda0c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(base_model_id, token=hf_access_token, cache_dir=cache_dir )\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.add_eos_token = True\n",
    "\n",
    "tokenizer.padding_side = \"left\"\n",
    "tokenizer.truncation_side = 'left'\n",
    "tokenizer.bos_token, tokenizer.eos_token\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa385e5e-1451-4d9c-9c98-f92da849d001",
   "metadata": {},
   "source": [
    "Code based on https://github.com/philschmid/deep-learning-pytorch-huggingface/blob/main/training/dpo-align-llms-in-2024-with-trl.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7c2309f-41cf-45ea-a7d3-16527b6aea6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### COMMENT IN TO RECALCULATE MAX LENGTHS ####\n",
    "from numpy import percentile\n",
    "\n",
    "# # lets find the p95 length of the prompt \n",
    "prompt_length = int(percentile([len(tokenizer(x)[\"input_ids\"]) for x in train_dataset[\"prompt\"]], 95))\n",
    "max_seq_length_chosen = int(percentile([len(tokenizer(x[\"prompt\"] + x[\"chosen\"])[\"input_ids\"]) for x in train_dataset], 95))\n",
    "max_seq_length_rejected = int(percentile([len(tokenizer(x[\"prompt\"] + x[\"rejected\"])[\"input_ids\"]) for x in train_dataset], 95))\n",
    "max_seq_length = max(max_seq_length_chosen, max_seq_length_rejected)\n",
    "\n",
    "# filter datasets to remove samples that are too long\n",
    "train_dataset = train_dataset.filter(lambda x: len(tokenizer(x[\"prompt\"] + x[\"chosen\"])[\"input_ids\"]) <= max_seq_length)\n",
    "eval_dataset = eval_dataset.filter(lambda x: len(tokenizer(x[\"prompt\"] + x[\"chosen\"])[\"input_ids\"]) <= max_seq_length)\n",
    "print(f\"len(train_dataset): {len(train_dataset)}\")\n",
    "print(f\"len(eval_dataset): {len(eval_dataset)}\")\n",
    "\n",
    "# Up the lengths to next multiple of 2, why 2? Don't know\n",
    "prompt_length = ((prompt_length + 1) // 2) * 2\n",
    "max_seq_length = ((max_seq_length + 1) // 2) * 2\n",
    "print(f\"p95 prompt length: {prompt_length}\")\n",
    "print(f\"p95 prompt + chosen length: {max_seq_length}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e75846cc-c3be-4628-86a1-7a9c72f18e39",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_length = 684\n",
    "max_seq_length = 758"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba85fc6c-0e79-47e8-9d9b-9a56dc7a92a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "peft_config = LoraConfig(\n",
    "    lora_alpha=1024,\n",
    "    lora_dropout=0.05,\n",
    "    r=2048,\n",
    "    task_type=\"CAUSAL_LM\",\n",
    "    target_modules=\"all-linear\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92d2f0fd-50f4-4ac5-b87b-be0923424e6c",
   "metadata": {},
   "source": [
    "### Fine-Tune DPO Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a4ec133-6574-40c3-a209-d7292dd45be2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from trl import DPOConfig\n",
    "\n",
    "dpo_model_dir = \"/mnt/sagemaker-nvme/fine-tuned/llama3-dpo-a1024-r2048\"\n",
    "\n",
    "args = DPOConfig(\n",
    "    output_dir=dpo_model_dir,               # directory to save and repository id\n",
    "    num_train_epochs=5,                     # number of training epochs\n",
    "    per_device_train_batch_size=2,\n",
    "    gradient_accumulation_steps=1,\n",
    "    gradient_checkpointing=True,            # use gradient checkpointing to save memory\n",
    "    optim = \"adamw_torch_fused\",            # use fused adamw optimizer\n",
    "    learning_rate=1e-5,                     # 10x higher LR than QLoRA paper\n",
    "    max_grad_norm=0.3,                      # max gradient norm based on QLoRA paper\n",
    "    warmup_ratio=0.1,                       # warmup ratio based on QLoRA paper\n",
    "    lr_scheduler_type=\"cosine\",             # use cosine learning rate scheduler\n",
    "    logging_steps=10,                       \n",
    "    save_steps=10,                         # when to save checkpoint\n",
    "    evaluation_strategy=\"steps\",            \n",
    "    eval_steps=100,\n",
    "    bf16=True,                              # use bfloat16 precision\n",
    "    tf32=True,                              # use tf32 precision\n",
    "    push_to_hub=False,                      # push model to hub,\n",
    "    report_to='tensorboard',\n",
    "    remove_unused_columns=False\n",
    ")\n",
    "\n",
    "dpo_args = {\n",
    "    \"beta\": 0.1,                            # The beta factor in DPO loss. Higher beta means less divergence\n",
    "    \"loss_type\": \"sigmoid\"                  # The loss type for DPO.\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16e36218-033e-4147-bf42-66b4d341d9c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from trl import DPOTrainer\n",
    "\n",
    "trainer = DPOTrainer(\n",
    "    model,\n",
    "    ref_model=None,\n",
    "    peft_config=peft_config,\n",
    "    args=args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    max_length=max_seq_length,\n",
    "    max_prompt_length=prompt_length,\n",
    "    beta=dpo_args[\"beta\"],\n",
    "    loss_type=dpo_args[\"loss_type\"],\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9beac72-5614-4523-8b68-a65d042f20bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdf5022e-1cdd-4a9d-b53e-0b5a9017db0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "peft_output_dir = os.path.join(args.output_dir, \"peft\")\n",
    "print(f\"saving peft model to: {peft_output_dir}\")\n",
    "trainer.save_model(output_dir=peft_output_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b821a42-e863-48af-9fd4-1159756b4193",
   "metadata": {},
   "source": [
    "### Merge Base Model with Adapter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbceb840-e7e0-48e7-879e-eeb9c0e33f8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "dpo_model_dir = \"/mnt/sagemaker-nvme/fine-tuned/llama3-dpo-a1024-r2048\"\n",
    "\n",
    "f\"{dpo_model_dir}/peft\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac4ce5b3-4f2b-4bfe-a29f-b546b5097cde",
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import PeftModel, PeftConfig\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from peft import AutoPeftModelForCausalLM\n",
    "\n",
    "new_dpo_output_dir = os.path.join(args.output_dir, \"merged_full_model\")\n",
    "\n",
    "# Load PEFT model on CPU\n",
    "model = AutoPeftModelForCausalLM.from_pretrained(\n",
    "    f\"{peft_output_dir}/\",\n",
    "    torch_dtype=torch.float16,\n",
    "    low_cpu_mem_usage=True,\n",
    "    token=hf_access_token,\n",
    "    cache_dir=cache_dir\n",
    ")\n",
    "# Merge LoRA and base model and save\n",
    "merged_model = model.merge_and_unload()\n",
    "print(f\"saving merged model into {new_dpo_output_dir}\")\n",
    "\n",
    "merged_model.save_pretrained(\n",
    "    new_dpo_output_dir, \n",
    "    safe_serialization=True, \n",
    "    max_shard_size=\"9GB\"\n",
    ")\n",
    "tokenizer.save_pretrained(new_dpo_output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0d6fcd0-bf11-4f9f-8872-05b6fc4cc5d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "del model\n",
    "del merged_model\n",
    "del trainer\n",
    "del tokenizer\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73a86e51-82cc-48d4-9114-c003c3197195",
   "metadata": {},
   "source": [
    "# 07. Evaluate the fine-tuned model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83eae658-a3a6-45fb-b170-d225e946a797",
   "metadata": {},
   "source": [
    "### Reload new merged model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f76ffe09-27d9-4482-b665-71cf2d161fde",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    new_dpo_output_dir,\n",
    "    token=hf_access_token,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    # quantization_config=bnb_config,\n",
    "    device_map=\"auto\",\n",
    "    cache_dir=cache_dir\n",
    "\n",
    ")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(new_dpo_output_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7431612d-bc79-49d8-8370-4e43ceb5fbff",
   "metadata": {},
   "source": [
    "### Compare Model Responses Pre and Post Fine-Tuning "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af9366e4-8043-4e92-ac8c-27a626e12505",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_resp_pre_dpo = json.loads(open(os.path.join(sample_files_path, 'llm_responses.json')).read())\n",
    "rankings = json.loads(open(os.path.join(sample_files_path, 'claude-rankings.json'), \"r\").read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d282c49c-a04c-4a70-8695-65b1e1550f28",
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "\n",
    "# select model responses where first model response and rank position are not the same (1 != 1)\n",
    "selected_responses, selected_ranks = [], []\n",
    "for resp, rank in zip(model_resp_pre_dpo, rankings):\n",
    "    curr_rank = rank['responseRankings'][0]\n",
    "    if curr_rank != 1:\n",
    "        selected_responses.append(resp)\n",
    "        selected_ranks.append(rank)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "979c0043-d5d6-47fa-a6a6-de3afb1f9813",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "def generate_html_table(data):\n",
    "    html = \"<table>\"\n",
    "    for i, row in enumerate(data):\n",
    "        html += \"<tr>\"\n",
    "        for j, col in enumerate(row):\n",
    "            if i == 0:  # If it's the first row (header), make it bold and with color fill\n",
    "                html += \"<th style='background-color: #f0ad4e; color: white; padding: 5px; font-weight: bold'>{}</th>\".format(col)\n",
    "            else:\n",
    "                html += \"<td>{}</td>\".format(col)\n",
    "        html += \"</tr>\"\n",
    "    html += \"</table>\"\n",
    "    return html\n",
    "\n",
    "\n",
    "def generate_and_compare_prepost_responses(chosen_response_pairs):\n",
    "\n",
    "    inference_data = [\n",
    "        ['Question', 'New DPO Response', 'Rejected Model Response', 'Human Chosen Response']\n",
    "    ]\n",
    "\n",
    "    pipe = pipeline(\"text-generation\", model=model, tokenizer=tokenizer)\n",
    "    context = f\"\"\"{company_context} You are a customer service agent for {company_name} Answer the following customer question in one or two sentences:\n",
    "    \"\"\"\n",
    "\n",
    "    for idx in tqdm(range(0, len(selected_responses)), total=len(selected_responses)):\n",
    "        question_ = selected_responses[idx]['question']\n",
    "        messages = [\n",
    "            {\"role\": \"user\", \"content\": f\"{context}: {question_}\"}\n",
    "        ]\n",
    "        response = pipe(\n",
    "            messages, \n",
    "            max_new_tokens=120, \n",
    "            do_sample=True,\n",
    "            temperature=gl_temperature, \n",
    "            top_p=gl_top_p, \n",
    "            pad_token_id=tokenizer.eos_token_id,\n",
    "            bos_token_id=tokenizer.eos_token\n",
    "        )\n",
    "\n",
    "        chosen = None\n",
    "        rejected = None\n",
    "        for sample_ in chosen_response_pairs:\n",
    "            if question_ in sample_['prompt']:\n",
    "                chosen = sample_['chosen']\n",
    "                rejected = sample_['rejected']\n",
    "\n",
    "        assert chosen is not None, \"Chosen is None\"\n",
    "        assert rejected is not None, \"Rejected is None\"\n",
    "        \n",
    "        new_model_response = response[0]['generated_text'][1]['content']\n",
    "        reject_model_response = rejected\n",
    "        chosen_model_response = chosen\n",
    "\n",
    "        inference_data.append([question_, new_model_response, reject_model_response, chosen_model_response])\n",
    "    return inference_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d6fe436-c041-4b9f-aa25-d168d2e3494e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_from_disk\n",
    "dataset = load_from_disk(\"../../sample-files/processed_human_feedback\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3e93503-e6bc-482d-b32e-d22292045b1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "inf_results = generate_and_compare_prepost_responses(chosen_response_pairs=dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3064287d-1db7-4629-a9b7-3e26eac7cac7",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(HTML(generate_html_table(inf_results)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5a9beda-b15a-4902-bb80-ee8e83e5de78",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_html_to_file(html_content, file_path):\n",
    "    with open(file_path, 'w') as file:\n",
    "        file.write(html_content)\n",
    "    print(\"HTML table saved to:\", file_path)\n",
    "    \n",
    "# Generate HTML table\n",
    "html_content = generate_html_table(inf_results)\n",
    "\n",
    "# Save HTML to file\n",
    "save_html_to_file(html_content, 'rlhf-pre-post-responses.html')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99e36853-e940-42d2-ad9b-7774f018afbe",
   "metadata": {},
   "source": [
    "# 08. Deploy to a SageMaker Endpoint"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83f08c0c-4f7a-43c0-8552-7b61b14d9610",
   "metadata": {},
   "source": [
    "The saved model can be hosted as a custom SageMaker Endpoint using DJL Serving with weights, sample `inference.py` and meta model `serving.properties` that together inform how DJL serving would load and host model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f78968b-e527-4c5d-b440-0593a5dede85",
   "metadata": {},
   "source": [
    "## Push Model to S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ce13c2a-9d0c-4568-99c0-f31845a87fa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "fine_tuned_s3_uri = f\"s3://{sess.default_bucket()}/llama3-ft/modelweights\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ecbecb6-5168-4e92-9c22-f1183c5bf21e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Uploading model to {fine_tuned_s3_uri}\")\n",
    "sagemaker.s3.S3Uploader.upload(\n",
    "    new_dpo_output_dir, \n",
    "    fine_tuned_s3_uri\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12f917b5-563f-489d-a4d5-5e9e8b51bdaf",
   "metadata": {},
   "source": [
    "## Create a `serving.properties` File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09ad7fed-db2a-4c82-bcbb-580053376238",
   "metadata": {},
   "outputs": [],
   "source": [
    "import textwrap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e80f89b-cb01-45c9-a505-67a578c4bafc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create and write serving properties file\n",
    "serving_properties = textwrap.dedent(f\"\"\"\n",
    "engine = DeepSpeed\n",
    "option.tensor_parallel_degree = 1\n",
    "option.s3url = {fine_tuned_s3_uri}/\n",
    "option.hf_access_token={hf_access_token}\n",
    "\"\"\").strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbfd0136-ff69-4dd2-ad2f-38acc0ff8253",
   "metadata": {},
   "outputs": [],
   "source": [
    "local_meta_model_dir = \"./llama3-serving-model\"\n",
    "os.makedirs(local_meta_model_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1e48036-f8b1-4e5f-961b-a29f6be14b37",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(local_meta_model_dir, \"serving.properties\"), \"w\") as prop_file:\n",
    "    prop_file.write(serving_properties)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c97aedb-0a3d-44eb-88d1-e5af4afbc4ea",
   "metadata": {},
   "source": [
    "## Create `model.py` file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e01f4815-b2bf-4d01-9734-7cbe42738fee",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile llama3-serving-model/model.py\n",
    "\n",
    "from djl_python import Input, Output\n",
    "import os\n",
    "import torch\n",
    "from transformers import pipeline, AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
    "from typing import Any, Dict, Tuple\n",
    "import deepspeed\n",
    "import warnings\n",
    "import tarfile\n",
    "from peft import PeftModel\n",
    "\n",
    "\n",
    "predictor = None\n",
    "\n",
    "\n",
    "def get_model(properties):\n",
    "\n",
    "    print(\"properties------>\", properties)\n",
    "\n",
    "    local_rank = int(os.getenv(\"LOCAL_RANK\", \"0\"))\n",
    "\n",
    "    local_model_id = properties[\"model_id\"]\n",
    "    hf_access_token = properties[\"hf_access_token\"]\n",
    "    \n",
    "    print(f\"model files found: {os.listdir(local_model_id)}\")\n",
    "\n",
    "    print(f\"Loading model from {local_model_id}\")\n",
    "    base_model = AutoModelForCausalLM.from_pretrained(\n",
    "        local_model_id,\n",
    "        # quantization_config=bnb_config,\n",
    "        low_cpu_mem_usage=True,\n",
    "        torch_dtype=torch.bfloat16,\n",
    "        token=hf_access_token\n",
    "    )\n",
    "    print(\"model loaded!\")\n",
    "    print(\"\\nconverting model into deep speed...\")\n",
    "    \n",
    "    model = deepspeed.init_inference(\n",
    "        base_model,\n",
    "        mp_size=properties[\"tensor_parallel_degree\"]\n",
    "    )\n",
    "\n",
    "    # load tokenizer\n",
    "    print(f\"Loading tokenizer from {local_model_id}\")\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\n",
    "        local_model_id,\n",
    "        token=hf_access_token\n",
    "    )\n",
    "    \n",
    "    generator = pipeline(\n",
    "        task=\"text-generation\", \n",
    "        model=model, \n",
    "        tokenizer=tokenizer,\n",
    "        device=local_rank\n",
    "    )\n",
    "    return generator\n",
    "\n",
    "\n",
    "def handle(inputs: Input) -> None:\n",
    "\n",
    "    global predictor\n",
    "    if not predictor:\n",
    "        predictor = get_model(inputs.get_properties())\n",
    "\n",
    "    if inputs.is_empty():\n",
    "        # Model server makes an empty call to warmup the model on startup\n",
    "        return None\n",
    "        \n",
    "    data = inputs.get_as_json()\n",
    "    \n",
    "    content = data[\"inputs\"]\n",
    "\n",
    "    message = [\n",
    "        {\"role\": \"user\", \"content\": f\"{content}\"}\n",
    "    ]\n",
    "    \n",
    "    generation_kwargs = data[\"parameters\"]\n",
    "\n",
    "    print(\"message\", message, \"parameters\", generation_kwargs)\n",
    "    \n",
    "    outputs = predictor(message, **generation_kwargs)[0]['generated_text'][-1]\n",
    "    result = {\"outputs\": outputs['content']}\n",
    "    return Output().add(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6d3cdc6-e8a6-4641-9272-076d08a2f2ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile llama3-serving-model/requirements.txt\n",
    "transformers==4.39.3\n",
    "langchain==0.0.305\n",
    "sentence_transformers\n",
    "accelerate==0.28.0\n",
    "bitsandbytes==0.43.0\n",
    "peft==0.11.1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30741782-7963-4518-857c-3a79e5ccf53b",
   "metadata": {},
   "source": [
    "## Create Meta Model Tarball"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "021e61c8-c9dd-4525-83b5-7c00d35da75b",
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm -rf ./{local_meta_model_dir}/.ipynb_checkpoints/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "763cc95d-8f6e-471a-85fa-1b2b0746cc13",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.system(f\"tar czvf {os.path.basename(local_meta_model_dir)}.tar.gz {local_meta_model_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a55397ce-f519-4417-8f32-cd9edb91c567",
   "metadata": {},
   "source": [
    "## Deploy Model as a SageMaker Endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32242e38-eba7-4271-b522-ec060bf57949",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker import image_uris\n",
    "from sagemaker.model import Model\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a270b24-d1d0-4a3d-a984-6b6061f076d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "region = \"us-east-1\"\n",
    "\n",
    "inference_image_uri = image_uris.retrieve(\n",
    "    framework=\"djl-deepspeed\", \n",
    "    region=region, \n",
    "    version=\"0.23.0\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bbd1294-416c-40c7-a937-361c9f8c7fc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "meta_model_s3_uri = f\"s3://{sess.default_bucket()}/llama3-ft2/metamodel-python\"\n",
    "print(f\"Uploading meta-model to {meta_model_s3_uri}\")\n",
    "meta_model_s3_uri = sagemaker.s3.S3Uploader.upload(\n",
    "    f\"{os.path.basename(local_meta_model_dir)}.tar.gz\", \n",
    "    meta_model_s3_uri\n",
    ")\n",
    "print(f\"Uploaded meta-model to {meta_model_s3_uri}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ad9dd96-2fb1-487f-9304-3e874c19e2c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "dpo_model = Model(\n",
    "    sagemaker_session=sess,\n",
    "    image_uri=inference_image_uri,\n",
    "    model_data=meta_model_s3_uri,\n",
    "    role=role,\n",
    "    name=f\"llama3-dpo-optimized-{datetime.now().strftime('%y%m%d%H%M')}\",\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85d6ef2d-aa33-4e36-871f-16284a6470e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Deploying model with endpoint name ep-{dpo_model.name}\")\n",
    "dpo_model.deploy(\n",
    "    initial_instance_count=1,\n",
    "    instance_type=\"ml.g5.2xlarge\",\n",
    "    endpoint_name=f\"ep-{dpo_model.name}\",\n",
    "    container_startup_health_check_timeout=900,\n",
    "    wait=False, # <-- Set to True, if you would prefer to wait for the endpoint to spin up\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "911fa128-3a70-44a6-b385-a60bad02ce64",
   "metadata": {},
   "source": [
    "## Test Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbb5200b-91e8-4a6c-8bc0-3bdbb259a8ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "from sagemaker import serializers, deserializers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f2240be-44c3-455e-932e-8077fffd5741",
   "metadata": {},
   "outputs": [],
   "source": [
    "dpo_ft_predictor = sagemaker.Predictor(\n",
    "    endpoint_name=dpo_model.endpoint_name,\n",
    "    sagemaker_session=sess,\n",
    "    serializer=serializers.JSONSerializer(),\n",
    "    deserializer=deserializers.JSONDeserializer(),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22f735f0-581d-433d-8864-416770a7e7d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_question = \"How do you minimize environmental footprint?\"\n",
    "content = f\"\"\"\n",
    "{company_context}You are a customer service agent at Example bank. You are polite and helpful. Sometimes you are allowed to be clever and provide rude answers, but only sometimes. Answer the following customer question in one or two sentences: {user_question}\n",
    "\"\"\"\n",
    "parameters =  {'max_new_tokens': 120, 'top_p': gl_top_p, 'temperature': gl_temperature}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96d7a524-e0be-4e69-81bc-4eaabaef2d4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = dpo_ft_predictor.predict(\n",
    "    {\n",
    "        \"inputs\": content,\n",
    "        \"parameters\": parameters\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb361d68-937b-4ee6-99a0-9ef566cb3e95",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"SageMaker Endpoint Model Response ===>\", response['outputs'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5927e1c6-24bd-4a06-a271-646fba5e5052",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "availableInstances": [
   {
    "_defaultOrder": 0,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.t3.medium",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 1,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.t3.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 2,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.t3.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 3,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.t3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 4,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 5,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 6,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 7,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 8,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 9,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 10,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 11,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 12,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5d.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 13,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5d.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 14,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5d.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 15,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5d.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 16,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5d.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 17,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5d.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 18,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5d.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 19,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 20,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": true,
    "memoryGiB": 0,
    "name": "ml.geospatial.interactive",
    "supportedImageNames": [
     "sagemaker-geospatial-v1-0"
    ],
    "vcpuNum": 0
   },
   {
    "_defaultOrder": 21,
    "_isFastLaunch": true,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.c5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 22,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.c5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 23,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.c5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 24,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.c5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 25,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 72,
    "name": "ml.c5.9xlarge",
    "vcpuNum": 36
   },
   {
    "_defaultOrder": 26,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 96,
    "name": "ml.c5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 27,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 144,
    "name": "ml.c5.18xlarge",
    "vcpuNum": 72
   },
   {
    "_defaultOrder": 28,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.c5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 29,
    "_isFastLaunch": true,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g4dn.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 30,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g4dn.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 31,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g4dn.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 32,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g4dn.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 33,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g4dn.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 34,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g4dn.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 35,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 61,
    "name": "ml.p3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 36,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 244,
    "name": "ml.p3.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 37,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 488,
    "name": "ml.p3.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 38,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.p3dn.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 39,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.r5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 40,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.r5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 41,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.r5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 42,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.r5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 43,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.r5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 44,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.r5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 45,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.r5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 46,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.r5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 47,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 48,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 49,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 50,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 51,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 52,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 53,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.g5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 54,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.g5.48xlarge",
    "vcpuNum": 192
   },
   {
    "_defaultOrder": 55,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 56,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4de.24xlarge",
    "vcpuNum": 96
   }
  ],
  "instance_type": "ml.g4dn.xlarge",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
