{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5f3e3387-0822-4c70-b9e3-7d52c1e54c42",
   "metadata": {},
   "source": [
    "## Lab 04. Fine Tune LLM on Custom Dataset using Amazon Trainium `trn1`/`trn1n` and SageMaker Studio"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f07126d6-f66b-42f1-b600-b9a8889f2205",
   "metadata": {},
   "source": [
    "____\n",
    "In this demo notebook, we demonstrate how to use the SageMaker Python SDK to deploy pre-trained Llama 2 model as well as fine-tune it for your dataset in domain adaptation or instruction tuning format on [AWS Trainium](https://aws.amazon.com/ec2/instance-types/trn1/) and [AWS Inferentia](https://aws.amazon.com/ec2/instance-types/inf2/) based instances.\n",
    "\n",
    "AWS Neuron is an SDK with a compiler, runtime, and profiling tools that unlocks high-performance and cost-effective deep learning (DL) acceleration. It supports high-performance training on AWS Trainium-based Amazon Elastic Compute Cloud (Amazon EC2) Trn1 instances. For model deployment, it supports high-performance and low-latency inference on AWS Inferentia-based Amazon EC2 Inf1 instances and AWS Inferentia2-based Amazon EC2 Inf2 instances. For details, see [Official documentation](https://awsdocs-neuron.readthedocs-hosted.com/en/latest/index.html).\n",
    "\n",
    "____"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2547830-03e0-4600-b2d3-ab99acb07f58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ideally your license must be set to custom_attribute = \"accept_eula=true\"\n",
    "custom_attribute = open(\"../studio-local-ui/custom_attribute.txt\", \"r\").read()\n",
    "print(f\"Your license condition is set to ---> {custom_attribute}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fcc6e6a-e227-4392-bbe7-48d169d91f53",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install --upgrade sagemaker datasets -q"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dd5bb44-b122-48aa-8fb5-ead812a831dc",
   "metadata": {},
   "source": [
    "### _Temporary Workaround 'till re:Invent 2023"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b711133-5528-4dcb-9526-520d7b8c1d12",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install ./sagemaker-2.297.1.dev0-py2.py3-none-any.whl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c0ea2c8-7a7b-4342-9cc7-090405e274ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ.update({\n",
    "    \"AWS_JUMPSTART_CONTENT_BUCKET_OVERRIDE\": \"jumpstart-cache-alpha-us-west-2\",\n",
    "    \"AWS_JUMPSTART_GATED_CONTENT_BUCKET_OVERRIDE\": \"jumpstart-private-cache-prod-us-west-2\",\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb02d340-70f4-4abe-afbe-93487a6b5a48",
   "metadata": {},
   "source": [
    "## Dataset Preparation for Fine-Tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a42e5f3f-4050-449d-8b2a-7bb7df45a3c3",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "You can fine-tune on the dataset with domain adaptation format or instruction tuning format. Below are the instructions for how the training data should be formatted for input to the model.\n",
    "\n",
    "- **Input:** A train directory containing either a JSON lines (`.jsonl`) or text (`.txt`) formatted file. \n",
    "  - For JSON lines (JSONL) file, each line is a dictionary, repsentating a dictionary. The key in dictionary (each line) has to be 'text'.\n",
    "  - The number of files under train directory should equal to one. \n",
    "- **Output:** A trained model that can be deployed for inference. \n",
    "\n",
    "In this demo, we will use a subset of [Dolly dataset](https://huggingface.co/datasets/databricks/databricks-dolly-15k) in an instruction tuning format. Dolly dataset contains roughly 15,000 instruction following records for various categories such as question answering, summarization, information extraction etc. It is available under Apache 2.0 license. We will select the summarization examples for fine-tuning.\n",
    "\n",
    "For demonstration of using text file as input, please see [Appendix 2](#2.-Use-text-file-as-input-to-fine-tune-LLaMA-2)\n",
    "\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a075bc9-fb11-4e4c-9d05-973bb0d5bfcc",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "    We're only going to be using a small subset (10%) of the original dataset. Please edit `train[:10%]` to expand training to full dataset\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fffc2bd-14c7-4c80-a837-05c23c21e173",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dolly_dataset = load_dataset(\n",
    "    \"databricks/databricks-dolly-15k\", \n",
    "    split=\"train[:10%]\"\n",
    ")\n",
    "\n",
    "task = \"information_extraction\"\n",
    "# To train for summarization/closed question and answering, you can replace the assertion in next line to example[\"category\"] == \"sumarization\"/\"closed_qa\".\n",
    "summarization_dataset = dolly_dataset.filter(\n",
    "    lambda example: example[\"category\"] == task\n",
    ")\n",
    "summarization_dataset = summarization_dataset.remove_columns(\"category\")\n",
    "\n",
    "# We split the dataset into two where test data is used to evaluate at the end.\n",
    "train_and_test_dataset = summarization_dataset.train_test_split(test_size=0.1)\n",
    "\n",
    "# Dumping the training data to a local file to be used for training.\n",
    "train_and_test_dataset[\"train\"].to_json(\"train.jsonl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf5ae07b-a296-4638-a6a5-1ce0cc703845",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_and_test_dataset[\"train\"][-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a33e2616-c4d7-4458-b206-55765ef3d1ca",
   "metadata": {},
   "source": [
    "---\n",
    "Next, we use a prompt template for preprocessing the data in an instruction / input format for the training job, and also for inferencing the deployed endpoint.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14b96a3f-3abc-4dc9-88b7-d0c3a4fe8abb",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = (\n",
    "    \"\"\"Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\\n\\n### Instruction:\\n{instruction}\\n\\n### Input:\\n{context}### Response:\\n{response}\\n\\n<s>\"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3e04c53-13bf-4582-802b-2588b0ff0993",
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_prompt_template(sample):\n",
    "    return {\n",
    "        \"text\": prompt.format(\n",
    "            instruction=sample[\"instruction\"], \n",
    "            context=sample[\"context\"], \n",
    "            response=sample[\"response\"]\n",
    "        )\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccfc702c-be36-4824-a469-5476acdeaaec",
   "metadata": {},
   "source": [
    "Apply prompt template across all text rows/objects in the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29bb4555-2d78-47a0-ae88-39f93cc86b8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_processed = train_and_test_dataset.map(\n",
    "    apply_prompt_template, \n",
    "    remove_columns=list(train_and_test_dataset[\"train\"].features)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f914542-f27e-4d2a-a667-ff2f50eccb87",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_processed[\"train\"].to_json(f\"dolly/processed-train-{task}.jsonl\")\n",
    "dataset_processed[\"test\"].to_json(f\"dolly/processed-test-{task}.jsonl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed21dad6-4216-4c17-9f89-80eef35b2a32",
   "metadata": {},
   "source": [
    "### Upload Fine-Tuning Dataset to S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa996f51-a273-4439-8988-5cd30457ce4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.s3 import S3Uploader\n",
    "import sagemaker\n",
    "import random\n",
    "\n",
    "output_bucket = sagemaker.Session().default_bucket()\n",
    "local_data_file = f\"dolly/processed-train-{task}.jsonl\"\n",
    "train_data_location = f\"s3://{output_bucket}/trn1_13b/dolly_dataset\"\n",
    "S3Uploader.upload(local_data_file, train_data_location)\n",
    "print(f\"Training data  ---> : {train_data_location}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36736751-bdba-4924-8b58-1d98b45dcbb7",
   "metadata": {},
   "source": [
    "We can quickly check if our dataset exists in the s3 prefix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1af0b6b4-98a0-43e8-b457-33f0fd11cef2",
   "metadata": {},
   "outputs": [],
   "source": [
    "!aws s3 ls s3://sagemaker-us-west-2-914153712152/trn1_13b/dolly_dataset/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa269ac1-5d75-4ed1-a604-d6bd40cbb7c6",
   "metadata": {},
   "source": [
    "## Fine-Tune Llama 2 Model "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cefbd828-84e8-4614-9dd9-fae3d3b0b594",
   "metadata": {},
   "source": [
    "---\n",
    "Next, we fine-tune the LLaMA v2 model on the summarization dataset from Dolly on [AWS Trainium](https://aws.amazon.com/ec2/instance-types/trn1/) instance. You have two options: `ml.trn1.32xlarge` (default) and `ml.trn1n.32xlarge`. Finetuning scripts are based on scripts provided by [Neuronx-Nemo-Megatron](https://github.com/aws-neuron/neuronx-nemo-megatron). For a list of supported hyper-parameters and their default values, please see [supported hyperparameters for fine-tuning](#3.-Supported-Hyper-parameters-for-fine-tuning).\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b9f3aaf-6122-40d1-9013-e4a7fd5681e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_id = \"meta-textgenerationneuron-llama-2-13b\"\n",
    "model_version = \"1.*\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6387b65-b40b-41e1-9ef6-b4c2b9f57ea6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker import hyperparameters\n",
    "\n",
    "my_hyperparameters = hyperparameters.retrieve_default(\n",
    "    model_id=model_id, \n",
    "    model_version=model_version\n",
    ")\n",
    "\n",
    "print(my_hyperparameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e7ba5a2-43d7-42a0-a8ac-4a4510623fc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#my_hyperparameters[\"max_input_length\"] = \"4096\" # you can increase it up to 4096 for sequence length.\n",
    "my_hyperparameters[\"max_steps\"] = \"25\"\n",
    "my_hyperparameters[\"learning_rate\"] = \"0.0001\"\n",
    "my_hyperparameters[\"global_train_batch_size\"] = \"1000\"\n",
    "print(my_hyperparameters)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b433aa6-9b3f-4e22-9fad-fa9c86882b7f",
   "metadata": {},
   "source": [
    "Validate if our hyper-parameters are llama2 model compliant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54205177-a6bb-4253-b964-884ca707c7c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparameters.validate(\n",
    "    model_id=model_id, model_version=model_version, hyperparameters=my_hyperparameters\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00dcf172-5d73-499e-9d12-2579a1865449",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.jumpstart.estimator import JumpStartEstimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e510dc3d-3edb-4b24-9607-caab9ca8ffe1",
   "metadata": {},
   "outputs": [],
   "source": [
    "estimator = JumpStartEstimator(\n",
    "    model_id=model_id,\n",
    "    model_version=model_version,\n",
    "    hyperparameters=my_hyperparameters,\n",
    "    environment={\"accept_eula\": \"true\"}, \n",
    "    role=\"arn:aws:iam::914153712152:role/workshop-studio-v2-cfn-OSE-EMR-SageMakerExecutionRole\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d5d5aa9-f2b7-44cb-b7ad-b44a49632c50",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "estimator.fit(\n",
    "    {\"train\": train_data_location}\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3d4ab4d-09da-4063-8c02-8029e5fe624c",
   "metadata": {},
   "source": [
    "## Deploy Fine-Tuned Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61b7e69d-fccf-4f89-a744-4bef267ae746",
   "metadata": {},
   "source": [
    "---\n",
    "Next, we deploy the fine-tuned model. We will compare the performance of fine-tuned and pre-trained model.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b60f0a8-f6a8-4bca-8986-aad6865f0ca7",
   "metadata": {},
   "outputs": [],
   "source": [
    "finetuned_predictor = estimator.deploy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35271231-5faa-4caf-b189-6ad12fe31ec4",
   "metadata": {},
   "source": [
    "## Evaluate Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10b79cfb-b236-4479-a617-b5558159c410",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display, Markdown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94974d9a-1d9d-4970-b1a1-b49448d0c8b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_inference = (\n",
    "    \"\"\"Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\\n\\n### Instruction:\\n{instruction}\\n\\n### Input:\\n{context}\"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "985f32c9-fcb8-448f-872b-6db10e3f6384",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = train_and_test_dataset[\"test\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a0b3a24-9462-4f68-9be3-d0e3eda08a22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For instruction fine-tuning, we insert a special key between input and output\n",
    "input_output_demarkation_key = \"\\n\\n### Response:\\n\"\n",
    "\n",
    "for i, datapoint in enumerate(test_dataset.select(range(2))):\n",
    "\n",
    "    payload = {\n",
    "        \"inputs\": prompt_inference.format(\n",
    "            instruction=datapoint[\"instruction\"], \n",
    "            context=datapoint[\"context\"]\n",
    "        )\n",
    "        + input_output_demarkation_key,\n",
    "        \"parameters\": {\"max_new_tokens\": 100},\n",
    "    }\n",
    "    finetuned_response = finetuned_predictor.predict(payload)\n",
    "    display(Markdown(f\"**Row: {i}**\\n---\\n{payload['inputs']} {finetuned_response['generated_text']}\\n---\\n\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a7591af-7c9e-484d-a506-08bceb484862",
   "metadata": {},
   "source": [
    "## Clean Up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b1b5ffc-b215-4b5f-a6d1-7b3b6513429f",
   "metadata": {},
   "outputs": [],
   "source": [
    "finetuned_predictor.delete_model()\n",
    "finetuned_predictor.delete_endpoint()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
