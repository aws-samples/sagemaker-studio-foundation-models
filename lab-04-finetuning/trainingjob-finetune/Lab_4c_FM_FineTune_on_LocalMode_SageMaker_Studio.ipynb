{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6c89e0f8",
   "metadata": {},
   "source": [
    "# Install Docker CLI"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a8143ef",
   "metadata": {},
   "source": [
    "First, let's install Docker CLI. Docker CLI is required to leverage SageMaker Local Mode.\n",
    "\n",
    "With Local Mode, developers can now train and test models, debug code, and validate end-to-end pipelines directly on their SageMaker Studio notebook instance without the need for spinning up remote compute resources. This reduces the iteration cycle from minutes down to seconds, boosting developer productivity. Docker support in SageMaker Studio notebooks enables developers to effortlessly build Docker containers and access pre-built containers, providing a consistent development environment across the team and avoiding time-consuming setup and dependency management.\n",
    "\n",
    "Learn more --> [Accelerate ML workflows with Amazon SageMaker Studio Local Mode and Docker support](https://aws.amazon.com/blogs/machine-learning/accelerate-ml-workflows-with-amazon-sagemaker-studio-local-mode-and-docker-support/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce1b55ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "sudo apt-get -y install ca-certificates curl gnupg\n",
    "\n",
    "sudo install -m 0755 -d /etc/apt/keyrings\n",
    "curl -fsSL https://download.docker.com/linux/ubuntu/gpg | sudo gpg --dearmor -o /etc/apt/keyrings/docker.gpg\n",
    "sudo chmod a+r /etc/apt/keyrings/docker.gpg\n",
    "echo \\\n",
    "  \"deb [arch=$(dpkg --print-architecture) signed-by=/etc/apt/keyrings/docker.gpg] https://download.docker.com/linux/ubuntu \\\n",
    "  $(. /etc/os-release && echo \"$VERSION_CODENAME\") stable\" | \\\n",
    "  sudo tee /etc/apt/sources.list.d/docker.list > /dev/null"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96ead2b4-44c2-424c-b8e7-3a0df00d4208",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "sudo apt-get -y update\n",
    "\n",
    "# pick the latest patch from:\n",
    "# apt-cache madison docker-ce | awk '{ print $3 }' | grep -i 20.10\n",
    "VERSION_STRING=5:20.10.24~3-0~ubuntu-jammy\n",
    "sudo apt-get install docker-ce-cli=$VERSION_STRING docker-compose-plugin -y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "035ecf24-9b4b-4d99-a2ac-21dbd7ce7aa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash \n",
    "# validate the Docker Client is able to access Docker Server at [unix:///docker/proxy.sock]\n",
    "docker version"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c29e22c3-d378-48ed-91f4-e26475d93d11",
   "metadata": {},
   "source": [
    "## Install Pre-requisites"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d39a25a-5d52-4b8c-b271-98e2ace4203f",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -Uq datasets==2.18.0\n",
    "%pip install -Uq transformers==4.39.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42676b69-67f0-4b35-87a9-da1c7c5c2acf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import boto3\n",
    "import pprint\n",
    "from tqdm import tqdm\n",
    "import sagemaker\n",
    "from sagemaker.collection import Collection\n",
    "from sagemaker.utils import name_from_base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2156bbd-7dbf-4b30-9b5a-b97548954859",
   "metadata": {},
   "outputs": [],
   "source": [
    "sagemaker_session =  sagemaker.session.Session() #sagemaker.session.Session()\n",
    "region = sagemaker_session.boto_region_name\n",
    "role = sagemaker.get_execution_role()\n",
    "default_bucket = sagemaker_session.default_bucket()\n",
    "sm_client = boto3.client('sagemaker', region_name=region)\n",
    "model_collector = Collection(sagemaker_session=sagemaker_session)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed0721f1-363d-41ad-9635-7aa545f433d0",
   "metadata": {},
   "source": [
    "## Define Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cf75077-25b1-4d42-a1ba-d6860b6e6013",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_id = \"mistralai/Mistral-7B-v0.1\" \n",
    "# define a base dataset to finetune this base model\n",
    "dataset_name = \"databricks/databricks-dolly-15k\"\n",
    "# data s3 path\n",
    "s3_data_uri = f\"s3://{default_bucket}/dataset-for-training/dolly2\"\n",
    "# your hf token\n",
    "hf_token = \"hf_xxxxx\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93255209-6d5b-44e0-a766-ed7f104bdb77",
   "metadata": {},
   "source": [
    "## Prepare Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38c8e30a-e13e-419f-a853-6d81d3a16f05",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from random import randrange\n",
    "\n",
    "# Load dataset from the hub\n",
    "train_dataset = load_dataset(dataset_name, split=\"train[:05%]\")\n",
    "validation_dataset = load_dataset(dataset_name, split=\"train[95%:]\")\n",
    "\n",
    "print(f\"Training size: {len(train_dataset)} | Validation size: {len(validation_dataset)}\")\n",
    "print(\"\\nTraining sample:\\n\")\n",
    "print(train_dataset[randrange(len(train_dataset))])\n",
    "print(\"\\nValidation sample:\\n\")\n",
    "print(validation_dataset[randrange(len(validation_dataset))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4f84658-4056-44e2-8afb-b4141185c131",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_dolly(sample):\n",
    "    instruction = f\"### Instruction\\n{sample['instruction']}\"\n",
    "    context = f\"### Context\\n{sample['context']}\" if len(sample[\"context\"]) > 0 else None\n",
    "    response = f\"### Answer\\n{sample['response']}\"\n",
    "    # join all the parts together\n",
    "    prompt = \"\\n\\n\".join([i for i in [instruction, context, response] if i is not None])\n",
    "    return prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dec39e17-4d20-4702-80a7-c62e25c93417",
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import randrange\n",
    "\n",
    "print(format_dolly(train_dataset[randrange(len(train_dataset))]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e98c3242-6259-4944-9a6a-83d419f4156b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id, token=hf_token)\n",
    "tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57590711-d39a-4f05-856f-3d444e6d6aae",
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import randint\n",
    "from itertools import chain\n",
    "from functools import partial\n",
    "\n",
    "\n",
    "# template dataset to add prompt to each sample\n",
    "def template_dataset(sample):\n",
    "    sample[\"text\"] = f\"{format_dolly(sample)}{tokenizer.eos_token}\"\n",
    "    return sample\n",
    "\n",
    "\n",
    "# apply prompt template per sample\n",
    "# train\n",
    "train_dataset = train_dataset.map(template_dataset, remove_columns=list(train_dataset.features))\n",
    "# validation\n",
    "validation_dataset = validation_dataset.map(template_dataset, remove_columns=list(validation_dataset.features))\n",
    "# print random sample\n",
    "print(validation_dataset[randint(0, len(validation_dataset))][\"text\"])\n",
    "\n",
    "# empty list to save remainder from batches to use in next batch\n",
    "remainder = {\"input_ids\": [], \"attention_mask\": [], \"token_type_ids\": []}\n",
    "\n",
    "def chunk(sample, chunk_length=2048):\n",
    "    # define global remainder variable to save remainder from batches to use in next batch\n",
    "    global remainder\n",
    "    # Concatenate all texts and add remainder from previous batch\n",
    "    concatenated_examples = {k: list(chain(*sample[k])) for k in sample.keys()}\n",
    "    concatenated_examples = {k: remainder[k] + concatenated_examples[k] for k in concatenated_examples.keys()}\n",
    "    # get total number of tokens for batch\n",
    "    batch_total_length = len(concatenated_examples[list(sample.keys())[0]])\n",
    "\n",
    "    # get max number of chunks for batch\n",
    "    if batch_total_length >= chunk_length:\n",
    "        batch_chunk_length = (batch_total_length // chunk_length) * chunk_length\n",
    "\n",
    "    # Split by chunks of max_len.\n",
    "    result = {\n",
    "        k: [t[i : i + chunk_length] for i in range(0, batch_chunk_length, chunk_length)]\n",
    "        for k, t in concatenated_examples.items()\n",
    "    }\n",
    "    # add remainder to global variable for next batch\n",
    "    remainder = {k: concatenated_examples[k][batch_chunk_length:] for k in concatenated_examples.keys()}\n",
    "    # prepare labels\n",
    "    result[\"labels\"] = result[\"input_ids\"].copy()\n",
    "    return result\n",
    "\n",
    "\n",
    "# tokenize and chunk dataset\n",
    "\n",
    "# training\n",
    "lm_train_dataset = train_dataset.map(\n",
    "    lambda sample: tokenizer(sample[\"text\"]), batched=True, remove_columns=list(train_dataset.features)\n",
    ").map(\n",
    "    partial(chunk, chunk_length=2048),\n",
    "    batched=True,\n",
    ")\n",
    "\n",
    "# validation\n",
    "lm_valid_dataset = validation_dataset.map(\n",
    "    lambda sample: tokenizer(sample[\"text\"]), batched=True, remove_columns=list(validation_dataset.features)\n",
    ").map(\n",
    "    partial(chunk, chunk_length=2048),\n",
    "    batched=True,\n",
    ")\n",
    "\n",
    "# Print total number of samples\n",
    "print(f\"Total number of samples: {len(validation_dataset)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "487e44b7-9e3a-40b9-bcf8-06431ecd4ddd",
   "metadata": {},
   "source": [
    "## Upload dataset to S3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ab9b751",
   "metadata": {},
   "source": [
    "This step is optional if you're using only local mode. S3 is required if you're scaling your model training to multi-node/multi-gpu training job."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e223ba11-0502-43bb-9d6e-e9319f5afa92",
   "metadata": {},
   "outputs": [],
   "source": [
    "f'{s3_data_uri}/train'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e25aae75-76c4-4ea5-929d-07f266756345",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save train_dataset to s3\n",
    "training_input_path = f'{s3_data_uri}/train'\n",
    "lm_train_dataset.save_to_disk(\"./train\")\n",
    "sagemaker.s3.S3Uploader.upload(local_path=\"./train\", desired_s3_uri=training_input_path, sagemaker_session=sagemaker_session)\n",
    "\n",
    "print(f\"saving training dataset to: {training_input_path}\")\n",
    "\n",
    "# save train_dataset to s3\n",
    "validation_input_path = f'{s3_data_uri}/validation'\n",
    "lm_valid_dataset.save_to_disk(\"./validation\")\n",
    "sagemaker.s3.S3Uploader.upload(local_path=\"./validation\", desired_s3_uri=validation_input_path, sagemaker_session=sagemaker_session)\n",
    "\n",
    "print(f\"saving validation dataset to: {validation_input_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "818736f9-41c5-4675-9071-afe33c34ce2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "from sagemaker.huggingface import HuggingFace\n",
    "from sagemaker.local import LocalSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28a2d55c-0f54-4399-b9ce-7d19f9310e05",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define Training Job Name \n",
    "time_suffix = datetime.now().strftime('%y%m%d%H%M')\n",
    "job_name = f'huggingface-qlora-{time_suffix}'\n",
    "experiments_name = f\"exp-{model_id.replace('/', '-')}\"\n",
    "run_name = f\"qlora-finetune-run-{time_suffix}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e9e54a8-389f-480a-8cd5-c541a0410edc",
   "metadata": {},
   "outputs": [],
   "source": [
    "DUMMY_IAM_ROLE = 'arn:aws:iam::111111111111:role/service-role/AmazonSageMaker-ExecutionRole-20200101T000001'\n",
    "LOCAL_SESSION = LocalSession()\n",
    "LOCAL_SESSION.config = {'local': {'local_code': True}}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b829c0ae-86e0-4b61-9f2e-dfb654e5fcb7",
   "metadata": {},
   "source": [
    "### Write Session Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d445f4c-122a-42ba-ac57-c6ce90907225",
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir ./code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8775293a-69f9-404a-828b-af783b40e76e",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile code/finetune_llm.py\n",
    "\n",
    "import os\n",
    "import boto3\n",
    "import json\n",
    "import tarfile\n",
    "import argparse\n",
    "from tqdm import tqdm\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    set_seed,\n",
    "    default_data_collator,\n",
    "    BitsAndBytesConfig,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    ")\n",
    "from peft import (\n",
    "    get_peft_model,\n",
    "    LoraConfig,\n",
    "    prepare_model_for_kbit_training,\n",
    ")\n",
    "from peft.tuners.lora import LoraLayer\n",
    "from datasets import load_from_disk\n",
    "import torch\n",
    "import bitsandbytes as bnb\n",
    "import sagemaker\n",
    "import shutil\n",
    "\n",
    "\n",
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"max_split_size_mb:512\"\n",
    "\n",
    "\n",
    "# Reference: https://github.com/artidoro/qlora/blob/main/qlora.py\n",
    "def print_trainable_parameters(\n",
    "    model, \n",
    "    use_4bit=False\n",
    "):\n",
    "    \"\"\"\n",
    "    Prints the number of trainable parameters in the model.\n",
    "    \"\"\"\n",
    "    trainable_params = 0\n",
    "    all_param = 0\n",
    "    for _, param in model.named_parameters():\n",
    "        num_params = param.numel()\n",
    "        # if using DS Zero 3 and the weights are initialized empty\n",
    "        if num_params == 0 and hasattr(param, \"ds_numel\"):\n",
    "            num_params = param.ds_numel\n",
    "\n",
    "        all_param += num_params\n",
    "        if param.requires_grad:\n",
    "            trainable_params += num_params\n",
    "    if use_4bit:\n",
    "        trainable_params /= 2\n",
    "    print(\n",
    "        f\"all params: {all_param:,d} || trainable params: {trainable_params:,d} || trainable%: {100 * trainable_params / all_param}\"\n",
    "    )\n",
    "\n",
    "\n",
    "# Reference: https://github.com/artidoro/qlora/blob/main/qlora.py\n",
    "def find_all_linear_names(\n",
    "    model\n",
    "):\n",
    "    lora_module_names = set()\n",
    "    for name, module in model.named_modules():\n",
    "        if isinstance(module, bnb.nn.Linear4bit):\n",
    "            names = name.split(\".\")\n",
    "            lora_module_names.add(names[0] if len(names) == 1 else names[-1])\n",
    "\n",
    "    if \"lm_head\" in lora_module_names:  # needed for 16-bit\n",
    "        lora_module_names.remove(\"lm_head\")\n",
    "    return list(lora_module_names)\n",
    "\n",
    "\n",
    "def create_peft_model(\n",
    "    model, \n",
    "    r_value, \n",
    "    lora_alpha, \n",
    "    lora_dropout, \n",
    "    task_type,\n",
    "    gradient_checkpointing=True, \n",
    "    bf16=True\n",
    "):\n",
    "\n",
    "    # prepare int-4 model for training\n",
    "    model = prepare_model_for_kbit_training(\n",
    "        model, use_gradient_checkpointing=gradient_checkpointing\n",
    "    )\n",
    "    if gradient_checkpointing:\n",
    "        model.gradient_checkpointing_enable()\n",
    "\n",
    "    # get lora target modules\n",
    "    modules = find_all_linear_names(model)\n",
    "    print(f\"Found {len(modules)} modules to quantize: {modules}\")\n",
    "\n",
    "    peft_config = LoraConfig(\n",
    "        r=r_value,\n",
    "        lora_alpha=lora_alpha,\n",
    "        target_modules=modules,\n",
    "        lora_dropout=lora_dropout,\n",
    "        bias=\"none\",\n",
    "        task_type=task_type\n",
    "    )\n",
    "\n",
    "    model = get_peft_model(model, peft_config)\n",
    "\n",
    "    model.print_trainable_parameters()\n",
    "    \n",
    "    return model\n",
    "\n",
    "\n",
    "def finetune_llm(args):\n",
    "    # set seed\n",
    "    set_seed(args.seed)\n",
    "    \n",
    "    print(f\"loading dataset from {args.sm_train_dir} and {args.sm_validation_dir}\")\n",
    "\n",
    "    train_dataset = load_from_disk(args.sm_train_dir)\n",
    "    validation_dataset = load_from_disk(args.sm_validation_dir)\n",
    "\n",
    "    print(f\"Training Dataset: {len(train_dataset)} || Validation Dataset: {len(validation_dataset)}\")\n",
    "    \n",
    "    print(f\"region: {args.region}\")\n",
    "    \n",
    "    # load model from the hub with a bnb config\n",
    "    bnb_config = BitsAndBytesConfig(\n",
    "        load_in_4bit=True,\n",
    "        bnb_4bit_use_double_quant=True,\n",
    "        bnb_4bit_quant_type=\"nf4\",\n",
    "        bnb_4bit_compute_dtype=torch.bfloat16,\n",
    "    )\n",
    "\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        args.model_id,\n",
    "        use_cache=False if args.gradient_checkpointing else True,\n",
    "        device_map=\"auto\",\n",
    "        quantization_config=bnb_config,\n",
    "        token=args.hf_token\n",
    "    )\n",
    "    \n",
    "    tokenizer = AutoTokenizer.from_pretrained(\n",
    "        args.model_id, \n",
    "        token=args.hf_token\n",
    "    )\n",
    "\n",
    "    # create peft config\n",
    "    model = create_peft_model(\n",
    "        model, \n",
    "        r_value=args.lora_r, \n",
    "        lora_alpha=args.lora_alpha, \n",
    "        lora_dropout=args.lora_dropout, \n",
    "        task_type=args.task_type,\n",
    "        gradient_checkpointing=args.gradient_checkpointing, \n",
    "        bf16=args.bf16\n",
    "    )\n",
    "\n",
    "    # Define training args\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=f\"{args.sm_output_dir}/{args.model_id}/trainer-outputs\",\n",
    "        per_device_train_batch_size=args.per_device_train_batch_size,\n",
    "        bf16=args.bf16,  # Use BF16 if available\n",
    "        learning_rate=args.learning_rate,\n",
    "        num_train_epochs=args.epochs,\n",
    "        gradient_checkpointing=args.gradient_checkpointing,\n",
    "        logging_dir=f\"{args.sm_output_dir}/{args.model_id}/logs\",\n",
    "        logging_strategy=\"steps\",\n",
    "        logging_steps=args.logging_steps,\n",
    "        save_strategy=\"no\",\n",
    "    )\n",
    "\n",
    "    # Create Trainer instance with SageMaker experiments callback\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=train_dataset,\n",
    "        eval_dataset=validation_dataset,\n",
    "        data_collator=default_data_collator,\n",
    "        # callbacks=[SageMakerExperimentsCallback(region=args.region)]\n",
    "    )\n",
    "    \n",
    "    # mutes warnings during training, reenable during inference\n",
    "    model.config.use_cache = False \n",
    "\n",
    "    # Start training\n",
    "    trainer.train()\n",
    "    \n",
    "    # Start evaluation\n",
    "    trainer.evaluate()\n",
    "    \n",
    "    temp_dir=\"/tmp/model/\"\n",
    "    \n",
    "    if args.merge_weights:\n",
    "        \n",
    "        trainer.model.save_pretrained(temp_dir, safe_serialization=False)\n",
    "        # clear memory\n",
    "        del model\n",
    "        del trainer\n",
    "        torch.cuda.empty_cache()\n",
    "        \n",
    "        from peft import AutoPeftModelForCausalLM\n",
    "\n",
    "        # load PEFT model in fp16\n",
    "        model = AutoPeftModelForCausalLM.from_pretrained(\n",
    "            temp_dir,\n",
    "            low_cpu_mem_usage=True,\n",
    "            torch_dtype=torch.float16,\n",
    "        )  \n",
    "        # Merge LoRA and base model and save\n",
    "        model = model.merge_and_unload()        \n",
    "        model.save_pretrained(\n",
    "            args.sm_model_dir, safe_serialization=True, max_shard_size=\"2GB\"\n",
    "        )   \n",
    "        \n",
    "        source_dir = './djl-inference/'\n",
    "\n",
    "        # copy djl-inference files to model directory\n",
    "        for f in os.listdir(source_dir):\n",
    "            source_f = os.path.join(source_dir, f)\n",
    "            \n",
    "            # Copy the files to the destination folder\n",
    "            shutil.copy(source_f, args.sm_model_dir)\n",
    "        \n",
    "    else:   \n",
    "        # save finetuned LoRA model and then the tokenizer for inference\n",
    "        trainer.model.save_pretrained(\n",
    "            args.sm_model_dir, \n",
    "            safe_serialization=True\n",
    "        )\n",
    "    tokenizer.save_pretrained(\n",
    "        args.sm_model_dir\n",
    "    )\n",
    "    \n",
    "    print(\"Done!\")\n",
    "    \n",
    "\n",
    "def read_parameters():\n",
    "    parser = argparse.ArgumentParser()\n",
    "    # add model id and dataset path argument\n",
    "    parser.add_argument(\"--hf_token\", type=str, help=\"Hugging face token for gated models\")\n",
    "    parser.add_argument(\"--model_id\", type=str, help=\"Hugging face model id to use for training.\")\n",
    "    parser.add_argument(\"--epochs\", type=int, default=1, help=\"No of epochs to train the model\")\n",
    "    parser.add_argument(\"--per_device_train_batch_size\", type=int, default=2, help=\"Batch size to use for training.\")\n",
    "    parser.add_argument(\"--learning_rate\", type=float, default=1e-5, help=\"Model learning rate\")\n",
    "    parser.add_argument(\"--seed\", type=int, default=8, help=\"Seed to use for training\")\n",
    "    parser.add_argument(\"--gradient_checkpointing\", type=bool, default=True, help=\"Path to deepspeed config file\")\n",
    "    parser.add_argument(\"--bf16\", type=bool, default=False if torch.cuda.get_device_capability()[0] == 8 else False, help=\"Whether to use bf16.\")\n",
    "    parser.add_argument(\"--lora_r\", type=int, default=64, help=\"Lora attention dimension value\")\n",
    "    parser.add_argument(\"--lora_alpha\", type=int, default=16, help=\"The alpha parameter for Lora scaling\")\n",
    "    parser.add_argument(\"--lora_dropout\", type=float, default=0.1, help=\"The dropout probability for Lora layers\")\n",
    "    parser.add_argument(\n",
    "        \"--task_type\", \n",
    "        type=str, default=\"CAUSAL_LM\", \n",
    "        help=\"Choose from: CAUSAL_LM, FEATURE_EXTRACTION, QUESTION_ANS, SEQ_2_SEQ_LM, SEQ_CLS, TOKEN_CLS\"\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--merge_weights\",\n",
    "        action='store_true',\n",
    "        help=\"Whether to merge LoRA weights with base model.\",\n",
    "    )\n",
    "    parser.add_argument(\"--logging_steps\", type=int, default=2, help=\"Step interval to start logging to console/sagemaker experiments\")\n",
    "    parser.add_argument(\"--region\", type=str, default=\"us-east-1\", help=\"SageMaker job execution region\")\n",
    "    \n",
    "    # sagemaker env args: refer to this for more arguments: https://github.com/aws/sagemaker-training-toolkit/blob/master/ENVIRONMENT_VARIABLES.md\n",
    "    parser.add_argument(\"--sm_model_dir\", type=str, default=os.environ[\"SM_MODEL_DIR\"])\n",
    "    parser.add_argument(\"--sm_train_dir\", type=str, default=os.environ[\"SM_CHANNEL_TRAINING\"])\n",
    "    parser.add_argument(\"--sm_validation_dir\", type=str, default=os.environ[\"SM_CHANNEL_VALIDATION\"])\n",
    "    parser.add_argument(\"--sm_current_host\", type=str, default=os.environ[\"SM_CURRENT_HOST\"])\n",
    "    parser.add_argument(\"--sm_hosts\", type=list, default=os.environ[\"SM_HOSTS\"])\n",
    "    parser.add_argument(\"--sm_output_dir\", type=list, default=os.environ[\"SM_OUTPUT_DIR\"])\n",
    "    parser.add_argument(\"--n_gpus\", type=list, default=os.environ[\"SM_NUM_GPUS\"])\n",
    "    \n",
    "    args, _ = parser.parse_known_args()\n",
    "    return args\n",
    "\n",
    "\n",
    "def main():\n",
    "    args = read_parameters()\n",
    "    print(args)\n",
    "    finetune_llm(args)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdc6aa34-d599-47c8-8922-ad60bda19263",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile code/smexperiments_callback.py\n",
    "\"\"\" SageMaker Experiments callback implementation\"\"\"\n",
    "\n",
    "import importlib\n",
    "import logging\n",
    "from transformers import TrainerCallback\n",
    "\n",
    "\n",
    "# disable INFO and WARNING logging status to prevent flood of WARNs\n",
    "logging.getLogger(\"sagemaker\").setLevel(logging.CRITICAL)\n",
    "\n",
    "\n",
    "def is_sagemaker_available():\n",
    "    return importlib.util.find_spec(\"sagemaker\") is not None\n",
    "\n",
    "\n",
    "class SageMakerExperimentsCallback(TrainerCallback):\n",
    "    \"\"\"\n",
    "    SageMaker Experiments Plus transformer callback. \n",
    "    Designed to allow auto logging from transformer API.\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self, \n",
    "        region,\n",
    "        _has_sagemaker_experiments=is_sagemaker_available()\n",
    "    ):\n",
    "        \n",
    "        assert (\n",
    "            _has_sagemaker_experiments\n",
    "        ), \"SageMakerExperimentsCallback requires sagemaker to be install. Run 'pip install -U sagemaker'\"\n",
    "        \n",
    "        import boto3\n",
    "        import sagemaker\n",
    "        from sagemaker.experiments.run import load_run      \n",
    "        \n",
    "        self.sagemaker_session = sagemaker.session.Session(\n",
    "            boto3.session.Session(region_name=region)\n",
    "        )\n",
    "        self.local_load_run = load_run\n",
    "        \n",
    "        # epoch tracker\n",
    "        self.last_epoch = None\n",
    "        \n",
    "        with load_run(sagemaker_session=self.sagemaker_session) as run: \n",
    "            self.sm_experiments_run = run\n",
    "            self.ctx_exp_name = run.experiment_name\n",
    "            self.ctx_run_name = run.run_name\n",
    "            \n",
    "            print(f\"[sm-callback] loaded sagemaker Experiment (name: {self.ctx_exp_name}) with run: {self.ctx_run_name}!\")\n",
    "    \n",
    "    def on_init_end(self, args, state, control, **kwargs):\n",
    "        \n",
    "        print(f\"[sm-callback] adding parameters to {self.ctx_exp_name}: {self.ctx_run_name}\")\n",
    "        \n",
    "        with self.local_load_run(\n",
    "            experiment_name=self.ctx_exp_name, \n",
    "            run_name=self.ctx_run_name,\n",
    "            sagemaker_session=self.sagemaker_session\n",
    "        ) as ctx_run: \n",
    "            ctx_run.log_parameters(\n",
    "                {\n",
    "                    k: str(v) if str(v) else None \n",
    "                        for k, v in vars(args).items() \n",
    "                            if isinstance(v, (str, int, float, bool))\n",
    "                }\n",
    "            )\n",
    "              \n",
    "    def on_log(self, args, state, control, logs=None, **kwargs):\n",
    "        \n",
    "        with self.local_load_run(\n",
    "            experiment_name=self.ctx_exp_name, \n",
    "            run_name=self.ctx_run_name,\n",
    "            sagemaker_session=self.sagemaker_session\n",
    "        ) as ctx_run: \n",
    "            \n",
    "            for k, v in logs.items():\n",
    "                if not k.startswith('eval'):\n",
    "                    ctx_run.log_metric(\n",
    "                        name=f\"train/step:{k}\", \n",
    "                        value=v, \n",
    "                        step=int(state.global_step)\n",
    "                    )\n",
    "\n",
    "    def on_epoch_end(self, args, state, control, logs=None, **kwargs): \n",
    "        \"\"\"\n",
    "        On epoch end we average results and log it into an epoch value as x \n",
    "        and average of metrics as y\n",
    "        \"\"\"\n",
    "        with self.local_load_run(\n",
    "            experiment_name=self.ctx_exp_name, \n",
    "            run_name=self.ctx_run_name,\n",
    "            sagemaker_session=self.sagemaker_session\n",
    "        ) as ctx_run:\n",
    "            \n",
    "            epoch_history = state.log_history\n",
    "            \n",
    "            if self.last_epoch is None:\n",
    "                self.last_epoch = 0\n",
    "            \n",
    "            current_epoch = int(round(epoch_history[-1]['epoch']))\n",
    "            \n",
    "            print(f\"[sm-callback] start: {self.last_epoch} ep to end: {current_epoch} ep!\")\n",
    "            \n",
    "            epoch_loss_values = {\n",
    "                row['epoch']: row['loss'] \n",
    "                for row in epoch_history \n",
    "                if self.last_epoch < row['epoch'] <= current_epoch\n",
    "            }\n",
    "            average_epoch_loss = sum(list(epoch_loss_values.values()))/len(epoch_loss_values)\n",
    "            \n",
    "            ctx_run.log_metric(\n",
    "                name=\"train/epoch:loss\",\n",
    "                value=average_epoch_loss, \n",
    "                step=int(current_epoch)\n",
    "            )\n",
    "            \n",
    "            self.last_epoch = current_epoch\n",
    "\n",
    "    def on_evaluate(self, args, state, control, logs=None, **kwargs):\n",
    "        \"\"\"\n",
    "        On train end we average results and log it into an epoch value as x \n",
    "        and average of metrics as y\n",
    "        \"\"\"\n",
    "        with self.local_load_run(\n",
    "            experiment_name=self.ctx_exp_name, \n",
    "            run_name=self.ctx_run_name,\n",
    "            sagemaker_session=self.sagemaker_session\n",
    "        ) as ctx_run:\n",
    "            \n",
    "            epoch_history = state.log_history\n",
    "            \n",
    "            ctx_run.log_metric(\n",
    "                name=\"final/eval:loss\",\n",
    "                value=epoch_history[-1][\"eval_loss\"] \n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5580a111-de84-4d60-916c-f7662750b703",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the Estimator\n",
    "huggingface_estimator = HuggingFace(\n",
    "    entry_point='finetune_llm.py',      \n",
    "    source_dir='code',         \n",
    "    instance_type='local_gpu',   \n",
    "    instance_count=1,       \n",
    "    role=DUMMY_IAM_ROLE,\n",
    "    base_job_name=job_name,             \n",
    "    transformers_version='4.36',            \n",
    "    pytorch_version='2.1',             \n",
    "    py_version='py310',           \n",
    "    hyperparameters={\n",
    "        'hf_token': hf_token,\n",
    "        'model_id': model_id,                             \n",
    "        'dataset_path': '/opt/ml/input/data/training',    \n",
    "        'epochs': 1,                                      \n",
    "        'per_device_train_batch_size': 2,                 \n",
    "        'lr': 1e-4,\n",
    "        'merge_weights':True,\n",
    "        'region':region,\n",
    "    },\n",
    "    output_path='file://model/',\n",
    "    sagemaker_session=LOCAL_SESSION\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e41ab2a-e26f-4d82-a7fd-bf0adf581fe5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# starting the train job with our uploaded datasets as input\n",
    "data = {\n",
    "    'training': \"file://./train\", #training_input_path, \n",
    "    'validation': \"file://./validation\", #validation_input_path\n",
    "}\n",
    "huggingface_estimator.fit(\n",
    "    data, \n",
    "    wait=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a1f7184-7594-4ff0-be98-05b848d5420f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
